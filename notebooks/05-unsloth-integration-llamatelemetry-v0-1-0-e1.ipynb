{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Check GPU Environment","metadata":{}},{"cell_type":"code","source":"# Verify we have 2Ã— T4 GPUs\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ” KAGGLE GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Check nvidia-smi\nresult = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\ngpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\nprint(f\"\\nğŸ“Š Detected GPUs: {len(gpu_lines)}\")\nfor line in gpu_lines:\n    print(f\"   {line}\")\n\n# Check CUDA version\nprint(\"\\nğŸ“Š CUDA Version:\")\n!nvcc --version | grep release\n\n# Check total VRAM\nprint(\"\\nğŸ“Š VRAM Summary:\")\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n\n# Verify we have 2 GPUs\nif len(gpu_lines) >= 2:\n    print(\"\\nâœ… Multi-GPU environment confirmed! Ready for llamatelemetry v0.1.0.\")\nelse:\n    print(\"\\nâš ï¸ WARNING: Less than 2 GPUs detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:55:08.154499Z","iopub.execute_input":"2026-02-04T20:55:08.155048Z","iopub.status.idle":"2026-02-04T20:55:08.515415Z","shell.execute_reply.started":"2026-02-04T20:55:08.155018Z","shell.execute_reply":"2026-02-04T20:55:08.514474Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ” KAGGLE GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected GPUs: 2\n   GPU 0: Tesla T4 (UUID: GPU-8993fd3a-98f8-ea3a-b787-cee9ef2db424)\n   GPU 1: Tesla T4 (UUID: GPU-7efb3485-c121-5d84-ebba-fa60e4e1ab39)\n\nğŸ“Š CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\nğŸ“Š VRAM Summary:\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\n\nâœ… Multi-GPU environment confirmed! Ready for llamatelemetry v0.1.0.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 2: Install Unsloth and llamatelemetry","metadata":{}},{"cell_type":"code","source":"%%time\nprint(\"ğŸ“¦ Installing Unsloth and llamatelemetry...\")\n\n# Install Unsloth (fast installation)\n!pip install -q unsloth huggingface-hub sseclient-py\n\n# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Additional dependencies\n!pip install -q datasets trl\n\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy\n\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\nâœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    from unsloth import FastLanguageModel\n    print(\"âœ… Unsloth installed\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Unsloth import issue: {e}\")\n\n\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\n\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:55:11.039948Z","iopub.execute_input":"2026-02-04T20:55:11.040946Z","iopub.status.idle":"2026-02-04T21:04:00.234766Z","shell.execute_reply.started":"2026-02-04T20:55:11.040869Z","shell.execute_reply":"2026-02-04T21:04:00.234030Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing Unsloth and llamatelemetry...\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:03\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:02\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m274.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m355.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m305.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m321.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m332.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m299.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m338.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m314.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m312.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m170.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m298.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m286.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.3.0 requires fsspec[http]<=2025.9.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.3.0 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65501eafab7641bea04cb5bd1327be1c"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llamatelemetry 0.1.0 installed\nğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-02-04 21:03:21.138633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770239001.377582      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770239001.449733      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770239002.040852      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770239002.040924      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770239002.040928      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770239002.040930      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nâœ… Unsloth installed\nâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\nCPU times: user 1min 24s, sys: 18.8 s, total: 1min 43s\nWall time: 8min 49s\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# First, set up Kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nhf_token = user_secrets.get_secret(\"HF_TOKEN_2\")\n\n# Then login to Hugging Face Hub using the token from secrets\nfrom huggingface_hub import login\n\nlogin(token=hf_token)  # Pass the token from Kaggle secrets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:04:53.605201Z","iopub.execute_input":"2026-02-04T21:04:53.605555Z","iopub.status.idle":"2026-02-04T21:04:53.921816Z","shell.execute_reply.started":"2026-02-04T21:04:53.605521Z","shell.execute_reply":"2026-02-04T21:04:53.921156Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Load Base Model with Unsloth\n\nWe'll use Gemma-3 1B as it's fast to fine-tune on T4.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom unsloth import FastLanguageModel\nimport torch\n\nprint(\"=\"*70)\nprint(\"ğŸ“¥ LOADING BASE MODEL WITH UNSLOTH\")\nprint(\"=\"*70)\n\n# Model configuration\nmodel_name = \"unsloth/gemma-3-1b-it\"  # Small model for demo\nmax_seq_length = 2048\n\nprint(f\"\\nğŸ“¥ Loading {model_name}...\")\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=torch.float16,\n    load_in_4bit=True,  # Use 4-bit for training\n)\n\nprint(f\"\\nâœ… Model loaded!\")\nprint(f\"   Model: {model_name}\")\nprint(f\"   Max Sequence Length: {max_seq_length}\")\nprint(f\"   Precision: 4-bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:04:58.456633Z","iopub.execute_input":"2026-02-04T21:04:58.457315Z","iopub.status.idle":"2026-02-04T21:05:19.369820Z","shell.execute_reply.started":"2026-02-04T21:04:58.457281Z","shell.execute_reply":"2026-02-04T21:05:19.368989Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“¥ LOADING BASE MODEL WITH UNSLOTH\n======================================================================\n\nğŸ“¥ Loading unsloth/gemma-3-1b-it...\n==((====))==  Unsloth 2026.1.4: Fast Gemma3 patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\nUnsloth: Gemma3 does not support SDPA - switching to fast eager.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f01c3bfe47a43b6a2c09f7ba281cac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68f5235d51040ed9d48b1b8ea4dabbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4c8d4aaf494c929dee7963f0628109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9be8bc6b78a04e7d961b48fd35055814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4df4a618025433e9973fca5180b66fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b68cf1096384f32a72b588223b7351b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2afb35642946958786de1322d0008a"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Model loaded!\n   Model: unsloth/gemma-3-1b-it\n   Max Sequence Length: 2048\n   Precision: 4-bit\nCPU times: user 13.3 s, sys: 4.57 s, total: 17.8 s\nWall time: 20.9 s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 4: Add LoRA Adapters","metadata":{}},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ”§ ADDING LORA ADAPTERS\")\nprint(\"=\"*70)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,                # LoRA rank\n    lora_alpha=32,       # LoRA alpha\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n)\n\n# Count trainable parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\n\nprint(f\"\\nâœ… LoRA adapters added!\")\nprint(f\"   Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\nprint(f\"   Total params: {total:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:06:08.251200Z","iopub.execute_input":"2026-02-04T21:06:08.251549Z","iopub.status.idle":"2026-02-04T21:06:13.706959Z","shell.execute_reply.started":"2026-02-04T21:06:08.251511Z","shell.execute_reply":"2026-02-04T21:06:13.706159Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ”§ ADDING LORA ADAPTERS\n======================================================================\nUnsloth: Making `model.base_model.model.model` require gradients\n\nâœ… LoRA adapters added!\n   Trainable params: 13,045,760 (1.93%)\n   Total params: 675,994,752\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Step 5: Prepare Training Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š PREPARING TRAINING DATASET\")\nprint(\"=\"*70)\n\n# Load a small dataset for demo (Alpaca format)\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n\nprint(f\"\\nğŸ“Š Dataset loaded: {len(dataset)} examples\")\nprint(f\"\\nğŸ“‹ Sample data:\")\nprint(dataset[0])\n\n# Format for training (Alpaca prompt format)\ndef format_alpaca(example):\n    instruction = example.get(\"instruction\", \"\")\n    input_text = example.get(\"input\", \"\")\n    output = example.get(\"output\", \"\")\n    \n    if input_text:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Response:\n{output}\"\"\"\n    \n    return {\"text\": prompt}\n\ndataset = dataset.map(format_alpaca)\nprint(f\"\\nâœ… Dataset formatted for training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:07:05.619110Z","iopub.execute_input":"2026-02-04T21:07:05.619475Z","iopub.status.idle":"2026-02-04T21:07:08.673656Z","shell.execute_reply.started":"2026-02-04T21:07:05.619435Z","shell.execute_reply":"2026-02-04T21:07:08.672745Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š PREPARING TRAINING DATASET\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79323a0ed45a4216a655adab0bca6b8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceecda1333994331a4763ddb04991974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c4ab9bb1364f4a8f88a3376adc3be6"}},"metadata":{}},{"name":"stdout","text":"\nğŸ“Š Dataset loaded: 500 examples\n\nğŸ“‹ Sample data:\n{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': '', 'instruction': 'Give three tips for staying healthy.'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e310428b47f64eedba4735c81ee2c763"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Dataset formatted for training\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Step 6: Train with SFTTrainer","metadata":{}},{"cell_type":"code","source":"%%time\nfrom trl import SFTTrainer, SFTConfig\n\nprint(\"=\"*70)\nprint(\"ğŸ‹ï¸ TRAINING MODEL\")\nprint(\"=\"*70)\n\n# Training configuration (quick demo - increase for real training)\ntraining_args = SFTConfig(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=30,  # Quick demo - use more for real training\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=5,\n    output_dir=\"./unsloth_output\",\n    optim=\"adamw_8bit\",\n    seed=42,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    args=training_args,\n)\n\nprint(\"\\nğŸ‹ï¸ Starting training...\")\nprint(f\"   Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Max steps: {training_args.max_steps}\")\nprint(f\"   Learning rate: {training_args.learning_rate}\")\n\ntrainer.train()\n\nprint(\"\\nâœ… Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:07:43.448212Z","iopub.execute_input":"2026-02-04T21:07:43.448565Z","iopub.status.idle":"2026-02-04T21:09:34.579377Z","shell.execute_reply.started":"2026-02-04T21:07:43.448535Z","shell.execute_reply":"2026-02-04T21:09:34.578555Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ‹ï¸ TRAINING MODEL\n======================================================================\nUnsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f39698135cc471a84217153341607b4"}},"metadata":{}},{"name":"stdout","text":"ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n\nğŸ‹ï¸ Starting training...\n   Batch size: 2\n   Max steps: 30\n   Learning rate: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 00:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.133600</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.484100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.533800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.503700</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.315200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.449200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nâœ… Training complete!\nCPU times: user 1min 32s, sys: 5.23 s, total: 1min 37s\nWall time: 1min 51s\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Step 7: Export to GGUF Format\n\nThis is the key step - converting from Unsloth to llama.cpp compatible format.","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(\"=\"*70)\nprint(\"ğŸ“¦ EXPORTING TO GGUF FORMAT\")\nprint(\"=\"*70)\n\n# Output path\nOUTPUT_DIR = \"/kaggle/working/gguf_output\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Unsloth's built-in GGUF export\nprint(\"\\nğŸ“¦ Exporting to GGUF with Q4_K_M quantization...\")\n\nmodel.save_pretrained_gguf(\n    OUTPUT_DIR,\n    tokenizer,\n    quantization_method=\"q4_k_m\",  # K-quant for best quality/size\n)\n\n# Find the exported file\ngguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\nprint(f\"\\nâœ… GGUF export complete!\")\nprint(f\"   Output directory: {OUTPUT_DIR}\")\nprint(f\"   Files: {gguf_files}\")\n\nif gguf_files:\n    gguf_path = os.path.join(OUTPUT_DIR, gguf_files[0])\n    size_mb = os.path.getsize(gguf_path) / (1024**2)\n    print(f\"   Size: {size_mb:.1f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:10:09.921847Z","iopub.execute_input":"2026-02-04T21:10:09.922707Z","iopub.status.idle":"2026-02-04T21:16:22.087657Z","shell.execute_reply.started":"2026-02-04T21:10:09.922670Z","shell.execute_reply":"2026-02-04T21:16:22.086231Z"}},"outputs":[{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n","output_type":"stream"},{"name":"stdout","text":"======================================================================\nğŸ“¦ EXPORTING TO GGUF FORMAT\n======================================================================\n\nğŸ“¦ Exporting to GGUF with Q4_K_M quantization...\nUnsloth: Merging model weights to 16-bit format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c43dc4558d4c30be1f18f520ffd712"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\nCache check failed: model.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e3de79d27e24fa5b67aeb3690d8e0d7"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.52s/it]\nUnsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/gguf_output`\nUnsloth: Converting to GGUF format...\n==((====))==  Unsloth: Conversion from HF to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: Updating system package directories\nUnsloth: All required system packages already installed!\nUnsloth: Install llama.cpp and building - please wait 1 to 3 minutes\nUnsloth: Cloning llama.cpp repository\nUnsloth: Install GGUF and other packages\nUnsloth: Successfully installed llama.cpp!\nUnsloth: Preparing converter script...\nUnsloth: [1] Converting model into f16 GGUF format.\nThis might take 3 minutes...\nUnsloth: Initial conversion completed! Files: ['gemma-3-1b-it.F16.gguf']\nUnsloth: [2] Converting GGUF f16 into q4_k_m. This might take 10 minutes...\nUnsloth: Model files cleanup...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### We removed it in GGUF's chat template for you.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: All GGUF conversions completed successfully!\nGenerated files: ['gemma-3-1b-it.Q4_K_M.gguf']\nUnsloth: example usage for text only LLMs: llama-cli --model gemma-3-1b-it.Q4_K_M.gguf -p \"why is the sky blue?\"\nUnsloth: Saved Ollama Modelfile to current directory\nUnsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n\nâœ… GGUF export complete!\n   Output directory: /kaggle/working/gguf_output\n   Files: []\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import glob\n\nprint(\"CWD:\", os.getcwd())\nprint(\"GGUF in /kaggle/working:\", glob.glob(\"/kaggle/working/*.gguf\"))\nprint(\"GGUF in output dir:\", glob.glob(\"/kaggle/working/gguf_output/*.gguf\"))\nprint(\"GGUF in output cache:\", glob.glob(\"/kaggle/working/gguf_output/**/**/*.gguf\", recursive=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:16:49.314281Z","iopub.execute_input":"2026-02-04T21:16:49.314609Z","iopub.status.idle":"2026-02-04T21:16:49.320931Z","shell.execute_reply.started":"2026-02-04T21:16:49.314577Z","shell.execute_reply":"2026-02-04T21:16:49.320221Z"}},"outputs":[{"name":"stdout","text":"CWD: /kaggle/working\nGGUF in /kaggle/working: ['/kaggle/working/gemma-3-1b-it.Q4_K_M.gguf']\nGGUF in output dir: []\nGGUF in output cache: []\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os, time\npath = \"/kaggle/working/gemma-3-1b-it.Q4_K_M.gguf\"\nprint(\"Exists:\", os.path.exists(path))\nprint(\"Size GB:\", os.path.getsize(path) / (1024**3))\nprint(\"Modified:\", time.ctime(os.path.getmtime(path)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:16:51.615993Z","iopub.execute_input":"2026-02-04T21:16:51.616616Z","iopub.status.idle":"2026-02-04T21:16:51.621711Z","shell.execute_reply.started":"2026-02-04T21:16:51.616577Z","shell.execute_reply":"2026-02-04T21:16:51.620986Z"}},"outputs":[{"name":"stdout","text":"Exists: True\nSize GB: 0.750700056552887\nModified: Wed Feb  4 21:16:21 2026\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import glob, os, time\n\nprint(\"Cached GGUFs:\")\nfor p in glob.glob(\"/root/.cache/huggingface/hub/**/*.gguf\", recursive=True):\n    print(p, os.path.getsize(p)/(1024**3), time.ctime(os.path.getmtime(p)))\n\nprint(\"\\nWorking dir GGUF:\")\np = \"/kaggle/working/gemma-3-1b-it.Q4_K_M.gguf\"\nprint(p, os.path.getsize(p)/(1024**3), time.ctime(os.path.getmtime(p)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:16:54.443770Z","iopub.execute_input":"2026-02-04T21:16:54.444422Z","iopub.status.idle":"2026-02-04T21:16:54.451320Z","shell.execute_reply.started":"2026-02-04T21:16:54.444385Z","shell.execute_reply":"2026-02-04T21:16:54.450565Z"}},"outputs":[{"name":"stdout","text":"Cached GGUFs:\n\nWorking dir GGUF:\n/kaggle/working/gemma-3-1b-it.Q4_K_M.gguf 0.750700056552887 Wed Feb  4 21:16:21 2026\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import shutil, os\nsrc = \"/kaggle/working/gemma-3-1b-it.Q4_K_M.gguf\"\ndst = \"/kaggle/working/gguf_output/gemma-3-1b-it.Q4_K_M.gguf\"\n\nif os.path.exists(src):\n    shutil.move(src, dst)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:17:26.200068Z","iopub.execute_input":"2026-02-04T21:17:26.200968Z","iopub.status.idle":"2026-02-04T21:17:26.205151Z","shell.execute_reply.started":"2026-02-04T21:17:26.200929Z","shell.execute_reply":"2026-02-04T21:17:26.204326Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Step 8: Clear GPU Memory Before Inference","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\n\nprint(\"ğŸ§¹ Clearing GPU memory...\")\n\n# Delete training objects\ndel model\ndel trainer\ndel tokenizer\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"\\nğŸ“Š GPU Memory After Cleanup:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv\n\nprint(\"\\nâœ… GPU memory cleared for inference\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:17:54.822053Z","iopub.execute_input":"2026-02-04T21:17:54.822367Z","iopub.status.idle":"2026-02-04T21:17:56.011180Z","shell.execute_reply.started":"2026-02-04T21:17:54.822335Z","shell.execute_reply":"2026-02-04T21:17:56.010405Z"}},"outputs":[{"name":"stdout","text":"ğŸ§¹ Clearing GPU memory...\n\nğŸ“Š GPU Memory After Cleanup:\nindex, memory.used [MiB], memory.free [MiB]\n0, 825 MiB, 14088 MiB\n1, 125 MiB, 14788 MiB\n\nâœ… GPU memory cleared for inference\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Deploy with llamatelemetry","metadata":{}},{"cell_type":"code","source":"import os\nfrom llamatelemetry.server import ServerManager\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nOUTPUT_DIR = \"/kaggle/working/gguf_output\"\ngguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".gguf\")]\n\nif not gguf_files:\n    print(\"âŒ No GGUF file found!\")\nelse:\n    gguf_path = os.path.join(OUTPUT_DIR, gguf_files[0])\n    print(f\"ğŸ“¥ Loading: {gguf_path}\")\n\n    server = ServerManager()\n    print(\"\\nğŸš€ Starting llama-server...\")\n    server.start_server(\n        model_path=gguf_path,\n        host=\"127.0.0.1\",\n        port=8090,\n        gpu_layers=99,\n        ctx_size=2048,\n        flash_attn=1,   # correct\n        timeout=90,\n        verbose=True\n    )\n\n    if server.check_server_health(timeout=60):\n        print(\"\\nâœ… Fine-tuned model deployed!\")\n        print(\"   API endpoint: http://127.0.0.1:8090\")\n    else:\n        print(\"\\nâŒ Server failed to start\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:17:59.072381Z","iopub.execute_input":"2026-02-04T21:17:59.072728Z","iopub.status.idle":"2026-02-04T21:18:02.137005Z","shell.execute_reply.started":"2026-02-04T21:17:59.072688Z","shell.execute_reply":"2026-02-04T21:18:02.136117Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¥ Loading: /kaggle/working/gguf_output/gemma-3-1b-it.Q4_K_M.gguf\n\nğŸš€ Starting llama-server...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it.Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready...... âœ“ Ready in 3.0s\n\nâœ… Fine-tuned model deployed!\n   API endpoint: http://127.0.0.1:8090\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Step 10: Test Your Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"from llamatelemetry.api.client import LlamaCppClient\n\nprint(\"=\"*70)\nprint(\"ğŸ§ª TESTING FINE-TUNED MODEL\")\nprint(\"=\"*70)\n\nclient = LlamaCppClient(\"http://127.0.0.1:8090\")  # match your server port\n\ntest_prompts = [\n    \"### Instruction:\\nExplain what machine learning is.\\n\\n### Response:\",\n    \"### Instruction:\\nWrite a short poem about coding.\\n\\n### Response:\",\n    \"### Instruction:\\nWhat are the benefits of GPU acceleration?\\n\\n### Response:\",\n]\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\nğŸ”¹ Test {i}:\")\n    print(f\"   Prompt: {prompt[:50]}...\")\n\n    response = client.complete(\n        prompt=prompt,\n        n_predict=100,\n        temperature=0.7,\n        stop=[\"###\", \"\\n\\n\"],\n    )\n\n    print(f\"   Response: {response.choices[0].text.strip()[:200]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:18:06.829163Z","iopub.execute_input":"2026-02-04T21:18:06.829953Z","iopub.status.idle":"2026-02-04T21:18:08.845727Z","shell.execute_reply.started":"2026-02-04T21:18:06.829908Z","shell.execute_reply":"2026-02-04T21:18:08.845077Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ§ª TESTING FINE-TUNED MODEL\n======================================================================\n\nğŸ”¹ Test 1:\n   Prompt: ### Instruction:\nExplain what machine learning is....\n   Response: Machine learning (ML) is a type of artificial intelligence that uses algorithms to allow computers or machines to learn from data without being explicitly programmed for each task. Machine learning al\n\nğŸ”¹ Test 2:\n   Prompt: ### Instruction:\nWrite a short poem about coding.\n...\n   Response: The keyboard clicks, the screen glows bright,\nA world of logic in the digital light.\nWith lines and functions, code begins to flow,\nBuilding worlds of dreams, both high and low.\n\nğŸ”¹ Test 3:\n   Prompt: ### Instruction:\nWhat are the benefits of GPU acce...\n   Response: GPU acceleration is the use of graphics processing units (GPUs) to speed up the calculation and display of digital imagery. It's one of the most effective ways to enhance computer performance for gami\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Step 11: Compare with Chat API","metadata":{}},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ’¬ CHAT COMPLETION TEST\")\nprint(\"=\"*70)\n\n# Test with chat format\nresponse = client.chat.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What did you learn during fine-tuning?\"}\n    ],\n    max_tokens=150,\n    temperature=0.7\n)\n\nprint(f\"\\nğŸ’¬ Response:\")\nprint(response.choices[0].message.content)\n\nprint(f\"\\nğŸ“Š Usage:\")\nprint(f\"   Prompt tokens: {response.usage.prompt_tokens}\")\nprint(f\"   Completion tokens: {response.usage.completion_tokens}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:18:12.955798Z","iopub.execute_input":"2026-02-04T21:18:12.956629Z","iopub.status.idle":"2026-02-04T21:18:14.746929Z","shell.execute_reply.started":"2026-02-04T21:18:12.956591Z","shell.execute_reply":"2026-02-04T21:18:14.746107Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¬ CHAT COMPLETION TEST\n======================================================================\n\nğŸ’¬ Response:\nOkay, let's dive into what I learned during the fine-tuning process! As an AI, my understanding and abilities are constantly evolving, but hereâ€™s a breakdown of what I gained from this iterative training process â€“ essentially, refining my responses based on new data and feedback:\n\n**1. Enhanced Understanding & Context:**\n\n*   **Longer Context Window:** Fine-tuning allows me to handle significantly larger pieces of text or conversation history. This is crucial for tasks like summarization, question answering, or role-playing where retaining context across multiple turns is vital. Previously, I was limited by a shorter \"memory\" window. Now, I can remember and reference details from much further back in the conversation or document.\n*   \n\nğŸ“Š Usage:\n   Prompt tokens: 25\n   Completion tokens: 150\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Step 12: Save Model for Later Use","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ’¾ SAVING MODEL\")\nprint(\"=\"*70)\n\n# Create a properly named copy\nOUTPUT_DIR = \"/kaggle/working/gguf_output\"\ngguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n\nif gguf_files:\n    src = os.path.join(OUTPUT_DIR, gguf_files[0])\n    dst = \"/kaggle/working/my-finetuned-model-Q4_K_M.gguf\"\n    \n    shutil.copy(src, dst)\n    \n    print(f\"\\nâœ… Model saved: {dst}\")\n    print(f\"   Size: {os.path.getsize(dst) / (1024**2):.1f} MB\")\n    print(f\"\\nğŸ’¡ To use this model later:\")\n    print(f\"   from llamatelemetry.server import ServerManager\")\n    print(f\"   server = ServerManager()\")\n    print(f\"   server.start(model_path='{dst}')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:18:26.270950Z","iopub.execute_input":"2026-02-04T21:18:26.271292Z","iopub.status.idle":"2026-02-04T21:18:26.746508Z","shell.execute_reply.started":"2026-02-04T21:18:26.271248Z","shell.execute_reply":"2026-02-04T21:18:26.745737Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¾ SAVING MODEL\n======================================================================\n\nâœ… Model saved: /kaggle/working/my-finetuned-model-Q4_K_M.gguf\n   Size: 768.7 MB\n\nğŸ’¡ To use this model later:\n   from llamatelemetry.server import ServerManager\n   server = ServerManager()\n   server.start(model_path='/kaggle/working/my-finetuned-model-Q4_K_M.gguf')\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 13: Cleanup","metadata":{}},{"cell_type":"code","source":"print(\"ğŸ›‘ Stopping server...\")\nserver.stop_server()\n\nprint(\"\\nâœ… Server stopped\")\nprint(\"\\nğŸ“Š Final GPU Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T21:18:31.103219Z","iopub.execute_input":"2026-02-04T21:18:31.103895Z","iopub.status.idle":"2026-02-04T21:18:31.849156Z","shell.execute_reply.started":"2026-02-04T21:18:31.103841Z","shell.execute_reply":"2026-02-04T21:18:31.848418Z"}},"outputs":[{"name":"stdout","text":"ğŸ›‘ Stopping server...\n\nâœ… Server stopped\n\nğŸ“Š Final GPU Status:\nindex, memory.used [MiB], memory.free [MiB]\n0, 825 MiB, 14088 MiB\n1, 125 MiB, 14788 MiB\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## ğŸ“š Summary\n\n### Complete Workflow:\n1. âœ… Installed Unsloth + llamatelemetry\n2. âœ… Loaded base model with 4-bit quantization\n3. âœ… Added LoRA adapters for efficient training\n4. âœ… Fine-tuned on custom dataset\n5. âœ… Exported to GGUF (Q4_K_M)\n6. âœ… Deployed with llamatelemetry llama-server\n7. âœ… Ran inference on fine-tuned model\n\n### Key llamatelemetry + Unsloth Integration:\n\n```python\nfrom llamatelemetry.unsloth import export_to_llamatelemetry\n\n# After Unsloth training\nexport_to_llamatelemetry(\n    model=model,\n    tokenizer=tokenizer,\n    output_path=\"my-model.gguf\",\n    quant_type=\"Q4_K_M\"\n)\n```\n\n---\n\n**Next:** [06-split-gpu-graphistry](06-split-gpu-graphistry-llamatelemetry-v0.1.0.ipynb)","metadata":{}}]}