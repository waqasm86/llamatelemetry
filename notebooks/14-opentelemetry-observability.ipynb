{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenTelemetry LLM Observability\n",
    "\n",
    "**Duration:** ~35 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook covers the full **observability stack** in llamatelemetry —\n",
    "decorator-based tracing, context managers, semantic conventions, sampling\n",
    "strategies, prompt redaction, and custom metrics.\n",
    "\n",
    "### What you'll learn\n",
    "1. Decorator-based tracing (`@trace`, `@workflow`, `@task`, `@tool`)\n",
    "2. Context managers (`span()`, `session()`, `suppress_tracing()`)\n",
    "3. LLM semantic conventions\n",
    "4. Sampling strategies\n",
    "5. Prompt redaction\n",
    "6. Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "\n",
    "import llamatelemetry\n",
    "\n",
    "# Initialize with OTLP endpoint for trace export\n",
    "llamatelemetry.init(\n",
    "    service_name=\"otel-observability\",\n",
    "    environment=\"kaggle\",\n",
    "    # otlp_endpoint=\"https://otlp-gateway-prod-us-central-0.grafana.net/otlp\",\n",
    "    # otlp_headers={\"Authorization\": \"Basic <token>\"},\n",
    ")\n",
    "print(f\"llamatelemetry {llamatelemetry.version()} — observability enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator-Based Tracing\n",
    "\n",
    "llamatelemetry provides four decorators that create spans with different semantics:\n",
    "\n",
    "| Decorator | Purpose | Span Kind |\n",
    "|-----------|---------|----------|\n",
    "| `@trace()` | General tracing | Internal |\n",
    "| `@workflow()` | Multi-step pipeline | Internal |\n",
    "| `@task()` | Single unit of work | Internal |\n",
    "| `@tool()` | External tool call | Internal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.trace(name=\"generic-operation\")\n",
    "def traced_function(x):\n",
    "    return x * 2\n",
    "\n",
    "@llamatelemetry.workflow(name=\"data-pipeline\")\n",
    "def pipeline(data):\n",
    "    cleaned = clean_data(data)\n",
    "    analyzed = analyze_data(cleaned)\n",
    "    return analyzed\n",
    "\n",
    "@llamatelemetry.task(name=\"clean-data\")\n",
    "def clean_data(data):\n",
    "    return [d.strip().lower() for d in data]\n",
    "\n",
    "@llamatelemetry.task(name=\"analyze-data\")\n",
    "def analyze_data(data):\n",
    "    return {\"count\": len(data), \"unique\": len(set(data))}\n",
    "\n",
    "@llamatelemetry.tool(name=\"external-lookup\")\n",
    "def lookup_tool(key):\n",
    "    return {\"key\": key, \"value\": f\"result-for-{key}\"}\n",
    "\n",
    "# Execute the traced pipeline\n",
    "result = pipeline([\" Hello \", \"World\", \" hello \", \"test\"])\n",
    "print(f\"Pipeline result: {result}\")\n",
    "\n",
    "tool_result = lookup_tool(\"my-key\")\n",
    "print(f\"Tool result: {tool_result}\")\n",
    "print(\"\\nTrace hierarchy: workflow → task(clean) → task(analyze)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Managers\n",
    "\n",
    "Use `span()` for inline tracing, `session()` for user/session grouping,\n",
    "and `suppress_tracing()` to skip noisy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# span() — inline tracing with custom attributes\n",
    "with llamatelemetry.span(\"data-processing\", batch_size=100, priority=\"high\") as span:\n",
    "    time.sleep(0.1)  # simulate work\n",
    "    print(\"Span: data-processing (with custom attrs)\")\n",
    "\n",
    "# session() — group spans by user session\n",
    "with llamatelemetry.session(\"user-abc-123\", user_id=\"user-42\") as session_span:\n",
    "    with llamatelemetry.span(\"user-request-1\"):\n",
    "        time.sleep(0.05)\n",
    "    with llamatelemetry.span(\"user-request-2\"):\n",
    "        time.sleep(0.05)\n",
    "    print(\"Session: user-abc-123 (2 requests grouped)\")\n",
    "\n",
    "# suppress_tracing() — skip tracing for noisy/internal operations\n",
    "with llamatelemetry.suppress_tracing():\n",
    "    # This operation will NOT be traced\n",
    "    for _ in range(100):\n",
    "        pass\n",
    "    print(\"Suppressed: 100 iterations not traced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Conventions\n",
    "\n",
    "llamatelemetry defines standardized attribute keys for LLM, GPU, and NCCL\n",
    "telemetry, following OpenTelemetry semantic convention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.semconv import keys\n",
    "from llamatelemetry.semconv.attrs import model_attrs, gpu_attrs, set_llm_attrs\n",
    "\n",
    "# View available semantic convention keys\n",
    "llm_keys = [k for k in dir(keys) if k.startswith(\"LLM_\")]\n",
    "gpu_keys = [k for k in dir(keys) if k.startswith(\"GPU_\")]\n",
    "nccl_keys = [k for k in dir(keys) if k.startswith(\"NCCL_\")]\n",
    "\n",
    "print(\"LLM Semantic Keys:\")\n",
    "for k in llm_keys:\n",
    "    print(f\"  {k} = \\\"{getattr(keys, k)}\\\"\")\n",
    "\n",
    "print(f\"\\nGPU Keys: {len(gpu_keys)} keys\")\n",
    "print(f\"NCCL Keys: {len(nccl_keys)} keys\")\n",
    "\n",
    "# Using attribute builders\n",
    "attrs = model_attrs(\"gemma-3-1b-it\", quant=\"Q4_K_M\", sha256=\"abc123\")\n",
    "print(f\"\\nModel attributes: {attrs}\")\n",
    "\n",
    "gpu_a = gpu_attrs(gpu_id=0, utilization_pct=85.0, mem_used_mb=5120)\n",
    "print(f\"GPU attributes: {gpu_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Tracing with Phases\n",
    "\n",
    "LLM inference has two distinct phases: **prefill** (prompt processing) and\n",
    "**decode** (token generation). The SDK can create child spans for each phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Traced request with timing breakdown\n",
    "@llamatelemetry.workflow(name=\"traced-inference\")\n",
    "def traced_inference(prompt):\n",
    "    with llamatelemetry.span(\"prefill\", **{keys.LLM_PHASE: \"prefill\"}):\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    with llamatelemetry.span(\"completion\", **{keys.LLM_MODEL: \"gemma-3-1b-it\"}):\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=64, temperature=0.7,\n",
    "        )\n",
    "        elapsed_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    with llamatelemetry.span(\"decode\", **{\n",
    "        keys.LLM_PHASE: \"decode\",\n",
    "        keys.LLM_TOKENS_TOTAL: resp.usage.completion_tokens,\n",
    "        keys.LLM_REQUEST_DURATION_MS: elapsed_ms,\n",
    "    }):\n",
    "        pass\n",
    "\n",
    "    return resp\n",
    "\n",
    "resp = traced_inference(\"What is OpenTelemetry?\")\n",
    "print(f\"Response: {resp.choices[0].message.content}\")\n",
    "print(f\"Tokens: {resp.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Strategies\n",
    "\n",
    "In production, you don't want to trace every request. Use sampling to control\n",
    "the volume of telemetry data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.otel import build_sampler\n",
    "\n",
    "# Available sampling strategies\n",
    "strategies = [\n",
    "    (\"always_on\",   {}, \"Trace everything — development/debugging\"),\n",
    "    (\"ratio\",       {\"ratio\": 0.1}, \"10% of requests — production\"),\n",
    "    (\"ratio\",       {\"ratio\": 0.01}, \"1% of requests — high-traffic production\"),\n",
    "]\n",
    "\n",
    "for strategy, kwargs, description in strategies:\n",
    "    sampler = build_sampler(strategy, **kwargs)\n",
    "    print(f\"  {strategy:15s} {str(kwargs):20s} — {description}\")\n",
    "\n",
    "print(\"\\nUsage: llamatelemetry.init(sampling='ratio', sampling_ratio=0.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Redaction\n",
    "\n",
    "Automatically redact sensitive information from traced prompts and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.otel import RedactionSpanProcessor\n",
    "\n",
    "# RedactionSpanProcessor replaces prompt/response content with [REDACTED]\n",
    "# when configured via init()\n",
    "print(\"Prompt redaction modes:\")\n",
    "print(\"  1. llamatelemetry.init(redact=True)\")\n",
    "print(\"     → Redacts all prompt and response content in spans\")\n",
    "print(\"\")\n",
    "print(\"  2. llamatelemetry.init(redact=True, redact_keys=['password', 'api_key'])\")\n",
    "print(\"     → Redacts specific attribute keys containing sensitive data\")\n",
    "print(\"\")\n",
    "print(\"  3. suppress_tracing() context manager\")\n",
    "print(\"     → Completely skip tracing for specific operations\")\n",
    "\n",
    "# Example: sensitive operation with suppression\n",
    "with llamatelemetry.suppress_tracing():\n",
    "    # API key handling — not traced\n",
    "    api_key = \"sk-secret-key-12345\"\n",
    "    print(f\"\\nSensitive operation completed (not traced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "\n",
    "Create custom OpenTelemetry metrics for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.otel import get_meter\n",
    "\n",
    "# Create a custom meter\n",
    "meter = get_meter(\"llm-app\")\n",
    "\n",
    "# Define custom metrics\n",
    "request_counter = meter.create_counter(\n",
    "    name=\"llm.requests.total\",\n",
    "    description=\"Total LLM requests\",\n",
    "    unit=\"requests\",\n",
    ")\n",
    "\n",
    "latency_histogram = meter.create_histogram(\n",
    "    name=\"llm.request.latency\",\n",
    "    description=\"LLM request latency\",\n",
    "    unit=\"ms\",\n",
    ")\n",
    "\n",
    "token_counter = meter.create_counter(\n",
    "    name=\"llm.tokens.generated\",\n",
    "    description=\"Total tokens generated\",\n",
    "    unit=\"tokens\",\n",
    ")\n",
    "\n",
    "# Use metrics in inference\n",
    "for i in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Question {i+1}: What is ML?\"}],\n",
    "        max_tokens=32, temperature=0.7,\n",
    "    )\n",
    "    latency_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    # Record metrics\n",
    "    request_counter.add(1, {\"model\": \"gemma-3-1b-it\", \"status\": \"success\"})\n",
    "    latency_histogram.record(latency_ms, {\"model\": \"gemma-3-1b-it\"})\n",
    "    token_counter.add(resp.usage.completion_tokens, {\"model\": \"gemma-3-1b-it\"})\n",
    "\n",
    "    print(f\"  Request {i+1}: {latency_ms:.0f} ms, {resp.usage.completion_tokens} tokens\")\n",
    "\n",
    "print(\"\\nCustom metrics recorded and will be exported with next flush.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary — Observability Best Practices\n",
    "\n",
    "1. **Use `@workflow` for pipelines**, `@task` for steps, `@tool` for external calls\n",
    "2. **Group by session** with `session()` for per-user tracking\n",
    "3. **Set semantic attributes** for standardized dashboards\n",
    "4. **Sample in production** — 1-10% is typical\n",
    "5. **Redact prompts** when tracing production user input\n",
    "6. **Custom metrics** for business-specific KPIs\n",
    "7. **Always `flush()` before shutdown** to export pending data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.flush()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
