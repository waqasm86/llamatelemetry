{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "32eed8ef",
   "cell_type": "markdown",
   "source": "## Step 1: Verify Environment and GPU Configuration\n\nValidates dual T4 GPU setup, CUDA version compatibility, and VRAM availability to ensure optimal configuration for advanced llama-server deployment scenarios.",
   "metadata": {}
  },
  {
   "id": "cbf1e6ab",
   "cell_type": "code",
   "source": "import subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"üîç ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# GPU check\nresult = subprocess.run([\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \n                         \"--format=csv,noheader\"], capture_output=True, text=True)\nprint(\"\\nüìä GPUs Available:\")\nfor line in result.stdout.strip().split('\\n'):\n    print(f\"   {line}\")\n\n# CUDA version\nprint(\"\\nüìä CUDA Version:\")\n!nvcc --version | grep release\n\nprint(\"\\n‚úÖ Environment ready for llama-server configuration\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:04:56.601039Z",
     "iopub.execute_input": "2026-02-04T05:04:56.601645Z",
     "iopub.status.idle": "2026-02-04T05:04:56.845608Z",
     "shell.execute_reply.started": "2026-02-04T05:04:56.601612Z",
     "shell.execute_reply": "2026-02-04T05:04:56.844816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüîç ENVIRONMENT CHECK\n======================================================================\n\nüìä GPUs Available:\n   0, Tesla T4, 15360 MiB, 7.5\n   1, Tesla T4, 15360 MiB, 7.5\n\nüìä CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\n‚úÖ Environment ready for llama-server configuration\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "id": "8b2c6025",
   "cell_type": "markdown",
   "source": "## Step 2: Install llamatelemetry and Dependencies\n\nInstalls llamatelemetry v0.1.0 with forced cache refresh to ensure latest binaries, plus HuggingFace Hub and SSE client for streaming support.",
   "metadata": {}
  },
  {
   "id": "5fb85191",
   "cell_type": "code",
   "source": "%%time\n# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n!pip install -q huggingface_hub sseclient-py\n\nimport llamatelemetry\nprint(f\"‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:04:59.413368Z",
     "iopub.execute_input": "2026-02-04T05:04:59.413660Z",
     "iopub.status.idle": "2026-02-04T05:07:07.631943Z",
     "shell.execute_reply.started": "2026-02-04T05:04:59.413634Z",
     "shell.execute_reply": "2026-02-04T05:07:07.631123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m255.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m278.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m339.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m331.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m328.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m340.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m383.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m268.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m368.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m309.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m319.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m318.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m343.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m333.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m385.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m289.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m376.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m248.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m314.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m314.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n======================================================================\nüéØ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(‚Ä¶):   0%|          | 0.00/1.40G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab25f9aeaafe47fe8114c47a7017fdbe"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\n‚úÖ Binaries installed successfully!\n\n‚úÖ llamatelemetry 0.1.0 installed\nCPU times: user 48 s, sys: 11.4 s, total: 59.4 s\nWall time: 2min 8s\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "d8be600c-8b08-4e9a-b674-6dcbd06ae6b8",
   "cell_type": "code",
   "source": "from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:09:12.400637Z",
     "iopub.execute_input": "2026-02-04T05:09:12.401351Z",
     "iopub.status.idle": "2026-02-04T05:09:12.624782Z",
     "shell.execute_reply.started": "2026-02-04T05:09:12.401316Z",
     "shell.execute_reply": "2026-02-04T05:09:12.624215Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "id": "63ba94af-7960-40f4-bf96-d4ec15ea1324",
   "cell_type": "code",
   "source": "from huggingface_hub import login\nimport os\n\n# Login to Hugging Face\ntry:\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face!\")\nexcept Exception as e:\n    print(f\"Error logging into Hugging Face: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:09:13.593818Z",
     "iopub.execute_input": "2026-02-04T05:09:13.594356Z",
     "iopub.status.idle": "2026-02-04T05:09:13.802098Z",
     "shell.execute_reply.started": "2026-02-04T05:09:13.594330Z",
     "shell.execute_reply": "2026-02-04T05:09:13.801584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Successfully logged into Hugging Face!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "id": "bff35470",
   "cell_type": "markdown",
   "source": "## Step 3: Explore Server Configuration Options\n\nComprehensive overview of llama-server configuration flags covering model settings, GPU allocation, performance tuning, and network parameters for production deployments.",
   "metadata": {}
  },
  {
   "id": "17b81ee0",
   "cell_type": "code",
   "source": "from llamatelemetry.server import ServerManager\nfrom llamatelemetry.api.multigpu import MultiGPUConfig, SplitMode\n\n# Display all configuration options\nprint(\"=\"*70)\nprint(\"üìã LLAMA-SERVER CONFIGURATION OPTIONS\")\nprint(\"=\"*70)\n\nconfig_options = {\n    \"Model Settings\": {\n        \"--model, -m\": \"Path to GGUF model file\",\n        \"--alias, -a\": \"Model alias for API responses\",\n        \"--ctx-size, -c\": \"Context size (default: 4096)\",\n        \"--batch-size, -b\": \"Batch size for prompt processing\",\n        \"--ubatch-size\": \"Physical batch size (default: 512)\",\n    },\n    \"GPU Settings\": {\n        \"--n-gpu-layers, -ngl\": \"Layers to offload to GPU (99 = all)\",\n        \"--main-gpu, -mg\": \"Main GPU for computations\",\n        \"--tensor-split, -ts\": \"VRAM distribution across GPUs\",\n        \"--split-mode, -sm\": \"Split mode: layer, row, none\",\n    },\n    \"Performance\": {\n        \"--flash-attn, -fa\": \"Enable FlashAttention (faster)\",\n        \"--threads, -t\": \"CPU threads for generation\",\n        \"--threads-batch, -tb\": \"CPU threads for batch processing\",\n        \"--cont-batching\": \"Enable continuous batching\",\n        \"--parallel, -np\": \"Number of parallel sequences\",\n    },\n    \"Server Settings\": {\n        \"--host\": \"Host address (default: 127.0.0.1)\",\n        \"--port\": \"Port number (default: 8080)\",\n        \"--timeout\": \"Server timeout in seconds\",\n        \"--embeddings\": \"Enable embeddings endpoint\",\n    },\n}\n\nfor category, options in config_options.items():\n    print(f\"\\nüìå {category}:\")\n    for flag, desc in options.items():\n        print(f\"   {flag:25} {desc}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:09:16.833189Z",
     "iopub.execute_input": "2026-02-04T05:09:16.833887Z",
     "iopub.status.idle": "2026-02-04T05:09:16.857789Z",
     "shell.execute_reply.started": "2026-02-04T05:09:16.833858Z",
     "shell.execute_reply": "2026-02-04T05:09:16.857255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüìã LLAMA-SERVER CONFIGURATION OPTIONS\n======================================================================\n\nüìå Model Settings:\n   --model, -m               Path to GGUF model file\n   --alias, -a               Model alias for API responses\n   --ctx-size, -c            Context size (default: 4096)\n   --batch-size, -b          Batch size for prompt processing\n   --ubatch-size             Physical batch size (default: 512)\n\nüìå GPU Settings:\n   --n-gpu-layers, -ngl      Layers to offload to GPU (99 = all)\n   --main-gpu, -mg           Main GPU for computations\n   --tensor-split, -ts       VRAM distribution across GPUs\n   --split-mode, -sm         Split mode: layer, row, none\n\nüìå Performance:\n   --flash-attn, -fa         Enable FlashAttention (faster)\n   --threads, -t             CPU threads for generation\n   --threads-batch, -tb      CPU threads for batch processing\n   --cont-batching           Enable continuous batching\n   --parallel, -np           Number of parallel sequences\n\nüìå Server Settings:\n   --host                    Host address (default: 127.0.0.1)\n   --port                    Port number (default: 8080)\n   --timeout                 Server timeout in seconds\n   --embeddings              Enable embeddings endpoint\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "id": "2b95c505",
   "cell_type": "markdown",
   "source": "## Step 4: Review Kaggle T4 Configuration Presets\n\nPre-configured settings for single T4 (15GB), dual T4 (30GB), and split-GPU modes with optimized context sizes, batch parameters, and FlashAttention settings.",
   "metadata": {}
  },
  {
   "id": "12d6d414",
   "cell_type": "code",
   "source": "# Step 4 (fixed for llamatelemetry v0.1.0)\n\nfrom llamatelemetry.api.multigpu import (\n    kaggle_t4_dual_config,\n    colab_t4_single_config,\n    auto_config,\n    detect_gpus,\n)\n\nprint(\"=\"*70)\nprint(\"üìã KAGGLE T4 CONFIGURATION PRESETS\")\nprint(\"=\"*70)\n\n# Single GPU configuration (use GPU 0 only)\nprint(\"\\nüîπ Single T4 Configuration (15GB VRAM):\")\nsingle_config = colab_t4_single_config()\nprint(f\"   GPU Layers: {single_config.n_gpu_layers}\")\nprint(f\"   Context Size: {single_config.ctx_size}\")\nprint(f\"   Batch Size: {single_config.batch_size}\")\nprint(f\"   Micro-batch Size: {single_config.ubatch_size}\")\nprint(f\"   Flash Attention: {single_config.flash_attention}\")\nprint(f\"   Split Mode: {single_config.split_mode.value}\")\nprint(\"   Best for: Models up to ~7B Q4_K_M\")\n\n# Dual GPU configuration (split across both GPUs)\nprint(\"\\nüîπ Dual T4 Configuration (30GB VRAM total):\")\ndual_config = kaggle_t4_dual_config()\nprint(f\"   GPU Layers: {dual_config.n_gpu_layers}\")\nprint(f\"   Context Size: {dual_config.ctx_size}\")\nprint(f\"   Batch Size: {dual_config.batch_size}\")\nprint(f\"   Micro-batch Size: {dual_config.ubatch_size}\")\nprint(f\"   Tensor Split: {dual_config.tensor_split}\")\nprint(f\"   Split Mode: {dual_config.split_mode.value}\")\nprint(f\"   Flash Attention: {dual_config.flash_attention}\")\nprint(\"   Best for: Models up to ~13B Q4_K_M\")\n\n# Auto-detect configuration (optional)\nprint(\"\\nüîπ Auto-Detected Configuration:\")\nauto_cfg = auto_config()\nprint(f\"   GPU Layers: {auto_cfg.n_gpu_layers}\")\nprint(f\"   Context Size: {auto_cfg.ctx_size}\")\nprint(f\"   Tensor Split: {auto_cfg.tensor_split}\")\nprint(f\"   Split Mode: {auto_cfg.split_mode.value}\")\nprint(f\"   Flash Attention: {auto_cfg.flash_attention}\")\n\n# Split GPU note\nprint(\"\\nüîπ Split-GPU Configuration (Recommended):\")\nprint(\"   GPU 0: llama-server (LLM inference)\")\nprint(\"   GPU 1: RAPIDS/Graphistry (graph processing)\")\nprint(\"   Best for: Combined LLM + visualization workflows\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:26:06.198211Z",
     "iopub.execute_input": "2026-02-04T05:26:06.198995Z",
     "iopub.status.idle": "2026-02-04T05:26:06.263460Z",
     "shell.execute_reply.started": "2026-02-04T05:26:06.198968Z",
     "shell.execute_reply": "2026-02-04T05:26:06.262651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüìã KAGGLE T4 CONFIGURATION PRESETS\n======================================================================\n\nüîπ Single T4 Configuration (15GB VRAM):\n   GPU Layers: -1\n   Context Size: 4096\n   Batch Size: 1024\n   Micro-batch Size: 256\n   Flash Attention: True\n   Split Mode: none\n   Best for: Models up to ~7B Q4_K_M\n\nüîπ Dual T4 Configuration (30GB VRAM total):\n   GPU Layers: -1\n   Context Size: 8192\n   Batch Size: 2048\n   Micro-batch Size: 512\n   Tensor Split: [0.5, 0.5]\n   Split Mode: layer\n   Flash Attention: True\n   Best for: Models up to ~13B Q4_K_M\n\nüîπ Auto-Detected Configuration:\n   GPU Layers: -1\n   Context Size: 8192\n   Tensor Split: [0.5, 0.5]\n   Split Mode: layer\n   Flash Attention: True\n\nüîπ Split-GPU Configuration (Recommended):\n   GPU 0: llama-server (LLM inference)\n   GPU 1: RAPIDS/Graphistry (graph processing)\n   Best for: Combined LLM + visualization workflows\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "da13fa6e",
   "cell_type": "markdown",
   "source": "## Step 5: Download Test Model\n\nDownloads Gemma 3-1B Instruct Q4_K_M (~750MB) for testing various server configurations with minimal VRAM requirements and fast loading times.",
   "metadata": {}
  },
  {
   "id": "83a39360",
   "cell_type": "code",
   "source": "%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# Download a small model for testing configurations\nMODEL_REPO = \"unsloth/gemma-3-1b-it-GGUF\"\nMODEL_FILE = \"gemma-3-1b-it-Q4_K_M.gguf\"\n\nprint(f\"üì• Downloading {MODEL_FILE} for configuration testing...\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\n‚úÖ Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:26:09.842773Z",
     "iopub.execute_input": "2026-02-04T05:26:09.843049Z",
     "iopub.status.idle": "2026-02-04T05:26:12.692365Z",
     "shell.execute_reply.started": "2026-02-04T05:26:09.843025Z",
     "shell.execute_reply": "2026-02-04T05:26:12.691828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üì• Downloading gemma-3-1b-it-Q4_K_M.gguf for configuration testing...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf3e0722a43a40bcb80b0161977ae461"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\n‚úÖ Model downloaded: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\n   Size: 0.75 GB\nCPU times: user 1.71 s, sys: 3.02 s, total: 4.73 s\nWall time: 2.85 s\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "id": "4c09c3a3",
   "cell_type": "markdown",
   "source": "## Step 6: Launch Server with Basic Configuration\n\nStarts llama-server with fundamental settings including GPU layer offloading, context size, and batch parameters for baseline performance testing.",
   "metadata": {}
  },
  {
   "id": "d518b25b",
   "cell_type": "code",
   "source": "from llamatelemetry.server import ServerManager\n\n# Create basic configuration settings (used as parameters to start_server)\nprint(\"=\"*70)\nprint(\"üîß BASIC SERVER CONFIGURATION\")\nprint(\"=\"*70)\n\n# Configuration parameters\nconfig = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \"gpu_layers\": 99,       # Offload all layers to GPU\n    \"ctx_size\": 4096,       # 4K context\n    \"batch_size\": 512,      # Batch size for prompt processing\n}\n\nprint(f\"\\nüìã Configuration:\")\nprint(f\"   Model: {config['model_path']}\")\nprint(f\"   Host: {config['host']}:{config['port']}\")\nprint(f\"   GPU Layers: {config['gpu_layers']}\")\nprint(f\"   Context: {config['ctx_size']}\")\n\n# Start server using ServerManager.start_server() API\nserver = ServerManager(server_url=f\"http://{config['host']}:{config['port']}\")\nprint(\"\\nüöÄ Starting server with basic configuration...\")\n\ntry:\n    server.start_server(\n        model_path=config['model_path'],\n        host=config['host'],\n        port=config['port'],\n        gpu_layers=config['gpu_layers'],\n        ctx_size=config['ctx_size'],\n        timeout=60,\n        verbose=True\n    )\n    print(\"\\n‚úÖ Server started successfully!\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Server failed to start: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:26:17.440584Z",
     "iopub.execute_input": "2026-02-04T05:26:17.441102Z",
     "iopub.status.idle": "2026-02-04T05:26:21.501648Z",
     "shell.execute_reply.started": "2026-02-04T05:26:17.441075Z",
     "shell.execute_reply": "2026-02-04T05:26:21.500818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüîß BASIC SERVER CONFIGURATION\n======================================================================\n\nüìã Configuration:\n   Model: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\n   Host: 127.0.0.1:8080\n   GPU Layers: 99\n   Context: 4096\n\nüöÄ Starting server with basic configuration...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready....... ‚úì Ready in 4.0s\n\n‚úÖ Server started successfully!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "id": "1b24f054-14c7-48bd-962f-507f9acf1145",
   "cell_type": "markdown",
   "source": "## Step 6.5: Wait for server readiness + sanity checks",
   "metadata": {}
  },
  {
   "id": "ad7f86cb-bbb2-4521-ac15-be07ec65dcdc",
   "cell_type": "code",
   "source": "# Step 6.5: Wait for readiness + quick inference sanity test\n\nimport time\nimport os\nimport requests\n\nprint(\"=\"*70)\nprint(\"‚è≥ WAIT FOR SERVER READY + SANITY TEST\")\nprint(\"=\"*70)\n\n# Confirm model file exists\nif not os.path.exists(model_path):\n    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\nbase_url = server.server_url if hasattr(server, \"server_url\") else \"http://127.0.0.1:8080\"\n\n# Poll health endpoint\nready = False\nfor i in range(60):\n    try:\n        r = requests.get(f\"{base_url}/health\", timeout=2)\n        if r.status_code == 200:\n            print(f\"‚úÖ Server ready ({i+1}s): {r.json()}\")\n            ready = True\n            break\n    except Exception:\n        pass\n    time.sleep(1)\n\nif not ready:\n    print(\"‚ùå Server not ready after 60s\")\n    info = server.get_server_info() if hasattr(server, \"get_server_info\") else {}\n    print(f\"Server info: {info}\")\n    if getattr(server, \"server_process\", None) is not None:\n        print(f\"Process running: {server.server_process.poll() is None}\")\n    raise RuntimeError(\"llama-server did not become ready\")\n\n# Quick props check\ntry:\n    props = requests.get(f\"{base_url}/props\", timeout=5).json()\n    print(f\"\\nüìå Loaded model path: {props.get('model_path', 'N/A')}\")\n    print(f\"üìå Context size: {props.get('default_generation_settings', {}).get('n_ctx', 'N/A')}\")\nexcept Exception as e:\n    print(f\"Props check skipped: {e}\")\n\n# Quick inference sanity test\nprint(\"\\nüß™ Running quick test completion...\")\npayload = {\n    \"prompt\": \"Hello! In one sentence, what is llamatelemetry?\",\n    \"n_predict\": 40,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"stream\": False\n}\n\ntry:\n    resp = requests.post(f\"{base_url}/completion\", json=payload, timeout=60)\n    resp.raise_for_status()\n    data = resp.json()\n    print(\"‚úÖ Completion OK\")\n    print(\"Response:\", data.get(\"content\", \"\").strip())\nexcept Exception as e:\n    print(f\"‚ùå Completion failed: {e}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:36:30.516181Z",
     "iopub.execute_input": "2026-02-04T05:36:30.517068Z",
     "iopub.status.idle": "2026-02-04T05:36:31.022106Z",
     "shell.execute_reply.started": "2026-02-04T05:36:30.517028Z",
     "shell.execute_reply": "2026-02-04T05:36:31.021546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\n‚è≥ WAIT FOR SERVER READY + SANITY TEST\n======================================================================\n‚úÖ Server ready (1s): {'status': 'ok'}\n\nüìå Loaded model path: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\nüìå Context size: 4096\n\nüß™ Running quick test completion...\n‚úÖ Completion OK\nResponse: Llamatelemetry is a process of analyzing and extracting insights from a company's call center data to improve its customer service processes.\n\nDo you want to learn more about this topic?\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "id": "09a6716c-5d40-47f6-a521-62e297341fab",
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "68f127a0",
   "cell_type": "markdown",
   "source": "## Step 7: Monitor Server Health and Status\n\nChecks server health endpoint, model properties, and slot availability to verify successful startup and readiness for inference requests.",
   "metadata": {}
  },
  {
   "id": "3037817f",
   "cell_type": "code",
   "source": "# Step 7 (updated): Server Health Monitoring + Live Slot Activity\n\nimport time\nimport requests\n\nprint(\"=\"*70)\nprint(\"üè• SERVER HEALTH MONITORING\")\nprint(\"=\"*70)\n\nbase_url = \"http://127.0.0.1:8080\"\n\ndef safe_json(resp):\n    try:\n        return resp.json()\n    except Exception:\n        return resp.text\n\n# 1) Health check\ntry:\n    health = requests.get(f\"{base_url}/health\", timeout=5)\n    print(f\"\\nüìä Health Status ({health.status_code}): {safe_json(health)}\")\nexcept Exception as e:\n    print(f\"‚ùå Health check failed: {e}\")\n\n# 2) Model properties\ntry:\n    props = requests.get(f\"{base_url}/props\", timeout=5)\n    data = safe_json(props)\n    print(f\"\\nüìä Model Properties ({props.status_code}):\")\n    if isinstance(data, dict):\n        print(f\"   Model path: {data.get('model_path', 'N/A')}\")\n        print(f\"   Context: {data.get('default_generation_settings', {}).get('n_ctx', 'N/A')}\")\n        print(f\"   Total slots: {data.get('total_slots', 'N/A')}\")\n    else:\n        print(f\"   Raw: {data}\")\nexcept Exception as e:\n    print(f\"   Props endpoint not available: {e}\")\n\n# 3) Slots (correct fields)\ndef print_slots(label=\"Server Slots\"):\n    try:\n        slots = requests.get(f\"{base_url}/slots\", timeout=5).json()\n        print(f\"\\nüìä {label}:\")\n        for slot in slots:\n            slot_id = slot.get(\"id\", \"N/A\")\n            is_processing = slot.get(\"is_processing\", \"N/A\")\n            n_ctx = slot.get(\"n_ctx\", \"N/A\")\n            n_predict = slot.get(\"n_predict\", \"N/A\")\n            print(f\"   Slot {slot_id}: processing={is_processing}, n_ctx={n_ctx}, n_predict={n_predict}\")\n    except Exception as e:\n        print(f\"   Slots endpoint not available: {e}\")\n\nprint_slots(\"Server Slots (Idle Check)\")\n\n# 4) Live slot activity check\nprint(\"\\nüß™ Live slot activity test (short completion)...\")\npayload = {\n    \"prompt\": \"Say hello in one short sentence.\",\n    \"n_predict\": 20,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"stream\": False\n}\n\ntry:\n    # Fire request and check slots while running\n    req = requests.post(f\"{base_url}/completion\", json=payload, timeout=60)\n    req.raise_for_status()\n    time.sleep(0.5)\n    print_slots(\"Server Slots (After Test Completion)\")\n    print(\"‚úÖ Test completion OK\")\nexcept Exception as e:\n    print(f\"‚ùå Test completion failed: {e}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:40:15.382624Z",
     "iopub.execute_input": "2026-02-04T05:40:15.383157Z",
     "iopub.status.idle": "2026-02-04T05:40:20.397263Z",
     "shell.execute_reply.started": "2026-02-04T05:40:15.383129Z",
     "shell.execute_reply": "2026-02-04T05:40:20.396535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüè• SERVER HEALTH MONITORING\n======================================================================\n\nüìä Health Status (200): {'status': 'ok'}\n\nüìä Model Properties (200):\n   Model path: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\n   Context: 4096\n   Total slots: 1\n\nüìä Server Slots (Idle Check):\n   Slot 0: processing=False, n_ctx=4096, n_predict=N/A\n\nüß™ Live slot activity test (short completion)...\n\nüìä Server Slots (After Test Completion):\n   Slot 0: processing=False, n_ctx=4096, n_predict=N/A\n‚úÖ Test completion OK\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "id": "8d0968c3",
   "cell_type": "markdown",
   "source": "## Step 8: Stop Server for Reconfiguration\n\nGracefully stops current server instance and waits for port release to prepare for testing advanced configuration scenarios.",
   "metadata": {}
  },
  {
   "id": "1b1a5cd1",
   "cell_type": "code",
   "source": "# Stop current server\nprint(\"üõë Stopping current server...\")\nserver.stop_server()\n\nimport time\ntime.sleep(2)  # Wait for port to be released\n\nprint(\"\\n‚úÖ Server stopped\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:40:38.466274Z",
     "iopub.execute_input": "2026-02-04T05:40:38.467007Z",
     "iopub.status.idle": "2026-02-04T05:40:41.587961Z",
     "shell.execute_reply.started": "2026-02-04T05:40:38.466979Z",
     "shell.execute_reply": "2026-02-04T05:40:41.587336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üõë Stopping current server...\n\n‚úÖ Server stopped\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "id": "54b06ed1",
   "cell_type": "markdown",
   "source": "## Step 9: Deploy High-Performance Configuration\n\nLaunches server with optimized settings for maximum throughput including larger context, increased batch size, and parallel slot configuration for concurrent requests.",
   "metadata": {}
  },
  {
   "id": "d8b3ca62",
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"‚ö° HIGH-PERFORMANCE CONFIGURATION\")\nprint(\"=\"*70)\n\n# High-performance configuration parameters\nhp_config = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \n    # GPU settings - maximize GPU utilization\n    \"gpu_layers\": 99,\n    \n    # Context and batching\n    \"ctx_size\": 8192,      # Larger context\n    \"batch_size\": 1024,    # Larger batch for prompt processing\n    \"ubatch_size\": 512,    # Physical batch size\n    \n    # Parallelism\n    \"n_parallel\": 4,       # 4 parallel request slots\n}\n\nprint(f\"\\nüìã High-Performance Settings:\")\nprint(f\"   Context Size: {hp_config['ctx_size']} tokens\")\nprint(f\"   Batch Size: {hp_config['batch_size']}\")\nprint(f\"   Parallel Slots: {hp_config['n_parallel']}\")\n\n# Create new server manager\nserver = ServerManager(server_url=f\"http://{hp_config['host']}:{hp_config['port']}\")\n\n# Start with high-performance config\nprint(\"\\nüöÄ Starting server with high-performance configuration...\")\ntry:\n    server.start_server(\n        model_path=hp_config['model_path'],\n        host=hp_config['host'],\n        port=hp_config['port'],\n        gpu_layers=hp_config['gpu_layers'],\n        ctx_size=hp_config['ctx_size'],\n        batch_size=hp_config['batch_size'],\n        ubatch_size=hp_config['ubatch_size'],\n        n_parallel=hp_config['n_parallel'],\n        timeout=60,\n        verbose=True\n    )\n    print(\"\\n‚úÖ High-performance server started!\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Server failed to start: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:40:43.932445Z",
     "iopub.execute_input": "2026-02-04T05:40:43.932717Z",
     "iopub.status.idle": "2026-02-04T05:40:46.992333Z",
     "shell.execute_reply.started": "2026-02-04T05:40:43.932694Z",
     "shell.execute_reply": "2026-02-04T05:40:46.991723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\n‚ö° HIGH-PERFORMANCE CONFIGURATION\n======================================================================\n\nüìã High-Performance Settings:\n   Context Size: 8192 tokens\n   Batch Size: 1024\n   Parallel Slots: 4\n\nüöÄ Starting server with high-performance configuration...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n\n‚úÖ High-performance server started!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 17
  },
  {
   "id": "12a2acb0",
   "cell_type": "markdown",
   "source": "## Step 10: Benchmark Inference Performance\n\nRuns multiple test prompts to measure tokens-per-second generation speed, validating performance improvements from optimized configuration settings.",
   "metadata": {}
  },
  {
   "id": "645fd1ec",
   "cell_type": "code",
   "source": "import time\nfrom llamatelemetry.api.client import LlamaCppClient\n\nprint(\"=\"*70)\nprint(\"üìä INFERENCE PERFORMANCE BENCHMARK\")\nprint(\"=\"*70)\n\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n\n# Benchmark parameters\nprompts = [\n    \"Explain quantum computing in simple terms.\",\n    \"Write a haiku about machine learning.\",\n    \"What are the benefits of GPU acceleration?\",\n]\n\nprint(\"\\nüèÉ Running benchmark with 3 prompts...\\n\")\n\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, prompt in enumerate(prompts, 1):\n    start = time.time()\n    \n    response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=100,\n        temperature=0.7\n    )\n    \n    elapsed = time.time() - start\n    tokens = response.usage.completion_tokens\n    \n    total_tokens += tokens\n    total_time += elapsed\n    \n    print(f\"   Prompt {i}: {tokens} tokens in {elapsed:.2f}s ({tokens/elapsed:.1f} tok/s)\")\n\nprint(f\"\\nüìä Benchmark Results:\")\nprint(f\"   Total Tokens: {total_tokens}\")\nprint(f\"   Total Time: {total_time:.2f}s\")\nprint(f\"   Average Speed: {total_tokens/total_time:.1f} tokens/second\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:40:51.284248Z",
     "iopub.execute_input": "2026-02-04T05:40:51.284847Z",
     "iopub.status.idle": "2026-02-04T05:40:54.693299Z",
     "shell.execute_reply.started": "2026-02-04T05:40:51.284822Z",
     "shell.execute_reply": "2026-02-04T05:40:54.692723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüìä INFERENCE PERFORMANCE BENCHMARK\n======================================================================\n\nüèÉ Running benchmark with 3 prompts...\n\n   Prompt 1: 100 tokens in 1.58s (63.3 tok/s)\n   Prompt 2: 20 tokens in 0.32s (62.9 tok/s)\n   Prompt 3: 100 tokens in 1.50s (66.5 tok/s)\n\nüìä Benchmark Results:\n   Total Tokens: 220\n   Total Time: 3.40s\n   Average Speed: 64.7 tokens/second\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "id": "6538c39c",
   "cell_type": "markdown",
   "source": "## Step 11: Monitor GPU Memory Allocation\n\nTracks VRAM usage across both GPUs using nvidia-smi to understand memory footprint and validate efficient resource utilization.",
   "metadata": {}
  },
  {
   "id": "c72be8f0",
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"üìä GPU MEMORY MONITORING\")\nprint(\"=\"*70)\n\n# Current memory usage\nprint(\"\\nüîπ Current GPU Memory Usage:\")\n!nvidia-smi --query-gpu=index,name,memory.used,memory.total,memory.free --format=csv\n\n# Memory usage over time (single snapshot)\nimport subprocess\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,memory.used,utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\nprint(\"\\nüîπ GPU Utilization:\")\nfor line in result.stdout.strip().split('\\n'):\n    parts = line.split(', ')\n    if len(parts) >= 3:\n        print(f\"   GPU {parts[0]}: {parts[1]} used, {parts[2]} utilization\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:41:11.982036Z",
     "iopub.execute_input": "2026-02-04T05:41:11.982725Z",
     "iopub.status.idle": "2026-02-04T05:41:12.179541Z",
     "shell.execute_reply.started": "2026-02-04T05:41:11.982695Z",
     "shell.execute_reply": "2026-02-04T05:41:12.178827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüìä GPU MEMORY MONITORING\n======================================================================\n\nüîπ Current GPU Memory Usage:\nThe history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\nindex, name, memory.used [MiB], memory.total [MiB], memory.free [MiB]\n0, Tesla T4, 509 MiB, 15360 MiB, 14404 MiB\n1, Tesla T4, 1229 MiB, 15360 MiB, 13684 MiB\n\nüîπ GPU Utilization:\n   GPU 0: 509 MiB used, 0 % utilization\n   GPU 1: 1229 MiB used, 0 % utilization\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 19
  },
  {
   "id": "f54554a7",
   "cell_type": "markdown",
   "source": "## Step 12: Command-Line Reference Guide\n\nProvides comprehensive CLI examples for running llama-server directly with various configurations including single GPU, dual GPU, high-performance, and embeddings mode.",
   "metadata": {}
  },
  {
   "id": "0de3bfdb",
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"üìã COMMAND-LINE REFERENCE\")\nprint(\"=\"*70)\n\ncli_examples = f\"\"\"\nüîπ Basic Start:\n   llama-server -m {model_path} --host 0.0.0.0 --port 8080\n\nüîπ Single GPU (GPU 0, 15GB):\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 --main-gpu 0 \\\\\n       --ctx-size 4096 --flash-attn\n\nüîπ Dual GPU (30GB total):\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 \\\\\n       --tensor-split 0.5,0.5 \\\\\n       --split-mode layer \\\\\n       --ctx-size 8192 --flash-attn\n\nüîπ High-Performance:\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 --flash-attn \\\\\n       --ctx-size 8192 --batch-size 1024 \\\\\n       --parallel 4 --cont-batching \\\\\n       --threads 4 --threads-batch 4\n\nüîπ With Embeddings:\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 --flash-attn \\\\\n       --embeddings\n\"\"\"\n\nprint(cli_examples)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:41:18.726796Z",
     "iopub.execute_input": "2026-02-04T05:41:18.727105Z",
     "iopub.status.idle": "2026-02-04T05:41:18.732485Z",
     "shell.execute_reply.started": "2026-02-04T05:41:18.727074Z",
     "shell.execute_reply": "2026-02-04T05:41:18.731889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüìã COMMAND-LINE REFERENCE\n======================================================================\n\nüîπ Basic Start:\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf --host 0.0.0.0 --port 8080\n\nüîπ Single GPU (GPU 0, 15GB):\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 --main-gpu 0 \\\n       --ctx-size 4096 --flash-attn\n\nüîπ Dual GPU (30GB total):\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 \\\n       --tensor-split 0.5,0.5 \\\n       --split-mode layer \\\n       --ctx-size 8192 --flash-attn\n\nüîπ High-Performance:\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 --flash-attn \\\n       --ctx-size 8192 --batch-size 1024 \\\n       --parallel 4 --cont-batching \\\n       --threads 4 --threads-batch 4\n\nüîπ With Embeddings:\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 --flash-attn \\\n       --embeddings\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 20
  },
  {
   "id": "4b8bc155",
   "cell_type": "markdown",
   "source": "## Step 13: Cleanup and Resource Release\n\nStops server, releases GPU memory, and verifies clean shutdown with final GPU status check for resource cleanup validation.",
   "metadata": {}
  },
  {
   "id": "58236462",
   "cell_type": "code",
   "source": "# Stop server\nprint(\"üõë Stopping server...\")\nserver.stop_server()\n\nprint(\"\\n‚úÖ Server stopped. Resources freed.\")\n\n# Final GPU status\nprint(\"\\nüìä Final GPU Memory Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:41:23.916988Z",
     "iopub.execute_input": "2026-02-04T05:41:23.917551Z",
     "iopub.status.idle": "2026-02-04T05:41:24.490253Z",
     "shell.execute_reply.started": "2026-02-04T05:41:23.917523Z",
     "shell.execute_reply": "2026-02-04T05:41:24.489590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üõë Stopping server...\n\n‚úÖ Server stopped. Resources freed.\n\nüìä Final GPU Memory Status:\nindex, memory.used [MiB], memory.free [MiB]\n0, 0 MiB, 14913 MiB\n1, 0 MiB, 14913 MiB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 21
  },
  {
   "id": "9ac5776e",
   "cell_type": "markdown",
   "source": "## üìö Summary\n\nYou've learned:\n1. ‚úÖ Server configuration options\n2. ‚úÖ Kaggle T4 presets (single/dual GPU)\n3. ‚úÖ High-performance tuning\n4. ‚úÖ Health monitoring\n5. ‚úÖ Command-line reference\n\n## Configuration Tips for Kaggle T4\n\n| Model Size | Quantization | VRAM | Context | Config |\n|------------|--------------|------|---------|--------|\n| 1-3B | Q4_K_M | ~2GB | 8192 | Single T4 |\n| 4-7B | Q4_K_M | ~5GB | 4096 | Single T4 |\n| 8-13B | Q4_K_M | ~8GB | 4096 | Dual T4 |\n| 13-30B | IQ3_XS | ~12GB | 2048 | Dual T4 |\n\n---\n\n**Next:** [03-multi-gpu-inference](03-multi-gpu-inference-llamatelemetry-v0.1.0.ipynb)",
   "metadata": {}
  }
 ]
}