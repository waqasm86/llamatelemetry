{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 15: Real-Time Inference Monitoring & Performance Analysis\n",
    "\n",
    "**Live Performance Dashboards with llama.cpp Metrics + Plotly**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives Demonstrated\n",
    "\n",
    "‚úÖ **CUDA Inference** (GPU 0) - Continuous inference workload\n",
    "\n",
    "‚úÖ **LLM Observability** (GPU 0) - llama.cpp /metrics endpoint + CUDA monitoring\n",
    "\n",
    "‚úÖ **Visualizations** (GPU 1) - Real-time Plotly dashboards with live updates\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **real-time performance monitoring** of LLM inference by continuously polling llama.cpp's built-in `/metrics` endpoint and NVIDIA's GPU metrics, then visualizing them as live-updating Plotly dashboards on GPU 1.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Access llama.cpp's Prometheus `/metrics` endpoint\n",
    "- Monitor GPU utilization with `nvidia-smi` and `pynvml`\n",
    "- Poll llama.cpp `/slots` endpoint for request queue monitoring\n",
    "- Create live-updating Plotly dashboards with `plotly.graph_objects.FigureWidget`\n",
    "- Identify performance bottlenecks and optimization opportunities\n",
    "- Benchmark different configurations (batch size, context length, etc.)\n",
    "\n",
    "**Time:** 30 minutes\n",
    "\n",
    "**Difficulty:** Intermediate-Advanced\n",
    "\n",
    "**VRAM:** GPU 0: 5-8 GB, GPU 1: 1-2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 1: Verify Dual GPU Environment\n",
    "# ==============================================================================\n",
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç SPLIT-GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpus = result.stdout.strip().split('\\n')\n",
    "print(f\"\\nüìä Detected {len(gpus)} GPU(s):\")\n",
    "for gpu in gpus:\n",
    "    print(f\"   {gpu}\")\n",
    "\n",
    "if len(gpus) >= 2:\n",
    "    print(\"\\n‚úÖ Dual T4 ready for split-GPU operation!\")\n",
    "    print(\"   GPU 0 ‚Üí llama-server (GGUF model inference)\")\n",
    "    print(\"   GPU 1 ‚Üí Real-time dashboards (Plotly)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Need 2 GPUs for split operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 2: Install llamatelemetry v0.1.0\n",
    "# ==============================================================================\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Install llamatelemetry v0.1.0\n",
    "!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n",
    "\n",
    "# Install monitoring packages\n",
    "!pip install -q plotly pandas numpy pynvml requests\n",
    "\n",
    "# Verify installations\n",
    "import llamatelemetry\n",
    "print(f\"\\n‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")\n",
    "\n",
    "try:\n",
    "    import plotly\n",
    "    print(f\"‚úÖ Plotly {plotly.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Plotly: {e}\")\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    print(f\"‚úÖ PyNVML installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è PyNVML: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Start Instrumented Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 3: Download GGUF Model\n",
    "# ==============================================================================\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
    "\n",
    "# Download model\n",
    "print(\"Downloading model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Qwen2.5-3B-Instruct-GGUF\",\n",
    "    filename=\"Qwen2.5-3B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=\"/kaggle/working/models\",\n",
    ")\n",
    "print(f\"‚úì Model downloaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 4: Start Server with Metrics Enabled\n",
    "# ==============================================================================\n",
    "from llamatelemetry.server import ServerManager\n",
    "import torch\n",
    "\n",
    "# Check GPUs\n",
    "print(f\"\\nFound {torch.cuda.device_count()} GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Start server with metrics enabled\n",
    "server = ServerManager(server_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "server.start_server(\n",
    "    model_path=model_path,\n",
    "    gpu_layers=99,\n",
    "    tensor_split=\"1.0,0.0\",  # GPU 0 only\n",
    "    flash_attn=1,\n",
    "    port=8090,\n",
    "    host=\"127.0.0.1\",\n",
    "    ctx_size=4096,\n",
    "    batch_size=512,\n",
    "    # Enable metrics endpoint\n",
    "    extra_args=[\"--metrics\"],\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Server running on http://127.0.0.1:8090\")\n",
    "print(\"‚úì GPU 0: Used for LLM\")\n",
    "print(\"‚úì GPU 1: Free for visualizations\")\n",
    "print(\"‚úì Metrics endpoint: /metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Metrics Collection Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 5: Define Metrics Collector\n",
    "# ==============================================================================\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "\n",
    "class LlamaMetricsCollector:\n",
    "    \"\"\"Collects metrics from llama.cpp server endpoints\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str = \"http://127.0.0.1:8090\"):\n",
    "        self.base_url = base_url\n",
    "        self.metrics_history = defaultdict(list)\n",
    "        self.slots_history = []\n",
    "        self.gpu_metrics_history = []\n",
    "        self.timestamps = []\n",
    "        self.running = False\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def parse_prometheus_metrics(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Parse Prometheus-format metrics from /metrics endpoint\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Parse metric lines (format: metric_name{labels} value)\n",
    "        for line in text.split(\"\\n\"):\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "\n",
    "            # Simple parsing (handles metrics without labels)\n",
    "            match = re.match(r\"(\\w+)\\s+([\\d.]+)\", line)\n",
    "            if match:\n",
    "                metric_name, value = match.groups()\n",
    "                metrics[metric_name] = float(value)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def fetch_server_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Fetch metrics from /metrics endpoint\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/metrics\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                return self.parse_prometheus_metrics(response.text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching metrics: {e}\")\n",
    "        return {}\n",
    "\n",
    "    def fetch_slots_info(self) -> List[Dict]:\n",
    "        \"\"\"Fetch slot information from /slots endpoint\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/slots\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching slots: {e}\")\n",
    "        return []\n",
    "\n",
    "    def fetch_gpu_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Fetch GPU metrics using pynvml\"\"\"\n",
    "        try:\n",
    "            import pynvml\n",
    "\n",
    "            # Initialize NVML (if not already done)\n",
    "            try:\n",
    "                pynvml.nvmlInit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Get GPU 0 handle\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "            # Query metrics\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "            power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # mW to W\n",
    "\n",
    "            return {\n",
    "                \"gpu_utilization\": utilization.gpu,  # %\n",
    "                \"memory_utilization\": utilization.memory,  # %\n",
    "                \"memory_used_mb\": memory_info.used / 1024**2,  # bytes to MB\n",
    "                \"memory_total_mb\": memory_info.total / 1024**2,\n",
    "                \"temperature_c\": temperature,\n",
    "                \"power_draw_w\": power_draw,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching GPU metrics: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def collect_once(self):\n",
    "        \"\"\"Collect all metrics at current timestamp\"\"\"\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Fetch from all sources\n",
    "        server_metrics = self.fetch_server_metrics()\n",
    "        slots_info = self.fetch_slots_info()\n",
    "        gpu_metrics = self.fetch_gpu_metrics()\n",
    "\n",
    "        # Store with lock\n",
    "        with self.lock:\n",
    "            self.timestamps.append(timestamp)\n",
    "\n",
    "            # Store server metrics\n",
    "            for key, value in server_metrics.items():\n",
    "                self.metrics_history[key].append(value)\n",
    "\n",
    "            # Store slots info\n",
    "            self.slots_history.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"slots\": slots_info,\n",
    "                \"num_processing\": sum(1 for s in slots_info if s.get(\"is_processing\", False)),\n",
    "                \"num_idle\": sum(1 for s in slots_info if not s.get(\"is_processing\", False)),\n",
    "            })\n",
    "\n",
    "            # Store GPU metrics\n",
    "            gpu_record = {\"timestamp\": timestamp, **gpu_metrics}\n",
    "            self.gpu_metrics_history.append(gpu_record)\n",
    "\n",
    "    def start_background_collection(self, interval: float = 1.0):\n",
    "        \"\"\"Start background thread for continuous collection\"\"\"\n",
    "        self.running = True\n",
    "\n",
    "        def collect_loop():\n",
    "            while self.running:\n",
    "                self.collect_once()\n",
    "                time.sleep(interval)\n",
    "\n",
    "        thread = threading.Thread(target=collect_loop, daemon=True)\n",
    "        thread.start()\n",
    "        print(f\"üìä Started metrics collection (interval={interval}s)\")\n",
    "\n",
    "    def stop_background_collection(self):\n",
    "        \"\"\"Stop background collection\"\"\"\n",
    "        self.running = False\n",
    "        print(\"‚èπÔ∏è Stopped metrics collection\")\n",
    "\n",
    "    def get_dataframe(self, metric_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Get metric history as pandas DataFrame\"\"\"\n",
    "        with self.lock:\n",
    "            if metric_name not in self.metrics_history:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            return pd.DataFrame({\n",
    "                \"timestamp\": pd.to_datetime(self.timestamps, unit=\"s\"),\n",
    "                \"value\": self.metrics_history[metric_name],\n",
    "            })\n",
    "\n",
    "    def get_gpu_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Get GPU metrics history as DataFrame\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.gpu_metrics_history:\n",
    "                return pd.DataFrame()\n",
    "            return pd.DataFrame(self.gpu_metrics_history)\n",
    "\n",
    "# Initialize collector\n",
    "collector = LlamaMetricsCollector()\n",
    "print(\"‚úÖ Metrics collector initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 6: Test Metrics Collection\n",
    "# ==============================================================================\n",
    "# Test single collection\n",
    "collector.collect_once()\n",
    "\n",
    "print(\"\\nüìä Server Metrics:\")\n",
    "for key in list(collector.metrics_history.keys())[:10]:\n",
    "    print(f\"  {key}: {collector.metrics_history[key][-1]}\")\n",
    "\n",
    "print(\"\\nüé∞ Slots Info:\")\n",
    "if collector.slots_history:\n",
    "    latest = collector.slots_history[-1]\n",
    "    print(f\"  Processing: {latest['num_processing']}\")\n",
    "    print(f\"  Idle: {latest['num_idle']}\")\n",
    "\n",
    "print(\"\\nüñ•Ô∏è GPU Metrics:\")\n",
    "if collector.gpu_metrics_history:\n",
    "    latest = collector.gpu_metrics_history[-1]\n",
    "    for key, value in latest.items():\n",
    "        if key != \"timestamp\":\n",
    "            print(f\"  {key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 7: Start Background Collection\n",
    "# ==============================================================================\n",
    "# Start collecting metrics in background\n",
    "collector.start_background_collection(interval=1.0)\n",
    "\n",
    "# Let it collect for a few seconds\n",
    "time.sleep(5)\n",
    "\n",
    "print(f\"üìà Collected {len(collector.timestamps)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Generate Continuous Inference Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 8: Define Load Generator\n",
    "# ==============================================================================\n",
    "from llamatelemetry.api import LlamaCppClient\n",
    "import random\n",
    "\n",
    "class InferenceLoadGenerator:\n",
    "    \"\"\"Generates continuous inference requests\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, prompts: List[str]):\n",
    "        self.client = LlamaCppClient(base_url)\n",
    "        self.prompts = prompts\n",
    "        self.running = False\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def generate_request(self):\n",
    "        \"\"\"Generate single inference request\"\"\"\n",
    "        try:\n",
    "            prompt = random.choice(self.prompts)\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=random.randint(50, 150),\n",
    "                temperature=random.uniform(0.5, 0.9),\n",
    "            )\n",
    "\n",
    "            with self.lock:\n",
    "                self.request_count += 1\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.lock:\n",
    "                self.error_count += 1\n",
    "            print(f\"‚ùå Request error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def start_continuous_load(self, qps: float = 2.0):\n",
    "        \"\"\"Start generating continuous load at specified QPS\"\"\"\n",
    "        self.running = True\n",
    "\n",
    "        def load_loop():\n",
    "            interval = 1.0 / qps\n",
    "            while self.running:\n",
    "                self.generate_request()\n",
    "                time.sleep(interval)\n",
    "\n",
    "        thread = threading.Thread(target=load_loop, daemon=True)\n",
    "        thread.start()\n",
    "        print(f\"üöÄ Started load generation (QPS={qps})\")\n",
    "\n",
    "    def stop_continuous_load(self):\n",
    "        \"\"\"Stop load generation\"\"\"\n",
    "        self.running = False\n",
    "        print(f\"‚èπÔ∏è Stopped load generation (Total: {self.request_count}, Errors: {self.error_count})\")\n",
    "\n",
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Explain how CUDA kernels work\",\n",
    "    \"What is quantization in neural networks?\",\n",
    "    \"Describe the transformer architecture\",\n",
    "    \"How does attention mechanism work?\",\n",
    "    \"What are the benefits of GGUF format?\",\n",
    "    \"Explain FlashAttention optimization\",\n",
    "    \"What is tensor parallelism?\",\n",
    "    \"How does KV cache improve inference?\",\n",
    "    \"Describe NCCL in distributed training\",\n",
    "    \"What is mixed precision training?\",\n",
    "]\n",
    "\n",
    "# Initialize load generator\n",
    "load_gen = InferenceLoadGenerator(\"http://127.0.0.1:8090\", test_prompts)\n",
    "print(\"‚úÖ Load generator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 9: Start Generating Load\n",
    "# ==============================================================================\n",
    "load_gen.start_continuous_load(qps=2.0)  # 2 requests per second\n",
    "\n",
    "# Let it run for a bit\n",
    "time.sleep(10)\n",
    "\n",
    "print(f\"üìä Requests sent: {load_gen.request_count}\")\n",
    "print(f\"‚ùå Errors: {load_gen.error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Live Plotly Dashboards (GPU 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 10: Switch to GPU 1\n",
    "# ==============================================================================\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(\"üîÑ Switched to GPU 1 for visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 11: Create Live Dashboard with Plotly FigureWidget\n",
    "# ==============================================================================\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Token Generation Rate (tokens/sec)\",\n",
    "        \"GPU Utilization (%)\",\n",
    "        \"Request Processing Time (ms)\",\n",
    "        \"GPU Memory Usage (MB)\",\n",
    "        \"Active Slots\",\n",
    "        \"GPU Temperature (¬∞C) & Power (W)\"\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]\n",
    "    ],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.15,\n",
    ")\n",
    "\n",
    "# Initialize traces\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines\", name=\"Tokens/sec\", line=dict(color=\"green\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines\", name=\"GPU %\", line=dict(color=\"blue\")), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines\", name=\"Latency\", line=dict(color=\"orange\")), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines\", name=\"Memory MB\", line=dict(color=\"red\")), row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Active\", line=dict(color=\"purple\")), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines\", name=\"Temp ¬∞C\", line=dict(color=\"darkred\")), row=3, col=2)\n",
    "fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines\", name=\"Power W\", line=dict(color=\"darkorange\")), row=3, col=2)\n",
    "\n",
    "# Configure layout\n",
    "fig.update_layout(\n",
    "    title_text=\"üî¥ LIVE LLM Performance Dashboard\",\n",
    "    showlegend=True,\n",
    "    height=900,\n",
    ")\n",
    "\n",
    "# Create FigureWidget for live updates\n",
    "fig_widget = go.FigureWidget(fig)\n",
    "display(fig_widget)\n",
    "print(\"‚úÖ Live dashboard created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 12: Dashboard Update Loop\n",
    "# ==============================================================================\n",
    "from datetime import datetime\n",
    "\n",
    "def update_dashboard():\n",
    "    \"\"\"Update dashboard with latest metrics\"\"\"\n",
    "\n",
    "    # Get GPU metrics\n",
    "    df_gpu = collector.get_gpu_dataframe()\n",
    "    if not df_gpu.empty:\n",
    "        timestamps = pd.to_datetime(df_gpu[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "        # Update GPU utilization\n",
    "        with fig_widget.batch_update():\n",
    "            fig_widget.data[1].x = timestamps\n",
    "            fig_widget.data[1].y = df_gpu[\"gpu_utilization\"]\n",
    "\n",
    "            # Update GPU memory\n",
    "            fig_widget.data[3].x = timestamps\n",
    "            fig_widget.data[3].y = df_gpu[\"memory_used_mb\"]\n",
    "\n",
    "            # Update temperature and power\n",
    "            fig_widget.data[5].x = timestamps\n",
    "            fig_widget.data[5].y = df_gpu[\"temperature_c\"]\n",
    "            fig_widget.data[6].x = timestamps\n",
    "            fig_widget.data[6].y = df_gpu[\"power_draw_w\"]\n",
    "\n",
    "    # Get server metrics (if available)\n",
    "    # Note: Metric names may vary, adjust as needed\n",
    "    metric_keys = list(collector.metrics_history.keys())\n",
    "    if metric_keys:\n",
    "        # Try to find token generation rate metric\n",
    "        for key in metric_keys:\n",
    "            if \"token\" in key.lower() and \"sec\" in key.lower():\n",
    "                df_tokens = collector.get_dataframe(key)\n",
    "                if not df_tokens.empty:\n",
    "                    with fig_widget.batch_update():\n",
    "                        fig_widget.data[0].x = df_tokens[\"timestamp\"]\n",
    "                        fig_widget.data[0].y = df_tokens[\"value\"]\n",
    "                break\n",
    "\n",
    "    # Get slots info\n",
    "    if collector.slots_history:\n",
    "        slots_times = [pd.Timestamp(s[\"timestamp\"], unit=\"s\") for s in collector.slots_history]\n",
    "        slots_active = [s[\"num_processing\"] for s in collector.slots_history]\n",
    "\n",
    "        with fig_widget.batch_update():\n",
    "            fig_widget.data[4].x = slots_times\n",
    "            fig_widget.data[4].y = slots_active\n",
    "\n",
    "# Update every 2 seconds\n",
    "print(\"üîÑ Starting live dashboard updates...\")\n",
    "for i in range(30):  # Update 30 times (60 seconds total)\n",
    "    update_dashboard()\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"‚úÖ Dashboard updates complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 13: Calculate Performance Statistics\n",
    "# ==============================================================================\n",
    "df_gpu = collector.get_gpu_dataframe()\n",
    "\n",
    "if not df_gpu.empty:\n",
    "    print(\"üìä Performance Statistics\\n\")\n",
    "\n",
    "    print(\"GPU Utilization:\")\n",
    "    print(f\"  Mean: {df_gpu['gpu_utilization'].mean():.2f}%\")\n",
    "    print(f\"  P50:  {df_gpu['gpu_utilization'].quantile(0.50):.2f}%\")\n",
    "    print(f\"  P95:  {df_gpu['gpu_utilization'].quantile(0.95):.2f}%\")\n",
    "    print(f\"  Max:  {df_gpu['gpu_utilization'].max():.2f}%\")\n",
    "\n",
    "    print(\"\\nGPU Memory:\")\n",
    "    print(f\"  Mean: {df_gpu['memory_used_mb'].mean():.2f} MB\")\n",
    "    print(f\"  Max:  {df_gpu['memory_used_mb'].max():.2f} MB\")\n",
    "\n",
    "    print(\"\\nTemperature:\")\n",
    "    print(f\"  Mean: {df_gpu['temperature_c'].mean():.2f}¬∞C\")\n",
    "    print(f\"  Max:  {df_gpu['temperature_c'].max():.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 14: Request Statistics\n",
    "# ==============================================================================\n",
    "print(f\"\\nüöÄ Load Generator Statistics:\")\n",
    "print(f\"  Total Requests: {load_gen.request_count}\")\n",
    "print(f\"  Errors: {load_gen.error_count}\")\n",
    "print(f\"  Success Rate: {(1 - load_gen.error_count / max(load_gen.request_count, 1)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 15: Stop Everything\n",
    "# ==============================================================================\n",
    "# Stop load generation\n",
    "load_gen.stop_continuous_load()\n",
    "\n",
    "# Stop metrics collection\n",
    "collector.stop_background_collection()\n",
    "\n",
    "# Stop server\n",
    "server.stop_server()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "### **1. llama.cpp Metrics**\n",
    "- ‚úÖ `/metrics` endpoint provides Prometheus-format metrics\n",
    "- ‚úÖ Token generation throughput (tokens/second)\n",
    "- ‚úÖ Request processing statistics\n",
    "- ‚úÖ Cache hit rates\n",
    "\n",
    "### **2. GPU Monitoring**\n",
    "- ‚úÖ PyNVML for programmatic GPU metrics access\n",
    "- ‚úÖ Utilization, memory, temperature, power draw\n",
    "- ‚úÖ Real-time monitoring at 1-second intervals\n",
    "\n",
    "### **3. Request Queue Monitoring**\n",
    "- ‚úÖ `/slots` endpoint shows request queue state\n",
    "- ‚úÖ Number of processing vs idle slots\n",
    "- ‚úÖ Per-slot token generation progress\n",
    "\n",
    "### **4. Live Visualization**\n",
    "- ‚úÖ Plotly FigureWidget for real-time updates\n",
    "- ‚úÖ Multi-panel dashboards with synchronized timelines\n",
    "- ‚úÖ Efficient batch updates for smooth rendering\n",
    "\n",
    "### **5. Performance Analysis**\n",
    "- ‚úÖ Identify bottlenecks (GPU, memory, queue depth)\n",
    "- ‚úÖ Optimize batch size and concurrency\n",
    "- ‚úÖ Monitor thermal throttling and power limits\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 16:** End-to-end production observability stack\n",
    "- Export metrics to Prometheus/Grafana\n",
    "- Set up alerting for performance degradation\n",
    "- Implement auto-scaling based on queue depth\n",
    "- A/B test different model configurations\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Objectives Achieved:**\n",
    "\n",
    "‚úÖ CUDA Inference (GPU 0) - Continuous workload\n",
    "\n",
    "‚úÖ LLM Observability (GPU 0) - Full metrics collection\n",
    "\n",
    "‚úÖ Plotly Visualizations (GPU 1) - Live dashboards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
