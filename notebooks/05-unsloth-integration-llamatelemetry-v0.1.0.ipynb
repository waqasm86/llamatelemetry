{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0c4fcb",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPUs\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "\n",
    "# CUDA version\n",
    "print(\"\\nüìä CUDA Version:\")\n",
    "!nvcc --version | grep release\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for Unsloth + llamatelemetry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584dd45",
   "metadata": {},
   "source": [
    "## Step 2: Install Unsloth and llamatelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"üì¶ Installing Unsloth and llamatelemetry...\")\n",
    "\n",
    "# Install Unsloth (fast installation)\n",
    "!pip install -q unsloth\n",
    "\n",
    "# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n",
    "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install -q datasets trl\n",
    "\n",
    "# Verify installations\n",
    "import llamatelemetry\n",
    "print(f\"\\n‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Unsloth import issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac151c",
   "metadata": {},
   "source": [
    "## Step 3: Load Base Model with Unsloth\n",
    "\n",
    "We'll use Gemma-3 1B as it's fast to fine-tune on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì• LOADING BASE MODEL WITH UNSLOTH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"unsloth/gemma-3-1b-it\"  # Small model for demo\n",
    "max_seq_length = 2048\n",
    "\n",
    "print(f\"\\nüì• Loading {model_name}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,  # Use 4-bit for training\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"   Precision: 4-bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257229ae",
   "metadata": {},
   "source": [
    "## Step 4: Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîß ADDING LORA ADAPTERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,                # LoRA rank\n",
    "    lora_alpha=32,       # LoRA alpha\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n‚úÖ LoRA adapters added!\")\n",
    "print(f\"   Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"   Total params: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8df930",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä PREPARING TRAINING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load a small dataset for demo (Alpaca format)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded: {len(dataset)} examples\")\n",
    "print(f\"\\nüìã Sample data:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Format for training (Alpaca prompt format)\n",
    "def format_alpaca(example):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = dataset.map(format_alpaca)\n",
    "print(f\"\\n‚úÖ Dataset formatted for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2480a22",
   "metadata": {},
   "source": [
    "## Step 6: Train with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üèãÔ∏è TRAINING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training configuration (quick demo - increase for real training)\n",
    "training_args = SFTConfig(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=30,  # Quick demo - use more for real training\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    output_dir=\"./unsloth_output\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"\\nüèãÔ∏è Starting training...\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Max steps: {training_args.max_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7361a",
   "metadata": {},
   "source": [
    "## Step 7: Export to GGUF Format\n",
    "\n",
    "This is the key step - converting from Unsloth to llama.cpp compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ EXPORTING TO GGUF FORMAT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Output path\n",
    "OUTPUT_DIR = \"/kaggle/working/gguf_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Unsloth's built-in GGUF export\n",
    "print(\"\\nüì¶ Exporting to GGUF with Q4_K_M quantization...\")\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    OUTPUT_DIR,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",  # K-quant for best quality/size\n",
    ")\n",
    "\n",
    "# Find the exported file\n",
    "gguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n",
    "print(f\"\\n‚úÖ GGUF export complete!\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"   Files: {gguf_files}\")\n",
    "\n",
    "if gguf_files:\n",
    "    gguf_path = os.path.join(OUTPUT_DIR, gguf_files[0])\n",
    "    size_mb = os.path.getsize(gguf_path) / (1024**2)\n",
    "    print(f\"   Size: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6705b5",
   "metadata": {},
   "source": [
    "## Step 8: Clear GPU Memory Before Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e598c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"üßπ Clearing GPU memory...\")\n",
    "\n",
    "# Delete training objects\n",
    "del model\n",
    "del trainer\n",
    "del tokenizer\n",
    "\n",
    "# Clear CUDA cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüìä GPU Memory After Cleanup:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv\n",
    "\n",
    "print(\"\\n‚úÖ GPU memory cleared for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfec0e",
   "metadata": {},
   "source": [
    "## Step 9: Deploy with llamatelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a45266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.server import ServerManager\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ DEPLOYING FINE-TUNED MODEL WITH LLCUDA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find the GGUF file\n",
    "OUTPUT_DIR = \"/kaggle/working/gguf_output\"\n",
    "gguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n",
    "\n",
    "if not gguf_files:\n",
    "    print(\"‚ùå No GGUF file found!\")\n",
    "else:\n",
    "    gguf_path = os.path.join(OUTPUT_DIR, gguf_files[0])\n",
    "    print(f\"üì• Loading: {gguf_path}\")\n",
    "    \n",
    "    # Start server\n",
    "    server = ServerManager()\n",
    "    print(\"\\nüöÄ Starting llama-server...\")\n",
    "    server.start_server(\n",
    "        model_path=gguf_path,\n",
    "        host=\"127.0.0.1\",\n",
    "        port=8080,\n",
    "        gpu_layers=99,\n",
    "        ctx_size=2048,\n",
    "        flash_attention=True,\n",
    "    )\n",
    "    \n",
    "    if server.check_server_health(timeout=60):\n",
    "        print(\"\\n‚úÖ Fine-tuned model deployed!\")\n",
    "        print(f\"   API endpoint: http://127.0.0.1:8080\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296af75",
   "metadata": {},
   "source": [
    "## Step 10: Test Your Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.client import LlamaCppClient\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Test with Alpaca-style prompt (matching training format)\n",
    "test_prompts = [\n",
    "    \"### Instruction:\\nExplain what machine learning is.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nWrite a short poem about coding.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nWhat are the benefits of GPU acceleration?\\n\\n### Response:\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüîπ Test {i}:\")\n",
    "    print(f\"   Prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    response = client.completion(\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.7,\n",
    "        stop=[\"###\", \"\\n\\n\"]  # Stop at next section\n",
    "    )\n",
    "    \n",
    "    print(f\"   Response: {response.choices[0].text.strip()[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30713f5a",
   "metadata": {},
   "source": [
    "## Step 11: Compare with Chat API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ab695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üí¨ CHAT COMPLETION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test with chat format\n",
    "response = client.chat.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What did you learn during fine-tuning?\"}\n",
    "    ],\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"\\nüí¨ Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(f\"\\nüìä Usage:\")\n",
    "print(f\"   Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"   Completion tokens: {response.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ccfe2",
   "metadata": {},
   "source": [
    "## Step 12: Save Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a properly named copy\n",
    "OUTPUT_DIR = \"/kaggle/working/gguf_output\"\n",
    "gguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n",
    "\n",
    "if gguf_files:\n",
    "    src = os.path.join(OUTPUT_DIR, gguf_files[0])\n",
    "    dst = \"/kaggle/working/my-finetuned-model-Q4_K_M.gguf\"\n",
    "    \n",
    "    shutil.copy(src, dst)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model saved: {dst}\")\n",
    "    print(f\"   Size: {os.path.getsize(dst) / (1024**2):.1f} MB\")\n",
    "    print(f\"\\nüí° To use this model later:\")\n",
    "    print(f\"   from llamatelemetry.server import ServerManager\")\n",
    "    print(f\"   server = ServerManager()\")\n",
    "    print(f\"   server.start(model_path='{dst}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7066e68",
   "metadata": {},
   "source": [
    "## Step 13: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a972de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõë Stopping server...\")\n",
    "server.stop_server()\n",
    "\n",
    "print(\"\\n‚úÖ Server stopped\")\n",
    "print(\"\\nüìä Final GPU Status:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c6449",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "### Complete Workflow:\n",
    "1. ‚úÖ Installed Unsloth + llamatelemetry\n",
    "2. ‚úÖ Loaded base model with 4-bit quantization\n",
    "3. ‚úÖ Added LoRA adapters for efficient training\n",
    "4. ‚úÖ Fine-tuned on custom dataset\n",
    "5. ‚úÖ Exported to GGUF (Q4_K_M)\n",
    "6. ‚úÖ Deployed with llamatelemetry llama-server\n",
    "7. ‚úÖ Ran inference on fine-tuned model\n",
    "\n",
    "### Key llamatelemetry + Unsloth Integration:\n",
    "\n",
    "```python\n",
    "from llamatelemetry.unsloth import export_to_llamatelemetry\n",
    "\n",
    "# After Unsloth training\n",
    "export_to_llamatelemetry(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=\"my-model.gguf\",\n",
    "    quant_type=\"Q4_K_M\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [06-split-gpu-graphistry](06-split-gpu-graphistry-llamatelemetry-v0.1.0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
