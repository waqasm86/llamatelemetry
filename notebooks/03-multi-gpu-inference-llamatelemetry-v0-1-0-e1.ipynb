{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "a8cf5402",
   "cell_type": "markdown",
   "source": "## Step 1: Verify Dual GPU Environment\n\nValidates dual Tesla T4 GPU availability with total 30GB VRAM capacity, confirming readiness for multi-GPU tensor-split inference scenarios.",
   "metadata": {}
  },
  {
   "id": "aa5253aa",
   "cell_type": "code",
   "source": "import subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ” DUAL GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Get GPU info\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nğŸ“Š Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\nâœ… Dual T4 environment confirmed!\")\n    print(\"   Total VRAM: 30GB (15GB Ã— 2)\")\nelse:\n    print(\"\\nâš ï¸ Only 1 GPU detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle settings.\")\n\n# CUDA version\nprint(\"\\nğŸ“Š CUDA Version:\")\n!nvcc --version | grep release",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:21:20.826226Z",
     "iopub.execute_input": "2026-02-04T06:21:20.826470Z",
     "iopub.status.idle": "2026-02-04T06:21:21.063117Z",
     "shell.execute_reply.started": "2026-02-04T06:21:20.826448Z",
     "shell.execute_reply": "2026-02-04T06:21:21.062208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ” DUAL GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 7.5\n   1, Tesla T4, 15360 MiB, 7.5\n\nâœ… Dual T4 environment confirmed!\n   Total VRAM: 30GB (15GB Ã— 2)\n\nğŸ“Š CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "id": "cfa5ca7c",
   "cell_type": "markdown",
   "source": "## Step 2: Install llamatelemetry v0.1.0 and RAPIDS\n\nInstalls llamatelemetry with CUDA binaries, plus cuGraph and Graphistry for combined GPU inference and graph analytics workflows.",
   "metadata": {}
  },
  {
   "id": "2fb3eff5",
   "cell_type": "code",
   "source": "%%time\n\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n!pip install -q huggingface_hub sseclient-py\n\nimport llamatelemetry\nprint(f\"âœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy\n\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:21:29.830752Z",
     "iopub.execute_input": "2026-02-04T06:21:29.831299Z",
     "iopub.status.idle": "2026-02-04T06:24:00.868494Z",
     "shell.execute_reply.started": "2026-02-04T06:21:29.831266Z",
     "shell.execute_reply": "2026-02-04T06:24:00.867641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ğŸ“¦ Installing dependencies...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m250.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m275.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m321.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m347.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m239.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m196.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m281.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m363.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m344.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m308.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m332.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m309.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m280.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m301.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m389.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m290.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m364.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m279.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m348.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m342.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e37b8009c53c44a3a64723f2d1198131"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\nâœ… llamatelemetry 0.1.0 installed\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\nCPU times: user 56.1 s, sys: 12.7 s, total: 1min 8s\nWall time: 2min 31s\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "2eec3c05-1381-4dce-aeba-93bcbaf6e604",
   "cell_type": "code",
   "source": "from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:24:24.483581Z",
     "iopub.execute_input": "2026-02-04T06:24:24.484984Z",
     "iopub.status.idle": "2026-02-04T06:24:24.826513Z",
     "shell.execute_reply.started": "2026-02-04T06:24:24.484948Z",
     "shell.execute_reply": "2026-02-04T06:24:24.825708Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "id": "712055b2-2035-4c89-8d9d-e7174d199006",
   "cell_type": "code",
   "source": "from huggingface_hub import login\nimport os\n\n# Login to Hugging Face\ntry:\n    login(token=hf_token)\n    print(\"Successfully logged into Hugging Face!\")\nexcept Exception as e:\n    print(f\"Error logging into Hugging Face: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:24:26.777695Z",
     "iopub.execute_input": "2026-02-04T06:24:26.778452Z",
     "iopub.status.idle": "2026-02-04T06:24:26.807155Z",
     "shell.execute_reply.started": "2026-02-04T06:24:26.778423Z",
     "shell.execute_reply": "2026-02-04T06:24:26.806419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Error logging into Hugging Face: cannot import name 'HfFolder' from 'huggingface_hub.utils' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/__init__.py)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "id": "d4f03e70-6ef6-4c56-b98d-d0e1d1a16f5e",
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "0a2febcb",
   "cell_type": "markdown",
   "source": "## Step 3: Understand Multi-GPU Configuration Options\n\nExplains tensor-split, split-mode (layer/row), and main-gpu parameters for distributing model layers and VRAM usage across multiple GPUs.",
   "metadata": {}
  },
  {
   "id": "c722131f",
   "cell_type": "code",
   "source": "from llamatelemetry.api.multigpu import MultiGPUConfig, SplitMode, GPUInfo\n\nprint(\"=\"*70)\nprint(\"ğŸ“‹ MULTI-GPU CONFIGURATION OPTIONS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nğŸ”¹ --tensor-split, -ts\n   Distributes VRAM usage across GPUs.\n   Example: --tensor-split 0.5,0.5 (50% each GPU)\n   Example: --tensor-split 0.7,0.3 (70% GPU0, 30% GPU1)\n\nğŸ”¹ --split-mode, -sm  \n   How to split the model across GPUs:\n   â€¢ 'layer' - Split by transformer layers (default, recommended)\n   â€¢ 'row'   - Split by matrix rows (can be slower)\n   â€¢ 'none'  - Disable multi-GPU (single GPU only)\n\nğŸ”¹ --main-gpu, -mg\n   Primary GPU for small tensors and scratch buffers.\n   Default: 0 (first GPU)\n\nğŸ”¹ --n-gpu-layers, -ngl\n   Number of layers to offload to GPU(s).\n   Use 99 to offload all layers.\n\"\"\")\n\n# Show split mode enum\nprint(\"\\nğŸ“‹ Split Modes:\")\nfor mode in SplitMode:\n    print(f\"   {mode.name}: {mode.value}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:53:17.140401Z",
     "iopub.execute_input": "2026-02-04T05:53:17.140707Z",
     "iopub.status.idle": "2026-02-04T05:53:17.795014Z",
     "shell.execute_reply.started": "2026-02-04T05:53:17.140681Z",
     "shell.execute_reply": "2026-02-04T05:53:17.793589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ“‹ MULTI-GPU CONFIGURATION OPTIONS\n======================================================================\n\nğŸ”¹ --tensor-split, -ts\n   Distributes VRAM usage across GPUs.\n   Example: --tensor-split 0.5,0.5 (50% each GPU)\n   Example: --tensor-split 0.7,0.3 (70% GPU0, 30% GPU1)\n\nğŸ”¹ --split-mode, -sm  \n   How to split the model across GPUs:\n   â€¢ 'layer' - Split by transformer layers (default, recommended)\n   â€¢ 'row'   - Split by matrix rows (can be slower)\n   â€¢ 'none'  - Disable multi-GPU (single GPU only)\n\nğŸ”¹ --main-gpu, -mg\n   Primary GPU for small tensors and scratch buffers.\n   Default: 0 (first GPU)\n\nğŸ”¹ --n-gpu-layers, -ngl\n   Number of layers to offload to GPU(s).\n   Use 99 to offload all layers.\n\n\nğŸ“‹ Split Modes:\n   NONE: none\n   LAYER: layer\n   ROW: row\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "id": "bdfed953",
   "cell_type": "markdown",
   "source": "## Step 4: Query GPU Information\n\nUses llamatelemetry API to detect GPUs, check free VRAM, and calculate total available memory for model sizing decisions.",
   "metadata": {}
  },
  {
   "id": "058365ea",
   "cell_type": "code",
   "source": "from llamatelemetry.api.multigpu import detect_gpus, get_free_vram\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š GPU INFORMATION\")\nprint(\"=\"*70)\n\n# Get detailed GPU info using detect_gpus (actual API function)\ngpus = detect_gpus()\n\nfor gpu in gpus:\n    print(f\"\\nğŸ”¹ GPU {gpu.id}: {gpu.name}\")\n    print(f\"   Total VRAM: {gpu.memory_total_gb:.1f} GB\")\n    print(f\"   Free VRAM: {gpu.memory_free_gb:.1f} GB\")\n    if gpu.compute_capability:\n        print(f\"   Compute Capability: {gpu.compute_capability}\")\n\n# Calculate total available VRAM\ntotal_vram = sum(gpu.memory_free_gb for gpu in gpus)\nprint(f\"\\nğŸ“Š Total Available VRAM: {total_vram:.1f} GB\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:59:04.231806Z",
     "iopub.execute_input": "2026-02-04T05:59:04.232192Z",
     "iopub.status.idle": "2026-02-04T05:59:04.298447Z",
     "shell.execute_reply.started": "2026-02-04T05:59:04.232163Z",
     "shell.execute_reply": "2026-02-04T05:59:04.297592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ“Š GPU INFORMATION\n======================================================================\n\nğŸ”¹ GPU 0: Tesla T4\n   Total VRAM: 15.0 GB\n   Free VRAM: 14.6 GB\n   Compute Capability: 7.5\n\nğŸ”¹ GPU 1: Tesla T4\n   Total VRAM: 15.0 GB\n   Free VRAM: 14.6 GB\n   Compute Capability: 7.5\n\nğŸ“Š Total Available VRAM: 29.1 GB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "id": "9140cd97",
   "cell_type": "markdown",
   "source": "## Step 5: Download Multi-GPU Test Model\n\nDownloads Gemma 3-4B Q4_K_M (~2.5GB) model optimized for dual-GPU distribution, demonstrating tensor-split effectiveness on larger models.",
   "metadata": {}
  },
  {
   "id": "3a4b9bc3",
   "cell_type": "code",
   "source": "%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# For multi-GPU testing, use a 4B model\nMODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\nMODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n\nprint(f\"ğŸ“¥ Downloading {MODEL_FILE}...\")\nprint(f\"   This ~2.5GB model will be split across both GPUs.\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\nâœ… Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:59:10.736930Z",
     "iopub.execute_input": "2026-02-04T05:59:10.737795Z",
     "iopub.status.idle": "2026-02-04T05:59:16.254490Z",
     "shell.execute_reply.started": "2026-02-04T05:59:10.737759Z",
     "shell.execute_reply": "2026-02-04T05:59:16.253831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ğŸ“¥ Downloading gemma-3-4b-it-Q4_K_M.gguf...\n   This ~2.5GB model will be split across both GPUs.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "gemma-3-4b-it-Q4_K_M.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5322aeaab92b400dbb3c3d9f7071e579"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nâœ… Model downloaded: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n   Size: 2.32 GB\nCPU times: user 5.73 s, sys: 9.2 s, total: 14.9 s\nWall time: 5.51 s\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "id": "1d35c0e9",
   "cell_type": "markdown",
   "source": "## Step 6: Explore Tensor-Split Configurations\n\nDemonstrates various VRAM distribution strategies: equal split (50/50), asymmetric (70/30), and single-GPU (100/0) for different use cases.",
   "metadata": {}
  },
  {
   "id": "f1a16bd0",
   "cell_type": "code",
   "source": "from llamatelemetry.api.multigpu import MultiGPUConfig, SplitMode\n\nprint(\"=\"*70)\nprint(\"ğŸ“‹ TENSOR-SPLIT CONFIGURATIONS\")\nprint(\"=\"*70)\n\nconfigs = [\n    {\n        \"name\": \"Equal Split (50/50)\",\n        \"tensor_split\": [0.5, 0.5],\n        \"description\": \"Equal distribution across both GPUs\",\n        \"use_case\": \"Default, balanced workload\"\n    },\n    {\n        \"name\": \"GPU 0 Heavy (70/30)\",\n        \"tensor_split\": [0.7, 0.3],\n        \"description\": \"More VRAM on GPU 0\",\n        \"use_case\": \"When GPU 1 needed for other tasks\"\n    },\n    {\n        \"name\": \"GPU 0 Only (100/0)\",\n        \"tensor_split\": [1.0, 0.0],\n        \"description\": \"Single GPU mode\",\n        \"use_case\": \"When GPU 1 reserved for RAPIDS/Graphistry\"\n    },\n]\n\nfor i, config in enumerate(configs, 1):\n    print(f\"\\nğŸ”¹ Config {i}: {config['name']}\")\n    print(f\"   Tensor Split: {config['tensor_split']}\")\n    print(f\"   Description: {config['description']}\")\n    print(f\"   Use Case: {config['use_case']}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:59:40.509483Z",
     "iopub.execute_input": "2026-02-04T05:59:40.509847Z",
     "iopub.status.idle": "2026-02-04T05:59:40.516209Z",
     "shell.execute_reply.started": "2026-02-04T05:59:40.509820Z",
     "shell.execute_reply": "2026-02-04T05:59:40.515278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ“‹ TENSOR-SPLIT CONFIGURATIONS\n======================================================================\n\nğŸ”¹ Config 1: Equal Split (50/50)\n   Tensor Split: [0.5, 0.5]\n   Description: Equal distribution across both GPUs\n   Use Case: Default, balanced workload\n\nğŸ”¹ Config 2: GPU 0 Heavy (70/30)\n   Tensor Split: [0.7, 0.3]\n   Description: More VRAM on GPU 0\n   Use Case: When GPU 1 needed for other tasks\n\nğŸ”¹ Config 3: GPU 0 Only (100/0)\n   Tensor Split: [1.0, 0.0]\n   Description: Single GPU mode\n   Use Case: When GPU 1 reserved for RAPIDS/Graphistry\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "d58e3e2c",
   "cell_type": "markdown",
   "source": "## Step 7: Launch Server with Dual-GPU Configuration\n\nStarts llama-server with 50/50 tensor-split across both GPUs, enabling larger models and higher context windows than single-GPU mode.",
   "metadata": {}
  },
  {
   "id": "433d461a",
   "cell_type": "code",
   "source": "from llamatelemetry.server import ServerManager\nfrom llamatelemetry.api.multigpu import SplitMode\n\nprint(\"=\"*70)\nprint(\"ğŸš€ STARTING DUAL-GPU SERVER\")\nprint(\"=\"*70)\n\n# Dual-GPU configuration parameters\ndual_config = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \n    # Multi-GPU settings\n    \"gpu_layers\": 99,              # Offload all layers\n    \"tensor_split\": \"0.5,0.5\",     # Equal split (as comma-separated string)\n    \n    # Performance\n    \"ctx_size\": 8192,\n    \"batch_size\": 1024,\n    \n    # Parallelism\n    \"n_parallel\": 4,\n}\n\nprint(f\"\\nğŸ“‹ Dual-GPU Configuration:\")\nprint(f\"   Model: {model_path.split('/')[-1]}\")\nprint(f\"   Tensor Split: GPU0=50%, GPU1=50%\")\nprint(f\"   Context Size: {dual_config['ctx_size']}\")\n\n# Start server\nserver = ServerManager(server_url=f\"http://{dual_config['host']}:{dual_config['port']}\")\nprint(\"\\nğŸš€ Starting server...\")\n\ntry:\n    server.start_server(\n        model_path=dual_config['model_path'],\n        host=dual_config['host'],\n        port=dual_config['port'],\n        gpu_layers=dual_config['gpu_layers'],\n        ctx_size=dual_config['ctx_size'],\n        batch_size=dual_config['batch_size'],\n        n_parallel=dual_config['n_parallel'],\n        timeout=120,\n        verbose=True,\n        # Multi-GPU tensor split\n        tensor_split=dual_config['tensor_split']\n    )\n    print(\"\\nâœ… Dual-GPU server started successfully!\")\nexcept Exception as e:\n    print(f\"\\nâŒ Server failed to start: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T05:59:48.699159Z",
     "iopub.execute_input": "2026-02-04T05:59:48.699479Z",
     "iopub.status.idle": "2026-02-04T05:59:53.768893Z",
     "shell.execute_reply.started": "2026-02-04T05:59:48.699453Z",
     "shell.execute_reply": "2026-02-04T05:59:53.768111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸš€ STARTING DUAL-GPU SERVER\n======================================================================\n\nğŸ“‹ Dual-GPU Configuration:\n   Model: gemma-3-4b-it-Q4_K_M.gguf\n   Tensor Split: GPU0=50%, GPU1=50%\n   Context Size: 8192\n\nğŸš€ Starting server...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready........ âœ“ Ready in 5.0s\n\nâœ… Dual-GPU server started successfully!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "id": "18c50fb0",
   "cell_type": "markdown",
   "source": "## Step 8: Verify Multi-GPU Memory Distribution\n\nConfirms both GPUs show VRAM usage via nvidia-smi, validating successful model distribution across hardware for balanced load sharing.",
   "metadata": {}
  },
  {
   "id": "299a7e2a",
   "cell_type": "code",
   "source": "import subprocess\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š GPU MEMORY DISTRIBUTION\")\nprint(\"=\"*70)\n\n# Check memory usage on both GPUs\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.used,memory.total,utilization.gpu\", \n     \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\nprint(\"\\nğŸ“Š GPU Memory After Model Load:\")\nfor line in result.stdout.strip().split('\\n'):\n    parts = line.split(', ')\n    if len(parts) >= 4:\n        idx, name, used, total = parts[0], parts[1], parts[2], parts[3]\n        print(f\"   GPU {idx}: {used} / {total}\")\n\nprint(\"\\nğŸ’¡ Both GPUs should show VRAM usage if tensor-split is working.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:00:01.785609Z",
     "iopub.execute_input": "2026-02-04T06:00:01.785920Z",
     "iopub.status.idle": "2026-02-04T06:00:01.831695Z",
     "shell.execute_reply.started": "2026-02-04T06:00:01.785897Z",
     "shell.execute_reply": "2026-02-04T06:00:01.830917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ“Š GPU MEMORY DISTRIBUTION\n======================================================================\n\nğŸ“Š GPU Memory After Model Load:\n   GPU 0: 1515 MiB / 15360 MiB\n   GPU 1: 1999 MiB / 15360 MiB\n\nğŸ’¡ Both GPUs should show VRAM usage if tensor-split is working.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "id": "fceb983b",
   "cell_type": "markdown",
   "source": "## Step 9: Benchmark Multi-GPU Performance\n\nRuns performance tests with multiple prompts measuring tokens-per-second throughput to quantify multi-GPU speedup benefits.",
   "metadata": {}
  },
  {
   "id": "a3690e5e",
   "cell_type": "code",
   "source": "import time\nfrom llamatelemetry.api.client import LlamaCppClient\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š MULTI-GPU PERFORMANCE BENCHMARK\")\nprint(\"=\"*70)\n\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n\n# Longer prompts to test multi-GPU throughput\nprompts = [\n    \"Write a detailed explanation of how GPU parallelism works in deep learning.\",\n    \"Explain the architecture of a transformer model step by step.\",\n    \"Describe the CUDA programming model and its key concepts.\",\n    \"What are the advantages of using multiple GPUs for inference?\",\n    \"Explain tensor parallelism vs pipeline parallelism.\",\n]\n\nprint(f\"\\nğŸƒ Running benchmark with {len(prompts)} prompts...\\n\")\n\ntotal_input_tokens = 0\ntotal_output_tokens = 0\ntotal_time = 0\n\nfor i, prompt in enumerate(prompts, 1):\n    start = time.time()\n    \n    response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=150,\n        temperature=0.7\n    )\n    \n    elapsed = time.time() - start\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n    \n    total_input_tokens += input_tokens\n    total_output_tokens += output_tokens\n    total_time += elapsed\n    \n    tok_per_sec = output_tokens / elapsed\n    print(f\"   Prompt {i}: {output_tokens} tokens in {elapsed:.2f}s ({tok_per_sec:.1f} tok/s)\")\n\nprint(f\"\\nğŸ“Š Benchmark Results:\")\nprint(f\"   Total Input Tokens: {total_input_tokens}\")\nprint(f\"   Total Output Tokens: {total_output_tokens}\")\nprint(f\"   Total Time: {total_time:.2f}s\")\nprint(f\"   Average Generation Speed: {total_output_tokens/total_time:.1f} tokens/second\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:00:26.970825Z",
     "iopub.execute_input": "2026-02-04T06:00:26.971239Z",
     "iopub.status.idle": "2026-02-04T06:00:44.927860Z",
     "shell.execute_reply.started": "2026-02-04T06:00:26.971201Z",
     "shell.execute_reply": "2026-02-04T06:00:44.926983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ“Š MULTI-GPU PERFORMANCE BENCHMARK\n======================================================================\n\nğŸƒ Running benchmark with 5 prompts...\n\n   Prompt 1: 150 tokens in 3.88s (38.7 tok/s)\n   Prompt 2: 150 tokens in 3.56s (42.1 tok/s)\n   Prompt 3: 150 tokens in 3.52s (42.6 tok/s)\n   Prompt 4: 150 tokens in 3.55s (42.2 tok/s)\n   Prompt 5: 150 tokens in 3.43s (43.7 tok/s)\n\nğŸ“Š Benchmark Results:\n   Total Input Tokens: 97\n   Total Output Tokens: 750\n   Total Time: 17.95s\n   Average Generation Speed: 41.8 tokens/second\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 11
  },
  {
   "id": "06b558e2",
   "cell_type": "markdown",
   "source": "## Step 10: Test Alternative Split Configurations\n\nStops server to reconfigure with 70/30 tensor-split, demonstrating flexibility for mixed workloads requiring different GPU allocations.",
   "metadata": {}
  },
  {
   "id": "2e7db403",
   "cell_type": "code",
   "source": "# Stop current server\nprint(\"ğŸ›‘ Stopping server for reconfiguration...\")\nserver.stop_server()\n\nimport time\ntime.sleep(2)\nprint(\"âœ… Server stopped\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:03:48.483651Z",
     "iopub.execute_input": "2026-02-04T06:03:48.484460Z",
     "iopub.status.idle": "2026-02-04T06:03:51.505263Z",
     "shell.execute_reply.started": "2026-02-04T06:03:48.484430Z",
     "shell.execute_reply": "2026-02-04T06:03:51.504576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ğŸ›‘ Stopping server for reconfiguration...\nâœ… Server stopped\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 12
  },
  {
   "id": "8741132f",
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"ğŸ”§ TESTING 70/30 SPLIT CONFIGURATION\")\nprint(\"=\"*70)\n\n# 70/30 split - more on GPU 0\nconfig_70_30 = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \"gpu_layers\": 99,\n    \"tensor_split\": \"0.7,0.3\",  # 70% GPU0, 30% GPU1\n    \"ctx_size\": 8192,\n}\n\nprint(f\"\\nğŸ“‹ Configuration:\")\nprint(f\"   Tensor Split: GPU0=70%, GPU1=30%\")\nprint(f\"   Use Case: When GPU1 needs memory for other tasks\")\n\ntry:\n    server.start_server(\n        model_path=config_70_30['model_path'],\n        host=config_70_30['host'],\n        port=config_70_30['port'],\n        gpu_layers=config_70_30['gpu_layers'],\n        ctx_size=config_70_30['ctx_size'],\n        timeout=60,\n        verbose=True,\n        tensor_split=config_70_30['tensor_split']\n    )\n    print(\"\\nâœ… 70/30 split server started!\")\n    \n    # Check memory distribution\n    print(\"\\nğŸ“Š Memory Distribution:\")\n    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\nexcept Exception as e:\n    print(f\"\\nâŒ Failed to start: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:03:55.132372Z",
     "iopub.execute_input": "2026-02-04T06:03:55.132725Z",
     "iopub.status.idle": "2026-02-04T06:03:58.359325Z",
     "shell.execute_reply.started": "2026-02-04T06:03:55.132690Z",
     "shell.execute_reply": "2026-02-04T06:03:58.358560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ”§ TESTING 70/30 SPLIT CONFIGURATION\n======================================================================\n\nğŸ“‹ Configuration:\n   Tensor Split: GPU0=70%, GPU1=30%\n   Use Case: When GPU1 needs memory for other tasks\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready...... âœ“ Ready in 3.0s\n\nâœ… 70/30 split server started!\n\nğŸ“Š Memory Distribution:\nindex, memory.used [MiB], memory.total [MiB]\n0, 1743 MiB, 15360 MiB\n1, 1357 MiB, 15360 MiB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "id": "d5da3269",
   "cell_type": "markdown",
   "source": "## Step 11: Configure Split-GPU Mode for LLM + RAPIDS\n\nRuns LLM entirely on GPU 0 (100/0 split) while reserving GPU 1 completely for RAPIDS/cuGraph/Graphistry analytics workloads.",
   "metadata": {}
  },
  {
   "id": "4ff9bd02",
   "cell_type": "code",
   "source": "# Stop current server\nserver.stop_server()\nimport time\ntime.sleep(2)\n\nprint(\"=\"*70)\nprint(\"ğŸ¯ SPLIT-GPU MODE: LLM + RAPIDS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nThis configuration runs the LLM entirely on GPU 0,\nleaving GPU 1 free for RAPIDS/cuGraph/Graphistry.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚       GPU 0 (15GB)      â”‚        GPU 1 (15GB)           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚  llama-server   â”‚    â”‚   â”‚  RAPIDS / Graphistry    â”‚ â”‚\nâ”‚  â”‚  (Full Model)   â”‚    â”‚   â”‚  (Graph Visualization)  â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\")\n\n# Single GPU configuration (GPU 0 only)\nsplit_gpu_config = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \"gpu_layers\": 99,\n    \"tensor_split\": \"1.0,0.0\",  # 100% on GPU 0\n    \"ctx_size\": 4096,  # Smaller context to fit in single GPU\n}\n\ntry:\n    server.start_server(\n        model_path=split_gpu_config['model_path'],\n        host=split_gpu_config['host'],\n        port=split_gpu_config['port'],\n        gpu_layers=split_gpu_config['gpu_layers'],\n        ctx_size=split_gpu_config['ctx_size'],\n        timeout=60,\n        verbose=True,\n        tensor_split=split_gpu_config['tensor_split']\n    )\n    print(\"\\nâœ… Split-GPU mode server started!\")\n    print(\"   GPU 0: llama-server\")\n    print(\"   GPU 1: Available for RAPIDS/Graphistry\")\n    \n    print(\"\\nğŸ“Š Memory Distribution:\")\n    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\nexcept Exception as e:\n    print(f\"\\nâŒ Failed to start: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:04:06.534231Z",
     "iopub.execute_input": "2026-02-04T06:04:06.534588Z",
     "iopub.status.idle": "2026-02-04T06:04:12.178197Z",
     "shell.execute_reply.started": "2026-02-04T06:04:06.534529Z",
     "shell.execute_reply": "2026-02-04T06:04:12.177335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ¯ SPLIT-GPU MODE: LLM + RAPIDS\n======================================================================\n\nThis configuration runs the LLM entirely on GPU 0,\nleaving GPU 1 free for RAPIDS/cuGraph/Graphistry.\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚       GPU 0 (15GB)      â”‚        GPU 1 (15GB)           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚  llama-server   â”‚    â”‚   â”‚  RAPIDS / Graphistry    â”‚ â”‚\nâ”‚  â”‚  (Full Model)   â”‚    â”‚   â”‚  (Graph Visualization)  â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready...... âœ“ Ready in 3.0s\n\nâœ… Split-GPU mode server started!\n   GPU 0: llama-server\n   GPU 1: Available for RAPIDS/Graphistry\n\nğŸ“Š Memory Distribution:\nindex, memory.used [MiB], memory.total [MiB]\n0, 2859 MiB, 15360 MiB\n1, 105 MiB, 15360 MiB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "id": "fa4e1778",
   "cell_type": "markdown",
   "source": "## Step 12: Verify GPU 1 Availability for RAPIDS\n\nConfirms GPU 1 has >14GB free VRAM via nvidia-smi, validating successful isolation for graph processing and visualization tasks.",
   "metadata": {}
  },
  {
   "id": "fe283bc0",
   "cell_type": "code",
   "source": "import subprocess\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š GPU 1 AVAILABILITY CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,memory.used,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\nlines = result.stdout.strip().split('\\n')\nif len(lines) >= 2:\n    gpu1_info = lines[1].split(', ')\n    used = gpu1_info[1].strip()\n    free = gpu1_info[2].strip()\n    \n    print(f\"\\nğŸ“Š GPU 1 Status:\")\n    print(f\"   Memory Used: {used}\")\n    print(f\"   Memory Free: {free}\")\n    \n    # Parse free memory\n    free_mb = int(free.replace(' MiB', ''))\n    if free_mb > 14000:  # > 14GB free\n        print(f\"\\nâœ… GPU 1 has {free_mb/1024:.1f} GB free - Ready for RAPIDS!\")\n    else:\n        print(f\"\\nâš ï¸ GPU 1 has limited free memory.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:37:18.786867Z",
     "iopub.execute_input": "2026-02-04T06:37:18.787225Z",
     "iopub.status.idle": "2026-02-04T06:37:18.831449Z",
     "shell.execute_reply.started": "2026-02-04T06:37:18.787186Z",
     "shell.execute_reply": "2026-02-04T06:37:18.830786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ“Š GPU 1 AVAILABILITY CHECK\n======================================================================\n\nğŸ“Š GPU 1 Status:\n   Memory Used: 3 MiB\n   Memory Free: 14910 MiB\n\nâœ… GPU 1 has 14.6 GB free - Ready for RAPIDS!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "id": "9bd73e7f",
   "cell_type": "markdown",
   "source": "## Step 13: Test RAPIDS on GPU 1\n\nCreates cuDF DataFrame and performs GPU computations on GPU 1 using CUDA_VISIBLE_DEVICES, verifying parallel LLM+analytics capability.",
   "metadata": {}
  },
  {
   "id": "8b5adf47-4deb-43b3-94b7-c945da821590",
   "cell_type": "code",
   "source": "# Step 13: Quick RAPIDS Verification on GPU 1 (Simplified)\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Force RAPIDS to use GPU 1\n\nprint(\"=\"*70)\nprint(\"ğŸ”¥ RAPIDS ON GPU 1 VERIFICATION\")\nprint(\"=\"*70)\n\ntry:\n    import cudf\n    import cupy as cp\n    import torch\n    \n    # Create a small cuDF DataFrame on GPU 1\n    df = cudf.DataFrame({\n        'x': range(1000),\n        'y': range(1000, 2000)\n    })\n    \n    print(f\"\\nâœ… cuDF working on GPU 1\")\n    print(f\"   DataFrame shape: {df.shape}\")\n    print(f\"   Memory used: {df.memory_usage().sum() / 1024:.2f} KB\")\n    \n    # Check GPU info using PyTorch instead (more reliable)\n    print(f\"\\nğŸ“Š GPU Info (via PyTorch):\")\n    if torch.cuda.is_available():\n        gpu_count = torch.cuda.device_count()\n        print(f\"   Number of GPUs available to PyTorch: {gpu_count}\")\n        \n        # Since we set CUDA_VISIBLE_DEVICES=\"1\", device 0 here is actually GPU 1\n        current_device = torch.cuda.current_device()\n        device_name = torch.cuda.get_device_name(current_device)\n        print(f\"   Current GPU: {device_name}\")\n        print(f\"   GPU Memory: {torch.cuda.get_device_properties(current_device).total_memory / 1e9:.2f} GB\")\n    else:\n        print(\"   PyTorch CUDA not available\")\n    \n    # Simple cuPy verification\n    print(f\"\\nğŸ§ª cuPy GPU Test:\")\n    with cp.cuda.Device(0) as dev:\n        print(f\"   Using cuPy device ID: {dev.id}\")\n        # Create arrays and do computation\n        x = cp.arange(10)\n        y = cp.arange(10, 20)\n        z = x + y\n        print(f\"   Computation test: sum = {z.sum()}\")\n        print(f\"   Arrays created successfully on GPU\")\n    \nexcept ImportError as e:\n    print(f\"\\nâš ï¸ RAPIDS not available: {e}\")\nexcept Exception as e:\n    print(f\"\\nâš ï¸ Error: {e}\")\n\n# Reset CUDA_VISIBLE_DEVICES\nos.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\nprint(f\"\\nâœ… Reset CUDA_VISIBLE_DEVICES\")\nprint(\"=\"*70)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-04T06:37:23.721237Z",
     "iopub.execute_input": "2026-02-04T06:37:23.721886Z",
     "iopub.status.idle": "2026-02-04T06:37:23.736838Z",
     "shell.execute_reply.started": "2026-02-04T06:37:23.721851Z",
     "shell.execute_reply": "2026-02-04T06:37:23.735860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nğŸ”¥ RAPIDS ON GPU 1 VERIFICATION\n======================================================================\n\nâœ… cuDF working on GPU 1\n   DataFrame shape: (1000, 2)\n   Memory used: 15.62 KB\n\nğŸ“Š GPU Info (via PyTorch):\n   Number of GPUs available to PyTorch: 2\n   Current GPU: Tesla T4\n   GPU Memory: 15.64 GB\n\nğŸ§ª cuPy GPU Test:\n   Using cuPy device ID: 0\n   Computation test: sum = 190\n   Arrays created successfully on GPU\n\nâœ… Reset CUDA_VISIBLE_DEVICES\n======================================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "id": "40c23da2",
   "cell_type": "markdown",
   "source": "## Step 14: Cleanup and Resource Release\n\nStops llama-server, clears GPU memory, and displays final GPU status confirming successful resource cleanup across both devices.",
   "metadata": {}
  },
  {
   "id": "b2bdf5c1",
   "cell_type": "code",
   "source": "print(\"ğŸ›‘ Stopping server...\")\nserver.stop_server()\n\nprint(\"\\nâœ… Server stopped\")\nprint(\"\\nğŸ“Š Final GPU Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "15e9a1f2",
   "cell_type": "markdown",
   "source": "## ğŸ“š Summary\n\nYou've learned:\n1. âœ… Multi-GPU configuration options (tensor-split, split-mode)\n2. âœ… Equal split (50/50) for maximum model size\n3. âœ… Asymmetric split (70/30) for mixed workloads\n4. âœ… Single-GPU mode (100/0) for LLM + RAPIDS\n5. âœ… Performance benchmarking across GPUs\n\n## Configuration Quick Reference\n\n| Use Case | Tensor Split | GPU 0 | GPU 1 |\n|----------|--------------|-------|-------|\n| Max Model Size | 0.5, 0.5 | LLM (50%) | LLM (50%) |\n| LLM + Light Task | 0.7, 0.3 | LLM (70%) | LLM (30%) |\n| LLM + RAPIDS | 1.0, 0.0 | LLM (100%) | RAPIDS (100%) |\n\n---\n\n**Next:** [04-gguf-quantization](04-gguf-quantization-llamatelemetry-v0.1.0.ipynb)",
   "metadata": {}
  }
 ]
}