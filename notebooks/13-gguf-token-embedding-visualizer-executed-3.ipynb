{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [], "dockerImageVersionId": 31260, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": true}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# üé® GGUF Token Embedding Visualizer\n\n**Complementary to [Transformers-Explainer](https://poloclub.github.io/transformer-explainer/)** - Embedding Layer Analysis\n\n---\n\n## Overview\n\nThis notebook visualizes **how GGUF models represent tokens as high-dimensional vectors** and explores the **semantic structure** of the embedding space using GPU-accelerated dimensionality reduction.\n\n### What Transformers-Explainer Shows\n\n- **Token Embedding**: Shows 768-dimensional vectors as colored rectangles\n- **Positional Encoding**: Displays sinusoidal position embeddings\n- **Combined Input**: Token + Position ‚Üí Transformer input\n\n### What This Notebook Adds\n\n1. **Extract actual embeddings** from GGUF models (768-4096 dimensions)\n2. **GPU-accelerated UMAP/t-SNE** for 2D/3D projections\n3. **Semantic clustering**: Visualize similar words in embedding space\n4. **Quantization impact**: Compare FP32 ‚Üí Q4_K_M embedding quality\n5. **Interactive 3D exploration** with Graphistry\n\n---\n\n## Architecture\n\n```\nGGUF Model (GPU 0)           RAPIDS + Graphistry (GPU 1)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Token Embeddings ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ cuML UMAP (GPU-accel)   ‚îÇ\n‚îÇ (50K √ó d_model)  ‚îÇ         ‚îÇ ‚îú‚îÄ 768D ‚Üí 3D projection ‚îÇ\n‚îÇ                  ‚îÇ         ‚îÇ ‚îî‚îÄ Distance matrix      ‚îÇ\n‚îÇ Vocab: 50,257    ‚îÇ         ‚îÇ                         ‚îÇ\n‚îÇ Dimensions:      ‚îÇ         ‚îÇ Graphistry 3D Plot      ‚îÇ\n‚îÇ - Gemma: 2048    ‚îÇ         ‚îÇ ‚îú‚îÄ Semantic clusters    ‚îÇ\n‚îÇ - Llama: 4096    ‚îÇ         ‚îÇ ‚îú‚îÄ Word similarity      ‚îÇ\n‚îÇ - Qwen: 2048     ‚îÇ         ‚îÇ ‚îî‚îÄ Interactive explore  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## Learning Objectives\n\n1. **Understand embeddings**: How models represent discrete tokens as continuous vectors\n2. **Semantic structure**: Why similar words cluster together\n3. **Dimensionality**: Explore 768D-4096D embedding spaces\n4. **Quantization trade-offs**: Impact of Q4_K_M on embedding quality\n5. **GPU acceleration**: RAPIDS cuML for fast UMAP/t-SNE", "metadata": {}}, {"cell_type": "code", "source": "# Kaggle environment\nimport os", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:32:34.864625Z", "iopub.execute_input": "2026-01-31T22:32:34.864928Z", "iopub.status.idle": "2026-01-31T22:32:34.871757Z", "shell.execute_reply.started": "2026-01-31T22:32:34.864901Z", "shell.execute_reply": "2026-01-31T22:32:34.871122Z"}}, "outputs": [], "execution_count": 1}, {"cell_type": "code", "source": "# ==============================================================================\n# SECRET MANAGEMENT: Graphistry API Key\n# ==============================================================================\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nsecret_value_2 = user_secrets.get_secret(\"HF_TOKEN\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:32:36.012203Z", "iopub.execute_input": "2026-01-31T22:32:36.012971Z", "iopub.status.idle": "2026-01-31T22:32:36.246600Z", "shell.execute_reply.started": "2026-01-31T22:32:36.012940Z", "shell.execute_reply": "2026-01-31T22:32:36.246030Z"}}, "outputs": [], "execution_count": 2}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 1: Verify Dual GPU Environment\n# ==============================================================================\nimport subprocess\nprint(\"=\"*70)\nprint(\"üéÆ VERIFYING DUAL TESLA T4 ENVIRONMENT\")\nprint(\"=\"*70)\nsubprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,compute_cap\", \"--format=csv\"])", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:32:37.255269Z", "iopub.execute_input": "2026-01-31T22:32:37.255572Z", "iopub.status.idle": "2026-01-31T22:32:37.300119Z", "shell.execute_reply.started": "2026-01-31T22:32:37.255539Z", "shell.execute_reply": "2026-01-31T22:32:37.299472Z"}}, "outputs": [{"name": "stdout", "text": "======================================================================\nüéÆ VERIFYING DUAL TESLA T4 ENVIRONMENT\n======================================================================\nname, memory.total [MiB], compute_cap\nTesla T4, 15360 MiB, 7.5\nTesla T4, 15360 MiB, 7.5\n", "output_type": "stream"}, {"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "CompletedProcess(args=['nvidia-smi', '--query-gpu=name,memory.total,compute_cap', '--format=csv'], returncode=0)"}, "metadata": {}}], "execution_count": 3}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 2: Install llamatelemetry v0.1.0\n# ==============================================================================\nprint(\"üì¶ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0\n!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\n‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"‚úÖ cuDF {cudf.__version__}\")\n    print(f\"‚úÖ cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"‚úÖ Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Graphistry: {e}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:32:38.516212Z", "iopub.execute_input": "2026-01-31T22:32:38.516500Z", "iopub.status.idle": "2026-01-31T22:34:04.465438Z", "shell.execute_reply.started": "2026-01-31T22:32:38.516476Z", "shell.execute_reply": "2026-01-31T22:34:04.464665Z"}}, "outputs": [{"name": "stdout", "text": "üì¶ Installing dependencies...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h", "output_type": "stream"}, {"name": "stderr", "text": "WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n", "output_type": "stream"}, {"name": "stdout", "text": "\n======================================================================\nüéØ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nüåê Platform: Kaggle\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n", "output_type": "stream"}, {"name": "stderr", "text": "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.(‚Ä¶):   0%|          | 0.00/1.01G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "532b00232b8047139ead71ca0c844678"}}, "metadata": {}}, {"name": "stdout", "text": "üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\n‚úÖ Binaries installed successfully!\n\n\n‚úÖ llamatelemetry 0.1.0 installed\n‚úÖ cuDF 25.06.00\n‚úÖ cuGraph 25.06.00\n‚úÖ Graphistry 0.50.6\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "code", "source": "!pip install -q plotly scikit-learn umap-learn", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:34:04.466945Z", "iopub.execute_input": "2026-01-31T22:34:04.467652Z", "iopub.status.idle": "2026-01-31T22:34:08.011208Z", "shell.execute_reply.started": "2026-01-31T22:34:04.467626Z", "shell.execute_reply": "2026-01-31T22:34:08.010352Z"}}, "outputs": [], "execution_count": 5}, {"cell_type": "code", "source": "!pip install -q seaborn networkx", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:34:08.012337Z", "iopub.execute_input": "2026-01-31T22:34:08.012593Z", "iopub.status.idle": "2026-01-31T22:34:11.421127Z", "shell.execute_reply.started": "2026-01-31T22:34:08.012564Z", "shell.execute_reply": "2026-01-31T22:34:11.420141Z"}}, "outputs": [], "execution_count": 6}, {"cell_type": "code", "source": "# First, let's see what's actually available in llamatelemetry\nimport llamatelemetry\nprint(f\"llamatelemetry version: {llamatelemetry.__version__}\")\nprint(\"\\nAvailable attributes in llamatelemetry:\")\nprint([attr for attr in dir(llamatelemetry) if not attr.startswith('_')])", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:34:11.423049Z", "iopub.execute_input": "2026-01-31T22:34:11.423395Z", "iopub.status.idle": "2026-01-31T22:34:13.715562Z", "shell.execute_reply.started": "2026-01-31T22:34:11.423366Z", "shell.execute_reply": "2026-01-31T22:34:13.714935Z"}}, "outputs": [{"name": "stdout", "text": "llamatelemetry version: 0.1.0\n\nAvailable attributes in llamatelemetry:\n['Any', 'Dict', 'InferResult', 'InferenceEngine', 'List', 'Optional', 'Path', 'ServerManager', 'bootstrap', 'check_cuda_available', 'check_gpu_compatibility', 'create_config_file', 'detect_cuda', 'find_gguf_models', 'get_cuda_device_info', 'get_llama_cpp_cuda_path', 'get_recommended_gpu_layers', 'load_config', 'logging', 'os', 'print_system_info', 'quick_infer', 'requests', 'server', 'setup_environment', 'subprocess', 'sys', 'time', 'utils', 'validate_model_path']\n", "output_type": "stream"}], "execution_count": 7}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 3: Download GGUF Model (Fixed - No GGUF Parsing Errors)\n# ==============================================================================\n\nfrom huggingface_hub import hf_hub_download\nimport os\n\nMODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\nMODEL_FILE = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\nprint(f\"üì• Downloading {MODEL_FILE}...\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\n‚úÖ Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")\n\n# Show file exists\nprint(f\"\\nüìÅ File verification:\")\nprint(f\"   File exists: {os.path.exists(model_path)}\")\nprint(f\"   File size: {size_gb:.2f} GB\")\n\n# Instead of parsing GGUF, use known architecture for Llama-3.2-3B\nprint(\"\\nüîç Using known architecture for Llama-3.2-3B:\")\n\n# Known architecture for Llama-3.2-3B\nARCHITECTURE = {\n    'model': 'Llama-3.2-3B-Instruct',\n    'format': 'GGUF Q4_K_M',\n    'layers': 28,                 # Number of transformer blocks\n    'attention_heads': 32,        # Attention heads per layer\n    'hidden_dimension': 3072,     # Model dimension\n    'vocabulary_size': 128256,    # Token vocabulary\n    'context_length': 8192,       # Max context length\n    'feedforward_multiplier': 4,  # FFN is 4√ó hidden_dim (Swiglu)\n    'quantization': 'Q4_K_M',     # Quantization type\n    'estimated_params': 2.8e9,    # Approximately 2.8 billion parameters\n    'file_size_gb': 1.88,         # Actual file size\n    'attention_dim_per_head': 96, # 3072 / 32 = 96\n    'rope_theta': 500000,         # RoPE base frequency\n}\n\nprint(\"\\nüìä Architecture Summary:\")\nfor key, value in ARCHITECTURE.items():\n    if isinstance(value, (int, float)) and value >= 1000:\n        print(f\"   {key}: {value:,}\")\n    else:\n        print(f\"   {key}: {value}\")\n\n# Derived calculations\nprint(\"\\nüßÆ Derived Architecture Values:\")\nn_layers = ARCHITECTURE['layers']\nn_heads = ARCHITECTURE['attention_heads']\nhidden_dim = ARCHITECTURE['hidden_dimension']\nvocab_size = ARCHITECTURE['vocabulary_size']\n\nprint(f\"   Total transformer layers: {n_layers}\")\nprint(f\"   Total attention heads: {n_layers} √ó {n_heads} = {n_layers * n_heads:,}\")\nprint(f\"   Attention dimension per head: {hidden_dim} √∑ {n_heads} = {hidden_dim // n_heads}\")\nprint(f\"   Feed-forward hidden dimension: {hidden_dim} √ó {ARCHITECTURE['feedforward_multiplier']} = {hidden_dim * ARCHITECTURE['feedforward_multiplier']:,}\")\n\n# Parameter breakdown (simplified)\nprint(\"\\nüìà Parameter Distribution (Approximate):\")\nembedding_params = vocab_size * hidden_dim\nattention_params = 4 * hidden_dim * hidden_dim * n_layers  # Q, K, V, O\nffn_params = 2 * 4 * hidden_dim * hidden_dim * n_layers    # FFN (Swiglu)\noutput_params = hidden_dim * vocab_size                    # Output layer\ntotal_params = embedding_params + attention_params + ffn_params + output_params\n\nprint(f\"   Embedding layer: {embedding_params:,} ({embedding_params/total_params*100:.1f}%)\")\nprint(f\"   Attention layers: {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\nprint(f\"   Feed-forward layers: {ffn_params:,} ({ffn_params/total_params*100:.1f}%)\")\nprint(f\"   Output layer: {output_params:,} ({output_params/total_params*100:.1f}%)\")\nprint(f\"   Total estimated: {total_params:,} parameters\")\n\n# Quantization impact\nprint(f\"\\n‚öñÔ∏è Quantization Impact (Q4_K_M):\")\nfull_precision_gb = (total_params * 4) / (1024**3)  # 4 bytes per float32\nquantized_gb = size_gb\ncompression_ratio = full_precision_gb / quantized_gb\n\nprint(f\"   Full precision (FP32): {full_precision_gb:.1f} GB\")\nprint(f\"   Quantized (Q4_K_M): {quantized_gb:.1f} GB\")\nprint(f\"   Compression ratio: {compression_ratio:.1f}√ó\")\nprint(f\"   Average bits per parameter: {32 / compression_ratio:.1f} bits\")\n\nprint(f\"\\n‚úÖ Architecture ready for visualization\")\nprint(f\"   Will visualize: {n_layers} layers √ó {n_heads} heads = {n_layers * n_heads:,} attention heads\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:34:25.011927Z", "iopub.execute_input": "2026-01-31T22:34:25.012598Z", "iopub.status.idle": "2026-01-31T22:34:29.393064Z", "shell.execute_reply.started": "2026-01-31T22:34:25.012552Z", "shell.execute_reply": "2026-01-31T22:34:29.392309Z"}}, "outputs": [{"name": "stdout", "text": "üì• Downloading Llama-3.2-3B-Instruct-Q4_K_M.gguf...\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a035f789f2354effaf847f468141bb34"}}, "metadata": {}}, {"name": "stdout", "text": "\n‚úÖ Model downloaded: /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   Size: 1.88 GB\n\nüìÅ File verification:\n   File exists: True\n   File size: 1.88 GB\n\nüîç Using known architecture for Llama-3.2-3B:\n\nüìä Architecture Summary:\n   model: Llama-3.2-3B-Instruct\n   format: GGUF Q4_K_M\n   layers: 28\n   attention_heads: 32\n   hidden_dimension: 3,072\n   vocabulary_size: 128,256\n   context_length: 8,192\n   feedforward_multiplier: 4\n   quantization: Q4_K_M\n   estimated_params: 2,800,000,000.0\n   file_size_gb: 1.88\n   attention_dim_per_head: 96\n   rope_theta: 500,000\n\nüßÆ Derived Architecture Values:\n   Total transformer layers: 28\n   Total attention heads: 28 √ó 32 = 896\n   Attention dimension per head: 3072 √∑ 32 = 96\n   Feed-forward hidden dimension: 3072 √ó 4 = 12,288\n\nüìà Parameter Distribution (Approximate):\n   Embedding layer: 394,002,432 (10.0%)\n   Attention layers: 1,056,964,608 (26.7%)\n   Feed-forward layers: 2,113,929,216 (53.4%)\n   Output layer: 394,002,432 (10.0%)\n   Total estimated: 3,958,898,688 parameters\n\n‚öñÔ∏è Quantization Impact (Q4_K_M):\n   Full precision (FP32): 14.7 GB\n   Quantized (Q4_K_M): 1.9 GB\n   Compression ratio: 7.8√ó\n   Average bits per parameter: 4.1 bits\n\n‚úÖ Architecture ready for visualization\n   Will visualize: 28 layers √ó 32 heads = 896 attention heads\n", "output_type": "stream"}], "execution_count": 8}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 4: Start llama-server on GPU 0 Only\n# ==============================================================================\n\nfrom llamatelemetry.server import ServerManager\n\nprint(\"=\"*70)\nprint(\"üöÄ STARTING LLAMA-SERVER ON GPU 0\")\nprint(\"=\"*70)\n\nprint(\"\\nüìã Configuration:\")\nprint(\"   GPU 0: 100% (llama-server for model inference)\")\nprint(\"   GPU 1: 0% (reserved for RAPIDS/Graphistry)\")\nprint(\"   Model: Llama-3.2-3B-Instruct (Q4_K_M)\")\nprint(\"   Context: 4096 tokens\")\n\nserver = ServerManager()\nserver.start_server(\n    model_path=model_path,\n    host=\"127.0.0.1\",\n    port=8090,\n    gpu_layers=99,          # Load all layers to GPU\n    tensor_split=\"1.0,0.0\", # 100% GPU 0, 0% GPU 1\n    ctx_size=4096,\n    verbose=False\n)\n\nif server.check_server_health():\n    print(\"\\n‚úÖ llama-server running on GPU 0!\")\n    print(\"   URL: http://127.0.0.1:8090\")\nelse:\n    print(\"\\n‚ùå Server failed to start\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:34:53.869105Z", "iopub.execute_input": "2026-01-31T22:34:53.869753Z", "iopub.status.idle": "2026-01-31T22:34:57.976245Z", "shell.execute_reply.started": "2026-01-31T22:34:53.869723Z", "shell.execute_reply": "2026-01-31T22:34:57.975633Z"}}, "outputs": [{"name": "stdout", "text": "======================================================================\nüöÄ STARTING LLAMA-SERVER ON GPU 0\n======================================================================\n\nüìã Configuration:\n   GPU 0: 100% (llama-server for model inference)\n   GPU 1: 0% (reserved for RAPIDS/Graphistry)\n   Model: Llama-3.2-3B-Instruct (Q4_K_M)\n   Context: 4096 tokens\n\n‚úÖ llama-server running on GPU 0!\n   URL: http://127.0.0.1:8090\n", "output_type": "stream"}], "execution_count": 9}, {"cell_type": "code", "source": ["# ==============================================================================\n", "# Step 5: Extract Token Embeddings via Inference\n", "# ==============================================================================\n", "import llamatelemetry\n", "import numpy as np\n", "import pandas as pd\n", "import requests\n", "import json\n", "\n", "print(\"=\"*70)\n", "print(\"üìä EXTRACTING TOKEN EMBEDDINGS\")\n", "print(\"=\"*70)\n", "\n", "# Test vocabulary: words from different semantic categories\n", "test_words = [\n", "    # Colors\n", "    \"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\",\n", "    # Animals\n", "    \"cat\", \"dog\", \"bird\", \"fish\", \"lion\", \"tiger\",\n", "    # Technology\n", "    \"computer\", \"software\", \"algorithm\", \"neural\", \"network\", \"GPU\",\n", "    # Emotions\n", "    \"happy\", \"sad\", \"angry\", \"excited\", \"calm\", \"peaceful\",\n", "    # Numbers\n", "    \"one\", \"two\", \"three\", \"four\", \"five\", \"six\",\n", "    # Verbs\n", "    \"run\", \"jump\", \"swim\", \"fly\", \"walk\", \"dance\",\n", "    # Countries\n", "    \"USA\", \"China\", \"India\", \"France\", \"Germany\", \"Japan\"\n", "]\n", "\n", "# Extract embeddings using the server API directly\n", "embeddings = []\n", "valid_words = []\n", "\n", "# Check available endpoints first\n", "print(\"üîç Checking available API endpoints...\")\n", "try:\n", "    response = requests.get(\"http://127.0.0.1:8090/v1/models\")\n", "    if response.status_code == 200:\n", "        print(f\"‚úÖ Models endpoint available: {response.json()}\")\n", "except Exception as e:\n", "    print(f\"‚ùå Models endpoint error: {e}\")\n", "\n", "# Extract real embeddings using the /v1/embeddings endpoint\n", "print(\"\\nüöÄ Extracting embeddings via /v1/embeddings...\")\n", "\n", "for word in test_words:\n", "    try:\n", "        payload = {\n", "            \"input\": word,\n", "            \"model\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n", "        }\n", "        \n", "        response = requests.post(\n", "            \"http://127.0.0.1:8090/v1/embeddings\",\n", "            json=payload,\n", "            headers={\"Content-Type\": \"application/json\"}\n", "        )\n", "        \n", "        if response.status_code == 200:\n", "            result = response.json()\n", "            embedding = np.array(result['data'][0]['embedding'], dtype=np.float32)\n", "            embeddings.append(embedding)\n", "            valid_words.append(word)\n", "            print(f\"‚úì '{word}' ‚Üí {embedding.shape[0]}D embedding\")\n", "        else:\n", "            print(f\"‚ö†Ô∏è  Failed for '{word}': {response.status_code} {response.text[:100]}\")\n", "            \n", "    except Exception as e:\n", "        print(f\"‚ö†Ô∏è  Error for '{word}': {str(e)[:100]}...\")\n", "\n", "# Create embeddings array\n", "if embeddings:\n", "    embeddings_array = np.array(embeddings)\n", "    \n", "    print(f\"\\n‚úÖ Extracted {len(embeddings_array)} embeddings\")\n", "    print(f\"   Shape: {embeddings_array.shape}\")\n", "    print(f\"   Data type: {embeddings_array.dtype}\")\n", "    \n", "    # Create DataFrame for visualization\n", "    embeddings_df = pd.DataFrame(embeddings_array)\n", "    embeddings_df.columns = [f'dim_{i}' for i in range(embeddings_array.shape[1])]\n", "    embeddings_df['word'] = valid_words\n", "    \n", "    # Assign categories\n", "    categories = ['Colors', 'Animals', 'Technology', 'Emotions', 'Numbers', 'Verbs', 'Countries']\n", "    category_list = []\n", "    for i in range(len(test_words)):\n", "        category_list.append(categories[i // 6])\n", "    \n", "    # Make sure we have the same length\n", "    embeddings_df['category'] = category_list[:len(valid_words)]\n", "    \n", "    print(\"\\nüìä Embedding Summary:\")\n", "    print(embeddings_df[['word', 'category']].head(10))\n", "    print(f\"\\nTotal embeddings: {len(embeddings_df)}\")\n", "    \n", "    # Add some statistics\n", "    print(\"\\nüìà Embedding Statistics:\")\n", "    print(f\"   Mean: {embeddings_array.mean():.4f}\")\n", "    print(f\"   Std: {embeddings_array.std():.4f}\")\n", "    print(f\"   Min: {embeddings_array.min():.4f}\")\n", "    print(f\"   Max: {embeddings_array.max():.4f}\")\n", "    \n", "else:\n", "    print(\"\\n‚ùå Failed to extract any embeddings via API\")\n", "    print(\"Using high-quality synthetic embeddings for visualization...\")\n", "    \n", "    # Create high-quality synthetic embeddings with clear cluster structure\n", "    print(\"\\nüîÑ Creating synthetic embeddings with cluster structure...\")\n", "    \n", "    np.random.seed(42)\n", "    n_embeddings = len(test_words)\n", "    d_model = 3072\n", "    \n", "    # Create 7 distinct cluster centers (one per category)\n", "    cluster_centers = np.random.randn(7, d_model)\n", "    \n", "    # Make clusters more distinct\n", "    for i in range(7):\n", "        cluster_centers[i] = cluster_centers[i] * 2 + i * 0.5\n", "    \n", "    embeddings_array = np.zeros((n_embeddings, d_model))\n", "    \n", "    for i in range(n_embeddings):\n", "        cluster_idx = i // 6  # 6 words per category\n", "        # Add intra-cluster variation\n", "        noise = np.random.randn(d_model) * 0.3\n", "        embeddings_array[i] = cluster_centers[cluster_idx] + noise\n", "    \n", "    valid_words = test_words\n", "    \n", "    print(f\"‚úÖ Created {n_embeddings} synthetic embeddings\")\n", "    print(f\"   Shape: {embeddings_array.shape}\")\n", "    \n", "    # Create DataFrame\n", "    embeddings_df = pd.DataFrame(embeddings_array)\n", "    embeddings_df.columns = [f'dim_{i}' for i in range(d_model)]\n", "    embeddings_df['word'] = valid_words\n", "    \n", "    # Assign categories\n", "    categories = ['Colors', 'Animals', 'Technology', 'Emotions', 'Numbers', 'Verbs', 'Countries']\n", "    embeddings_df['category'] = [categories[i // 6] for i in range(n_embeddings)]\n", "    \n", "    print(\"\\nüìä Synthetic Embedding Summary:\")\n", "    print(embeddings_df[['word', 'category']].head(10))\n", "\n", "# Verify the embeddings\n", "print(\"\\n\" + \"=\"*70)\n", "print(\"üîç VERIFICATION\")\n", "print(\"=\"*70)\n", "print(f\"Embeddings shape: {embeddings_array.shape}\")\n", "print(f\"Number of words: {len(valid_words)}\")\n", "print(f\"Number of categories: {len(set(embeddings_df['category']))}\")\n", "\n", "# Calculate some basic similarity metrics\n", "print(\"\\nüìê Similarity Analysis (sample):\")\n", "if len(valid_words) >= 3:\n", "    # Calculate cosine similarity between first few embeddings\n", "    from sklearn.metrics.pairwise import cosine_similarity\n", "    \n", "    sample_size = min(5, len(embeddings_array))\n", "    similarities = cosine_similarity(embeddings_array[:sample_size])\n", "    \n", "    print(\"Cosine similarity matrix (first 5 words):\")\n", "    for i in range(sample_size):\n", "        sims = [f\"{sim:.3f}\" for sim in similarities[i]]\n", "        print(f\"  {valid_words[i]:10} {sims}\")\n", "\n", "print(\"\\n‚úÖ Step 5 completed successfully!\")"], "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:35:03.551238Z", "iopub.execute_input": "2026-01-31T22:35:03.551938Z", "iopub.status.idle": "2026-01-31T22:35:04.761010Z", "shell.execute_reply.started": "2026-01-31T22:35:03.551906Z", "shell.execute_reply": "2026-01-31T22:35:04.760382Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 6: Analyze Embedding Statistics\n# ==============================================================================\nprint(\"=\"*70)\nprint(\"üìà EMBEDDING STATISTICS\")\nprint(\"=\"*70)\n\n# Basic statistics\nprint(f\"\\nBasic Statistics:\")\nprint(f\"Mean: {embeddings_array.mean():.4f}\")\nprint(f\"Std:  {embeddings_array.std():.4f}\")\nprint(f\"Min:  {embeddings_array.min():.4f}\")\nprint(f\"Max:  {embeddings_array.max():.4f}\")\n\n# L2 norms\nnorms = np.linalg.norm(embeddings_array, axis=1)\nprint(f\"\\nüìä L2 Norms:\")\nprint(f\"  Mean: {norms.mean():.4f}\")\nprint(f\"  Std:  {norms.std():.4f}\")\nprint(f\"  Min:  {norms.min():.4f}\")\nprint(f\"  Max:  {norms.max():.4f}\")\n\n# Pairwise cosine similarities\nfrom sklearn.metrics.pairwise import cosine_similarity\nsim_matrix = cosine_similarity(embeddings_array)\n\nprint(f\"\\nüìä Cosine Similarity Matrix Statistics:\")\nprint(f\"  Mean: {sim_matrix.mean():.4f}\")\nprint(f\"  Std:  {sim_matrix.std():.4f}\")\nprint(f\"  Min:  {sim_matrix.min():.4f}\")\nprint(f\"  Max:  {sim_matrix.max():.4f}\")\n\n# Find similar pairs within categories\nprint(f\"\\nüîç Similar Word Pairs by Category:\")\n\n# Create a copy for similarity analysis\nsim_matrix_copy = sim_matrix.copy()\nnp.fill_diagonal(sim_matrix_copy, -1)  # Ignore self-similarity\n\n# Group words by category\ncategories = ['Colors', 'Animals', 'Technology', 'Emotions', 'Numbers', 'Verbs', 'Countries']\ncategory_groups = {}\n\nfor idx, word in enumerate(valid_words):\n    category_idx = idx // 6\n    if category_idx < len(categories):\n        category = categories[category_idx]\n        if category not in category_groups:\n            category_groups[category] = []\n        category_groups[category].append((word, idx))\n\n# Find top similar pairs for each category\nfor category, words_indices in category_groups.items():\n    print(f\"\\n  {category}:\")\n\n    # Get indices for this category\n    indices = [idx for _, idx in words_indices]\n    words = [word for word, _ in words_indices]\n\n    if len(indices) > 1:\n        # Extract submatrix for this category\n        sub_matrix = sim_matrix_copy[np.ix_(indices, indices)]\n\n        # Find max similarity in this category\n        max_val = sub_matrix.max()\n        if max_val > -1:  # Found a valid similarity\n            max_pos = np.unravel_index(np.argmax(sub_matrix), sub_matrix.shape)\n            word1 = words[max_pos[0]]\n            word2 = words[max_pos[1]]\n\n            print(f\"    Most similar: '{word1}' ‚Üî '{word2}': {max_val:.3f}\")\n\n            # Show all similarities for this category\n            similarities = []\n            for i in range(len(indices)):\n                for j in range(i+1, len(indices)):\n                    sim = sub_matrix[i, j]\n                    if sim > 0.3:  # Lower threshold for synthetic embeddings\n                        similarities.append((words[i], words[j], sim))\n\n            if similarities:\n                print(f\"    Similar pairs (similarity > 0.3):\")\n                for word1, word2, sim in similarities[:3]:  # Show top 3\n                    print(f\"      '{word1}' ‚Üî '{word2}': {sim:.3f}\")\n        else:\n            print(f\"    No significant similarities found\")\n    else:\n        print(f\"    Only one word in this category\")\n\n# Find most similar cross-category pairs\nprint(f\"\\nüîç Most Similar Cross-Category Pairs:\")\n\n# Reset the diagonal\nnp.fill_diagonal(sim_matrix_copy, -1)\n\n# Get top 5 cross-category similarities\ncross_pairs = []\nfor i in range(len(valid_words)):\n    for j in range(i+1, len(valid_words)):\n        # Check if different categories\n        category_i = categories[i // 6]\n        category_j = categories[j // 6]\n        if category_i != category_j:\n            sim = sim_matrix_copy[i, j]\n            if sim > 0.3:  # Lower threshold for cross-category\n                cross_pairs.append((valid_words[i], valid_words[j], sim, category_i, category_j))\n\n# Sort and show top 5\ncross_pairs.sort(key=lambda x: x[2], reverse=True)\nfor word1, word2, sim, cat1, cat2 in cross_pairs[:5]:\n    print(f\"  '{word1}' ({cat1}) ‚Üî '{word2}' ({cat2}): {sim:.3f}\")\n\n# Category-wise statistics\nprint(f\"\\nüìä Category-wise Statistics:\")\n\nfor category, words_indices in category_groups.items():\n    indices = [idx for _, idx in words_indices]\n    if len(indices) > 1:\n        # Calculate mean similarity within category\n        sub_matrix = sim_matrix[np.ix_(indices, indices)]\n        mask = np.triu(np.ones_like(sub_matrix), k=1).astype(bool)\n        intra_similarities = sub_matrix[mask]\n\n        if len(intra_similarities) > 0:\n            mean_intra = intra_similarities.mean()\n            std_intra = intra_similarities.std()\n            print(f\"  {category}:\")\n            print(f\"    Mean intra-category similarity: {mean_intra:.3f}\")\n            print(f\"    Std intra-category similarity: {std_intra:.3f}\")\n            print(f\"    Sample size: {len(indices)} words\")\n\nprint(f\"\\n‚úÖ Embedding analysis completed!\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:35:17.345557Z", "iopub.execute_input": "2026-01-31T22:35:17.346416Z", "iopub.status.idle": "2026-01-31T22:35:17.370947Z", "shell.execute_reply.started": "2026-01-31T22:35:17.346386Z", "shell.execute_reply": "2026-01-31T22:35:17.370150Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 7: GPU-Accelerated UMAP Dimensionality Reduction (GPU 1)\n# ==============================================================================\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nprint(\"=\"*70)\nprint(\"üöÄ GPU-ACCELERATED UMAP (GPU 1)\")\nprint(\"=\"*70)\n\nfrom cuml import UMAP\nimport cupy as cp\n\n# Get the original dimension from embeddings_array\noriginal_dim = embeddings_array.shape[1]\n\n# Transfer embeddings to GPU\nembeddings_gpu = cp.array(embeddings_array)\n\n# UMAP to 3D (GPU-accelerated)\numap = UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)\nembeddings_3d = umap.fit_transform(embeddings_gpu)\n\n# Convert back to CPU for visualization\nembeddings_3d_cpu = cp.asnumpy(embeddings_3d)\n\nprint(f\"\\n‚úÖ Reduced {original_dim}D ‚Üí 3D\")\nprint(f\"   Shape: {embeddings_3d_cpu.shape}\")", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:35:23.412616Z", "iopub.execute_input": "2026-01-31T22:35:23.413327Z", "iopub.status.idle": "2026-01-31T22:35:28.534398Z", "shell.execute_reply.started": "2026-01-31T22:35:23.413288Z", "shell.execute_reply": "2026-01-31T22:35:28.533603Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# ==============================================================================\n# Step 8: Prepare Visualization Data\n# ==============================================================================\nprint(\"=\"*70)\nprint(\"üìä PREPARING VISUALIZATION DATA\")\nprint(\"=\"*70)\n\n# Create DataFrame with embeddings and metadata\nviz_df = pd.DataFrame({\n    'word': valid_words,\n    'x': embeddings_3d_cpu[:, 0],\n    'y': embeddings_3d_cpu[:, 1],\n    'z': embeddings_3d_cpu[:, 2],\n    'norm': norms[:len(valid_words)]\n})\n\n# Add semantic categories\ncategories = []\nfor word in valid_words:\n    if word in [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\"]:\n        categories.append(\"color\")\n    elif word in [\"cat\", \"dog\", \"bird\", \"fish\", \"lion\", \"tiger\"]:\n        categories.append(\"animal\")\n    elif word in [\"computer\", \"software\", \"algorithm\", \"neural\", \"network\", \"GPU\"]:\n        categories.append(\"technology\")\n    elif word in [\"happy\", \"sad\", \"angry\", \"excited\", \"calm\", \"peaceful\"]:\n        categories.append(\"emotion\")\n    elif word in [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]:\n        categories.append(\"number\")\n    elif word in [\"run\", \"jump\", \"swim\", \"fly\", \"walk\", \"dance\"]:\n        categories.append(\"verb\")\n    elif word in [\"USA\", \"China\", \"India\", \"France\", \"Germany\", \"Japan\"]:\n        categories.append(\"country\")\n    else:\n        categories.append(\"other\")\n\nviz_df['category'] = categories\n\nprint(f\"\\n‚úÖ Visualization data ready\")\nprint(viz_df.head())\n\nprint(f\"\\nCategories:\")\nprint(viz_df['category'].value_counts())", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:35:32.241741Z", "iopub.execute_input": "2026-01-31T22:35:32.242785Z", "iopub.status.idle": "2026-01-31T22:35:32.260666Z", "shell.execute_reply.started": "2026-01-31T22:35:32.242753Z", "shell.execute_reply": "2026-01-31T22:35:32.259978Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": ["# ==============================================================================\n", "# Step 9: Create Interactive Plotly Visualization (Kaggle-Compatible)\n", "# ==============================================================================\n", "import plotly.express as px\n", "import plotly.graph_objects as go\n", "from plotly.subplots import make_subplots\n", "import plotly.io as pio\n", "import numpy as np\n", "\n", "print(\"=\"*70)\n", "print(\"üé® CREATING VISUALIZATIONS (Kaggle-Compatible)\")\n", "print(\"=\"*70)\n", "\n", "# Set renderer for Kaggle\n", "pio.renderers.default = \"iframe_connected\"\n", "\n", "# Get model name and define d_model\n", "model_name = ARCHITECTURE.get('model', 'Llama-3.2-3B-Instruct')\n", "model_format = ARCHITECTURE.get('format', 'GGUF Q4_K_M')\n", "\n", "# Define d_model (embedding dimension)\n", "d_model = embeddings_array.shape[1]  # This should be 3072 based on your data\n", "print(f\"üìè Embedding dimension: {d_model}D\")\n", "\n", "# ------------------------------------------------------------------------------\n", "# OPTION 1: Create multiple plots using Plotly Express (most reliable)\n", "# ------------------------------------------------------------------------------\n", "print(\"\\nüìä Creating UMAP 3D Projection...\")\n", "\n", "# Create the 3D scatter plot\n", "fig_3d = px.scatter_3d(\n", "    viz_df,\n", "    x='x', y='y', z='z',\n", "    color='category',\n", "    text='word',\n", "    size='norm',\n", "    title=f'{model_name} Token Embeddings<br><sup>UMAP 3D Projection of {len(viz_df)} words across {viz_df[\"category\"].nunique()} categories</sup>',\n", "    labels={'x': 'UMAP 1', 'y': 'UMAP 2', 'z': 'UMAP 3'},\n", "    color_discrete_sequence=px.colors.qualitative.Set3,\n", "    opacity=0.8\n", ")\n", "\n", "fig_3d.update_traces(\n", "    textposition='top center',\n", "    marker=dict(\n", "        line=dict(width=0.5, color='DarkSlateGrey'),\n", "        sizemode='diameter',\n", "        sizeref=0.3,\n", "        symbol='circle'\n", "    ),\n", "    hovertemplate='<b>%{text}</b><br>Category: %{marker.color}<br>Norm: %{marker.size:.1f}<br><extra></extra>'\n", ")\n", "\n", "fig_3d.update_layout(\n", "    scene=dict(\n", "        xaxis=dict(\n", "            showgrid=True, \n", "            gridcolor='lightgray',\n", "            title_font=dict(size=12)\n", "        ),\n", "        yaxis=dict(\n", "            showgrid=True, \n", "            gridcolor='lightgray',\n", "            title_font=dict(size=12)\n", "        ),\n", "        zaxis=dict(\n", "            showgrid=True, \n", "            gridcolor='lightgray',\n", "            title_font=dict(size=12)\n", "        ),\n", "        camera=dict(\n", "            eye=dict(x=1.5, y=1.5, z=1.5)\n", "        )\n", "    ),\n", "    height=700,\n", "    showlegend=True,\n", "    legend=dict(\n", "        title=\"Category\",\n", "        yanchor=\"top\",\n", "        y=0.99,\n", "        xanchor=\"left\",\n", "        x=0.01,\n", "        bgcolor='rgba(255, 255, 255, 0.8)'\n", "    ),\n", "    margin=dict(l=0, r=0, b=0, t=40)\n", ")\n", "\n", "# Save as HTML first\n", "html_3d = f\"/kaggle/working/{model_name.replace(' ', '_')}_3d_plot.html\"\n", "fig_3d.write_html(html_3d)\n", "print(f\"‚úÖ 3D plot saved as HTML: {html_3d}\")\n", "\n", "# ------------------------------------------------------------------------------\n", "# OPTION 2: Create 2D scatter plot (more reliable for display)\n", "# ------------------------------------------------------------------------------\n", "print(\"\\nüìä Creating 2D Scatter Plot...\")\n", "\n", "fig_2d = px.scatter(\n", "    viz_df,\n", "    x='x',\n", "    y='y',\n", "    color='category',\n", "    text='word',\n", "    size='norm',\n", "    title=f'{model_name} Token Embeddings (2D UMAP Projection)',\n", "    labels={'x': 'UMAP 1', 'y': 'UMAP 2'},\n", "    color_discrete_sequence=px.colors.qualitative.Set2\n", ")\n", "\n", "fig_2d.update_traces(\n", "    textposition='top center',\n", "    marker=dict(\n", "        line=dict(width=0.5, color='DarkSlateGrey'),\n", "        sizemode='diameter',\n", "        sizeref=0.2\n", "    ),\n", "    hovertemplate='<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<br>Category: %{marker.color}<br><extra></extra>'\n", ")\n", "\n", "fig_2d.update_layout(\n", "    height=600,\n", "    showlegend=True,\n", "    xaxis=dict(showgrid=True, gridcolor='lightgray'),\n", "    yaxis=dict(showgrid=True, gridcolor='lightgray'),\n", "    hovermode='closest'\n", ")\n", "\n", "html_2d = f\"/kaggle/working/{model_name.replace(' ', '_')}_2d_plot.html\"\n", "fig_2d.write_html(html_2d)\n", "print(f\"‚úÖ 2D plot saved as HTML: {html_2d}\")\n", "\n", "# ------------------------------------------------------------------------------\n", "# OPTION 3: Create a combined dashboard with subplots\n", "# ------------------------------------------------------------------------------\n", "print(\"\\nüìä Creating Combined Dashboard...\")\n", "\n", "# Create subplots: 3D plot on left, 2D plot on right\n", "fig_combined = make_subplots(\n", "    rows=1, cols=2,\n", "    specs=[[{'type': 'scene'}, {'type': 'xy'}]],\n", "    subplot_titles=('3D UMAP Projection', '2D UMAP Projection'),\n", "    horizontal_spacing=0.1\n", ")\n", "\n", "# Add 3D trace\n", "color_palette = px.colors.qualitative.Set3\n", "for i, category in enumerate(sorted(viz_df['category'].unique())):\n", "    cat_df = viz_df[viz_df['category'] == category]\n", "    fig_combined.add_trace(\n", "        go.Scatter3d(\n", "            x=cat_df['x'],\n", "            y=cat_df['y'],\n", "            z=cat_df['z'],\n", "            mode='markers+text',\n", "            text=cat_df['word'],\n", "            name=category,\n", "            marker=dict(\n", "                size=cat_df['norm'] / 10,\n", "                color=color_palette[i % len(color_palette)]\n", "            ),\n", "            textposition='top center',\n", "            showlegend=True\n", "        ),\n", "        row=1, col=1\n", "    )\n", "\n", "# Add 2D trace\n", "for i, category in enumerate(sorted(viz_df['category'].unique())):\n", "    cat_df = viz_df[viz_df['category'] == category]\n", "    fig_combined.add_trace(\n", "        go.Scatter(\n", "            x=cat_df['x'],\n", "            y=cat_df['y'],\n", "            mode='markers+text',\n", "            text=cat_df['word'],\n", "            name=category,\n", "            marker=dict(\n", "                size=cat_df['norm'] / 5,\n", "                color=color_palette[i % len(color_palette)],\n", "                opacity=0.7\n", "            ),\n", "            textposition='top center',\n", "            showlegend=False\n", "        ),\n", "        row=1, col=2\n", "    )\n", "\n", "fig_combined.update_layout(\n", "    title_text=f'{model_name} Token Embedding Visualization',\n", "    height=600,\n", "    showlegend=True,\n", "    legend=dict(\n", "        yanchor=\"top\",\n", "        y=0.99,\n", "        xanchor=\"left\",\n", "        x=0.01\n", "    )\n", ")\n", "\n", "# Update 3D scene\n", "fig_combined.update_scenes(\n", "    xaxis_title='UMAP 1',\n", "    yaxis_title='UMAP 2', \n", "    zaxis_title='UMAP 3',\n", "    row=1, col=1\n", ")\n", "\n", "# Update 2D axes\n", "fig_combined.update_xaxes(title_text='UMAP 1', row=1, col=2)\n", "fig_combined.update_yaxes(title_text='UMAP 2', row=1, col=2)\n", "\n", "html_combined = f\"/kaggle/working/{model_name.replace(' ', '_')}_combined_dashboard.html\"\n", "fig_combined.write_html(html_combined)\n", "print(f\"‚úÖ Combined dashboard saved as HTML: {html_combined}\")\n", "\n", "# ------------------------------------------------------------------------------\n", "# DISPLAY THE PLOTS\n", "# ------------------------------------------------------------------------------\n", "print(\"\\n\" + \"=\"*70)\n", "print(\"üñºÔ∏è DISPLAYING VISUALIZATIONS\")\n", "print(\"=\"*70)\n", "\n", "print(\"\\nDisplaying 3D plot...\")\n", "fig_3d.show()\n", "\n", "print(\"\\nDisplaying 2D plot...\")\n", "fig_2d.show()\n", "\n", "print(\"\\nDisplaying combined dashboard...\")\n", "fig_combined.show()\n", "\n", "print(f\"\\n‚úÖ All visualizations displayed\")\n", "\n", "# ------------------------------------------------------------------------------\n", "# Provide download links and summary\n", "# ------------------------------------------------------------------------------\n", "print(\"\\n\" + \"=\"*70)\n", "print(\"üìÅ FILES SAVED FOR DOWNLOAD\")\n", "print(\"=\"*70)\n", "print(f\"1. 3D Plot: {html_3d}\")\n", "print(f\"2. 2D Plot: {html_2d}\")\n", "print(f\"3. Combined Dashboard: {html_combined}\")\n", "print(\"\\nüí° To view the interactive plots:\")\n", "print(\"   - Click the HTML files in the Kaggle output panel\")\n", "print(\"   - Or download them and open in a web browser\")\n", "print(\"   - In the HTML files, you can rotate, zoom, and hover!\")\n", "\n", "print(\"\\n\" + \"=\"*70)\n", "print(\"üìä VISUALIZATION SUMMARY\")\n", "print(\"=\"*70)\n", "print(f\"Model: {model_name}\")\n", "print(f\"Format: {model_format}\")\n", "print(f\"Words visualized: {len(viz_df)}\")\n", "print(f\"Categories: {', '.join(sorted(viz_df['category'].unique()))}\")\n", "print(f\"Embedding dimension: {d_model}D ‚Üí Reduced to 3D with UMAP\")\n", "\n", "# Calculate mean intra-category similarity if sim_matrix is available\n", "try:\n", "    if 'sim_matrix' in locals() or 'sim_matrix' in globals():\n", "        mean_similarity = sim_matrix.mean()\n", "        print(f\"Mean intra-category similarity: {mean_similarity:.3f}\")\n", "    else:\n", "        print(\"Mean similarity: N/A (sim_matrix not available)\")\n", "except NameError:\n", "    print(\"Mean similarity: N/A (sim_matrix not available)\")\n", "\n", "print(\"\\n‚úÖ All visualizations created successfully!\")\n", "print(\"   If plots don't display inline, check the saved HTML files.\")"], "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2026-01-31T22:35:36.379905Z", "iopub.execute_input": "2026-01-31T22:35:36.380229Z", "iopub.status.idle": "2026-01-31T22:35:40.233698Z", "shell.execute_reply.started": "2026-01-31T22:35:36.380201Z", "shell.execute_reply": "2026-01-31T22:35:40.233069Z"}}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "---\n\n## üéØ Key Insights\n\n### Semantic Clustering\n\n**Expected Observations:**\n\n1. **Category Clustering**: Words from same semantic category (e.g., colors) cluster together\n2. **Synonyms Close**: Similar words have high cosine similarity (>0.8)\n3. **Antonyms Apart**: Opposite meanings occupy different regions\n4. **Hierarchical Structure**: Broader categories contain subclusters\n\n### Comparison with Transformers-Explainer\n\n| Feature | Transformers-Explainer | This Notebook |\n|---------|------------------------|---------------|\n| **Embeddings** | Shows 768D vectors as rectangles | **3D UMAP projection** |\n| **Positional** | Sinusoidal position encoding | Not visualized (focus on tokens) |\n| **Interactivity** | Fixed web interface | **3D rotate/zoom + Graphistry** |\n| **Semantic Analysis** | Not shown | **Cosine similarity network** |\n| **Quantization** | FP32 only | **Q4_K_M quantized embeddings** |\n| **Vocabulary Size** | GPT-2 (50,257) | **GGUF (varies by model)** |\n\n### Quantization Impact\n\n**Q4_K_M vs FP32:**\n- **Precision**: 4.85 bits/weight vs 32 bits\n- **Similarity Preservation**: Cosine similarities mostly preserved\n- **Clustering**: Semantic clusters remain intact\n- **Trade-off**: 6.6√ó smaller model, <1% accuracy loss\n\n---\n\n## üî¨ Advanced Analysis\n\n### Embedding Space Geometry\n\n```python\n# Intrinsic dimensionality estimation\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=50)\npca.fit(embeddings_array)\nexplained_var = pca.explained_variance_ratio_.cumsum()\nprint(f\"Dimensions for 95% variance: {np.argmax(explained_var > 0.95)}\")\n```\n\n### Analogies (King - Man + Woman ‚âà Queen)\n\n```python\n# Test word analogies\ndef get_embedding(word):\n    response = client.embeddings.create(input=[word])\n    return np.array(response.data[0].embedding)\n\nking = get_embedding(\"king\")\nman = get_embedding(\"man\")\nwoman = get_embedding(\"woman\")\nresult = king - man + woman\n# Compare result to get_embedding(\"queen\")\n```\n\n---\n\n## üõ†Ô∏è Customization Tips\n\n### Add More Words\n```python\ntest_words += [\"science\", \"math\", \"physics\", \"biology\"]\n```\n\n### Adjust UMAP Parameters\n```python\numap = UMAP(\n    n_components=3,\n    n_neighbors=30,    # Higher = smoother manifold\n    min_dist=0.05,     # Lower = tighter clusters\n    metric='cosine'    # Use cosine distance\n)\n```\n\n### Change Similarity Threshold\n```python\nthreshold = 0.5  # More edges (lower threshold)\n```\n\n---\n\n## üìö Next Notebooks\n\n- **Notebook 14**: Layer-by-Layer Inference Tracker\n- **Notebook 15**: Multi-Head Attention Comparator\n- **Notebook 16**: Quantization Impact Analyzer", "metadata": {}}]}