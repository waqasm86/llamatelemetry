{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism Explorer\n",
    "\n",
    "**Duration:** ~30 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook dives deep into **attention mechanisms** — extracting attention\n",
    "configuration from GGUF metadata, analyzing multi-head attention structure,\n",
    "and visualizing attention patterns.\n",
    "\n",
    "### What you'll learn\n",
    "1. Extract attention configuration from GGUF\n",
    "2. Understand multi-head attention dimensions\n",
    "3. Analyze attention patterns during inference\n",
    "4. Visualize per-head patterns\n",
    "5. Q/K/V weight matrix analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "!pip install -q matplotlib\n",
    "\n",
    "import llamatelemetry\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient, parse_gguf_header\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llamatelemetry.init(service_name=\"attention-explorer\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# Start server for inference experiments\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Explained\n",
    "\n",
    "In transformer models, each attention layer splits the input into multiple\n",
    "\"heads\" that attend to different aspects of the input independently.\n",
    "\n",
    "```\n",
    "Input → [Q_proj] → Q matrix → split into H heads\n",
    "      → [K_proj] → K matrix → split into H heads\n",
    "      → [V_proj] → V matrix → split into H heads\n",
    "\n",
    "Each head: Attention(Q_h, K_h, V_h) = softmax(Q_h × K_h^T / √d_k) × V_h\n",
    "\n",
    "All heads → [concatenate] → [O_proj] → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.task(name=\"extract-attention-config\")\n",
    "def extract_attention_config(model_path):\n",
    "    info = parse_gguf_header(model_path, read_tensors=True)\n",
    "    meta = info.metadata\n",
    "\n",
    "    embed_dim = meta.embedding_length or 0\n",
    "    n_layers = meta.block_count or 0\n",
    "\n",
    "    # Find Q/K/V projection tensors\n",
    "    qkv_tensors = {\"q_proj\": [], \"k_proj\": [], \"v_proj\": [], \"o_proj\": []}\n",
    "    for t in info.tensors:\n",
    "        for key in qkv_tensors:\n",
    "            if key in t.name:\n",
    "                qkv_tensors[key].append(t)\n",
    "\n",
    "    config = {\n",
    "        \"architecture\": meta.architecture,\n",
    "        \"embedding_dim\": embed_dim,\n",
    "        \"n_layers\": n_layers,\n",
    "    }\n",
    "\n",
    "    # Estimate head count from Q projection\n",
    "    if qkv_tensors[\"q_proj\"]:\n",
    "        q_shape = qkv_tensors[\"q_proj\"][0].shape\n",
    "        config[\"q_shape\"] = q_shape\n",
    "        # Try common head dimensions\n",
    "        for head_dim in [64, 128, 256]:\n",
    "            if embed_dim % head_dim == 0:\n",
    "                config[\"n_heads\"] = embed_dim // head_dim\n",
    "                config[\"head_dim\"] = head_dim\n",
    "                break\n",
    "\n",
    "    if qkv_tensors[\"k_proj\"]:\n",
    "        k_shape = qkv_tensors[\"k_proj\"][0].shape\n",
    "        config[\"k_shape\"] = k_shape\n",
    "        # GQA: K/V may have fewer heads than Q\n",
    "        k_dim = k_shape[0] if len(k_shape) > 0 else embed_dim\n",
    "        if config.get(\"head_dim\") and k_dim != embed_dim:\n",
    "            config[\"n_kv_heads\"] = k_dim // config[\"head_dim\"]\n",
    "            config[\"gqa_ratio\"] = config.get(\"n_heads\", 0) // config[\"n_kv_heads\"] if config.get(\"n_kv_heads\") else 1\n",
    "\n",
    "    return config\n",
    "\n",
    "attn_config = extract_attention_config(model_path)\n",
    "\n",
    "print(\"Attention Configuration:\")\n",
    "for k, v in attn_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Patterns\n",
    "\n",
    "Run inference and analyze how the model distributes attention across tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Run inference with timing per phase\n",
    "test_prompts = [\n",
    "    (\"Short\", \"What is AI?\"),\n",
    "    (\"Medium\", \"Explain the transformer architecture and its key innovations over previous approaches.\"),\n",
    "    (\"Long\", \"Write a detailed comparison of RNNs, LSTMs, and Transformers for sequence modeling, \"\n",
    "             \"covering their architectures, strengths, weaknesses, and modern applications.\"),\n",
    "]\n",
    "\n",
    "for name, prompt in test_prompts:\n",
    "    # Tokenize to get token count\n",
    "    tokens = client.tokenize(prompt)\n",
    "    n_input = len(tokens.tokens)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=64, temperature=0.7,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    n_output = resp.usage.completion_tokens\n",
    "    n_total = resp.usage.prompt_tokens + n_output\n",
    "\n",
    "    print(f\"{name} prompt ({n_input} tokens → {n_output} output):\")\n",
    "    print(f\"  Total time: {elapsed*1000:.0f} ms\")\n",
    "    print(f\"  Prefill: ~{resp.usage.prompt_tokens} tokens processed\")\n",
    "    print(f\"  Decode: {n_output/elapsed:.1f} tok/s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Specialization\n",
    "\n",
    "Visualize how different attention heads might specialize in different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_heads = attn_config.get(\"n_heads\", 8)\n",
    "head_dim = attn_config.get(\"head_dim\", 64)\n",
    "\n",
    "# Simulate attention patterns for visualization\n",
    "# (Real attention weights require model internals access)\n",
    "np.random.seed(42)\n",
    "seq_len = 12\n",
    "tokens_display = [\"[BOS]\", \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"looked\", \"at\", \"me\", \"[EOS]\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, min(4, n_heads // 2 + 1), figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "patterns = {\n",
    "    0: \"Local (nearby tokens)\",\n",
    "    1: \"Global (attend to [BOS])\",\n",
    "    2: \"Diagonal (self + next)\",\n",
    "    3: \"Content (nouns)\",\n",
    "    4: \"Syntactic (verbs)\",\n",
    "    5: \"Positional (even/odd)\",\n",
    "    6: \"Uniform\",\n",
    "    7: \"Sink (first token)\",\n",
    "}\n",
    "\n",
    "for h in range(min(8, len(axes))):\n",
    "    # Generate synthetic attention patterns\n",
    "    attn = np.random.rand(seq_len, seq_len) * 0.1\n",
    "    if h == 0:  # Local attention\n",
    "        for i in range(seq_len):\n",
    "            for j in range(max(0, i-2), min(seq_len, i+3)):\n",
    "                attn[i, j] += 0.5\n",
    "    elif h == 1:  # Global\n",
    "        attn[:, 0] += 0.8\n",
    "    elif h == 2:  # Diagonal\n",
    "        for i in range(seq_len):\n",
    "            attn[i, i] += 0.7\n",
    "            if i + 1 < seq_len:\n",
    "                attn[i, i+1] += 0.3\n",
    "    elif h == 3:  # Content-based\n",
    "        noun_pos = [2, 6]  # \"cat\", \"mat\"\n",
    "        for i in range(seq_len):\n",
    "            for j in noun_pos:\n",
    "                attn[i, j] += 0.6\n",
    "\n",
    "    # Normalize rows\n",
    "    attn = attn / attn.sum(axis=1, keepdims=True)\n",
    "\n",
    "    im = axes[h].imshow(attn, cmap=\"Blues\", aspect=\"auto\")\n",
    "    axes[h].set_title(f\"Head {h}: {patterns.get(h, 'Mixed')}\"[:30], fontsize=9)\n",
    "    if h >= 4:\n",
    "        axes[h].set_xticks(range(seq_len))\n",
    "        axes[h].set_xticklabels(tokens_display, rotation=45, ha=\"right\", fontsize=7)\n",
    "    else:\n",
    "        axes[h].set_xticks([])\n",
    "\n",
    "plt.suptitle(f\"Attention Head Patterns ({n_heads} heads, dim={head_dim})\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QKV Analysis\n",
    "\n",
    "Analyze the Q, K, V weight matrices from the GGUF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_full = parse_gguf_header(model_path, read_tensors=True)\n",
    "\n",
    "# Categorize attention tensors per layer\n",
    "qkv_by_layer = {}\n",
    "for t in info_full.tensors:\n",
    "    for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "        if proj in t.name:\n",
    "            # Extract layer number from tensor name\n",
    "            parts = t.name.split(\".\")\n",
    "            layer_num = None\n",
    "            for part in parts:\n",
    "                if part.isdigit():\n",
    "                    layer_num = int(part)\n",
    "                    break\n",
    "            if layer_num is not None:\n",
    "                qkv_by_layer.setdefault(layer_num, {})[proj] = t\n",
    "\n",
    "print(f\"Attention layers found: {len(qkv_by_layer)}\")\n",
    "print(f\"\\n{'Layer':<8} {'Q shape':<20} {'K shape':<20} {'V shape':<20} {'O shape'}\")\n",
    "print(\"-\" * 80)\n",
    "for layer in sorted(list(qkv_by_layer.keys()))[:5]:  # Show first 5 layers\n",
    "    tensors = qkv_by_layer[layer]\n",
    "    q_s = str(tensors.get(\"q_proj\", {}).shape) if \"q_proj\" in tensors else \"N/A\"\n",
    "    k_s = str(tensors.get(\"k_proj\", {}).shape) if \"k_proj\" in tensors else \"N/A\"\n",
    "    v_s = str(tensors.get(\"v_proj\", {}).shape) if \"v_proj\" in tensors else \"N/A\"\n",
    "    o_s = str(tensors.get(\"o_proj\", {}).shape) if \"o_proj\" in tensors else \"N/A\"\n",
    "    print(f\"{layer:<8} {q_s:<20} {k_s:<20} {v_s:<20} {o_s}\")\n",
    "\n",
    "# Check for Grouped Query Attention (GQA)\n",
    "if qkv_by_layer:\n",
    "    first_layer = qkv_by_layer[min(qkv_by_layer.keys())]\n",
    "    if \"q_proj\" in first_layer and \"k_proj\" in first_layer:\n",
    "        q_dim = first_layer[\"q_proj\"].shape[0] if first_layer[\"q_proj\"].shape else 0\n",
    "        k_dim = first_layer[\"k_proj\"].shape[0] if first_layer[\"k_proj\"].shape else 0\n",
    "        if q_dim != k_dim and k_dim > 0:\n",
    "            ratio = q_dim // k_dim\n",
    "            print(f\"\\nGrouped Query Attention (GQA) detected: ratio={ratio}:1\")\n",
    "            print(f\"  Q heads: {q_dim // head_dim}, KV heads: {k_dim // head_dim}\")\n",
    "        else:\n",
    "            print(f\"\\nStandard Multi-Head Attention (MHA): Q=K=V dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|--------|\n",
    "| **MHA vs GQA** | Modern models use Grouped Query Attention to reduce KV cache |\n",
    "| **Head dim** | Typically 64 or 128 — determines attention resolution |\n",
    "| **Head count** | More heads = more diverse attention patterns |\n",
    "| **KV cache cost** | Proportional to `n_kv_heads × head_dim × seq_len × n_layers` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
