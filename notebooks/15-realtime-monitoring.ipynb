{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Performance Monitoring\n",
    "\n",
    "**Duration:** ~25 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook demonstrates **real-time monitoring** of LLM inference —\n",
    "continuous GPU metrics, inference latency tracking, throughput analysis,\n",
    "and performance profiling.\n",
    "\n",
    "### What you'll learn\n",
    "1. Background GPU monitoring with `start_sampler()`\n",
    "2. Inference latency tracking\n",
    "3. Throughput analysis and charts\n",
    "4. Performance profiling across configurations\n",
    "5. Live monitoring in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "!pip install -q matplotlib\n",
    "\n",
    "import llamatelemetry\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from llamatelemetry.gpu import start_sampler, snapshot\n",
    "from huggingface_hub import hf_hub_download\n",
    "import time\n",
    "\n",
    "llamatelemetry.init(service_name=\"realtime-monitoring\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"0.5,0.5\", ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Background Monitoring\n",
    "\n",
    "`start_sampler()` launches a background thread that captures GPU metrics\n",
    "at regular intervals without blocking inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start background sampling at 500ms intervals\n",
    "handle = start_sampler(interval_ms=500)\n",
    "\n",
    "# Run a sustained workload\n",
    "prompts = [\n",
    "    \"Explain backpropagation in detail.\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\",\n",
    "    \"Describe the attention mechanism in transformers.\",\n",
    "    \"How does batch normalization work?\",\n",
    "    \"What are the advantages of residual connections?\",\n",
    "    \"Explain the concept of regularization.\",\n",
    "    \"What is transfer learning and why is it useful?\",\n",
    "    \"Describe the architecture of a GAN.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=64, temperature=0.7,\n",
    "    )\n",
    "\n",
    "# Stop and collect\n",
    "handle.stop()\n",
    "samples = handle.get_snapshots()\n",
    "print(f\"Collected {len(samples)} GPU samples over {prompts.__len__()} requests\")\n",
    "\n",
    "# Summary statistics per GPU\n",
    "for gpu_id in [0, 1]:\n",
    "    gpu_samples = [s for s in samples if s.gpu_id == gpu_id]\n",
    "    if gpu_samples:\n",
    "        mem = [s.mem_used_mb for s in gpu_samples]\n",
    "        util = [s.utilization_pct for s in gpu_samples]\n",
    "        print(f\"\\nGPU {gpu_id}:\")\n",
    "        print(f\"  Memory: {min(mem)}-{max(mem)} MB (avg {sum(mem)//len(mem)} MB)\")\n",
    "        print(f\"  Utilization: {min(util)}-{max(util)}% (avg {sum(util)//len(util)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Latency Tracking\n",
    "\n",
    "Measure per-request latency with traced spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = []\n",
    "token_counts = []\n",
    "token_rates = []\n",
    "\n",
    "@llamatelemetry.workflow(name=\"latency-test\")\n",
    "def latency_test(client, prompts, max_tokens=64):\n",
    "    for prompt in prompts:\n",
    "        with llamatelemetry.span(\"inference\") as span:\n",
    "            t0 = time.perf_counter()\n",
    "            resp = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens, temperature=0.7,\n",
    "            )\n",
    "            elapsed_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        tokens = resp.usage.completion_tokens\n",
    "        tps = tokens / (elapsed_ms / 1000) if elapsed_ms > 0 else 0\n",
    "\n",
    "        latencies.append(elapsed_ms)\n",
    "        token_counts.append(tokens)\n",
    "        token_rates.append(tps)\n",
    "\n",
    "test_prompts = [f\"Tell me interesting fact #{i+1} about neural networks.\" for i in range(20)]\n",
    "latency_test(client, test_prompts)\n",
    "\n",
    "import numpy as np\n",
    "print(f\"Latency statistics ({len(latencies)} requests):\")\n",
    "print(f\"  Mean: {np.mean(latencies):.0f} ms\")\n",
    "print(f\"  P50:  {np.percentile(latencies, 50):.0f} ms\")\n",
    "print(f\"  P95:  {np.percentile(latencies, 95):.0f} ms\")\n",
    "print(f\"  P99:  {np.percentile(latencies, 99):.0f} ms\")\n",
    "print(f\"  Avg tokens/s: {np.mean(token_rates):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput Analysis\n",
    "\n",
    "Measure how throughput varies with batch size and output length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test different output lengths\n",
    "output_lengths = [16, 32, 64, 128, 256]\n",
    "throughput_results = []\n",
    "\n",
    "for max_tok in output_lengths:\n",
    "    times = []\n",
    "    total_tokens = 0\n",
    "    for _ in range(5):\n",
    "        t0 = time.perf_counter()\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Explain gradient descent.\"}],\n",
    "            max_tokens=max_tok, temperature=0.7,\n",
    "        )\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        times.append(elapsed)\n",
    "        total_tokens += resp.usage.completion_tokens\n",
    "\n",
    "    avg_time = np.mean(times)\n",
    "    avg_tps = total_tokens / sum(times)\n",
    "    throughput_results.append((max_tok, avg_time * 1000, avg_tps))\n",
    "    print(f\"  max_tokens={max_tok:4d}: {avg_time*1000:.0f} ms avg, {avg_tps:.1f} tok/s\")\n",
    "\n",
    "# Plot throughput\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "lengths = [r[0] for r in throughput_results]\n",
    "avg_latencies = [r[1] for r in throughput_results]\n",
    "avg_tps = [r[2] for r in throughput_results]\n",
    "\n",
    "axes[0].plot(lengths, avg_latencies, \"o-\", color=\"steelblue\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Max Output Tokens\")\n",
    "axes[0].set_ylabel(\"Latency (ms)\")\n",
    "axes[0].set_title(\"Latency vs Output Length\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(lengths, avg_tps, \"o-\", color=\"green\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Max Output Tokens\")\n",
    "axes[1].set_ylabel(\"Tokens/sec\")\n",
    "axes[1].set_title(\"Throughput vs Output Length\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Profiling\n",
    "\n",
    "Profile different configurations to find the optimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile latency distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Latency histogram\n",
    "axes[0].hist(latencies, bins=15, color=\"steelblue\", edgecolor=\"black\", alpha=0.8)\n",
    "axes[0].axvline(np.mean(latencies), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(latencies):.0f} ms\")\n",
    "axes[0].axvline(np.percentile(latencies, 95), color=\"orange\", linestyle=\"--\", label=f\"P95: {np.percentile(latencies, 95):.0f} ms\")\n",
    "axes[0].set_xlabel(\"Latency (ms)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Latency Distribution\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Token rate over time\n",
    "axes[1].plot(range(len(token_rates)), token_rates, \"o-\", color=\"green\", markersize=4)\n",
    "axes[1].axhline(np.mean(token_rates), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(token_rates):.1f} tok/s\")\n",
    "axes[1].set_xlabel(\"Request #\")\n",
    "axes[1].set_ylabel(\"Tokens/sec\")\n",
    "axes[1].set_title(\"Token Generation Rate Over Time\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live GPU Dashboard (Jupyter)\n",
    "\n",
    "Display live GPU utilization and inference metrics in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Live monitoring loop (runs for 10 iterations)\n",
    "handle = start_sampler(interval_ms=1000)\n",
    "\n",
    "for iteration in range(10):\n",
    "    # Run inference\n",
    "    t0 = time.perf_counter()\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Fact #{iteration+1} about deep learning.\"}],\n",
    "        max_tokens=32, temperature=0.7,\n",
    "    )\n",
    "    latency = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    # Get latest GPU snapshot\n",
    "    latest = handle.get_latest()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"=== Live Dashboard (iteration {iteration+1}/10) ===\")\n",
    "    print(f\"\\nInference: {latency:.0f} ms | {resp.usage.completion_tokens} tokens | \"\n",
    "          f\"{resp.usage.completion_tokens/(latency/1000):.1f} tok/s\")\n",
    "\n",
    "    if latest:\n",
    "        for s in snapshot():\n",
    "            bar_len = s.utilization_pct // 5\n",
    "            bar = \"█\" * bar_len + \"░\" * (20 - bar_len)\n",
    "            print(f\"\\nGPU {s.gpu_id}: [{bar}] {s.utilization_pct}%\")\n",
    "            print(f\"  Memory: {s.mem_used_mb}/{s.mem_total_mb} MB\")\n",
    "            print(f\"  Temp: {s.temp_c}°C | Power: {s.power_w:.0f} W\")\n",
    "\n",
    "handle.stop()\n",
    "print(\"\\n--- Monitoring complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary — Monitoring Checklist\n",
    "\n",
    "- [x] **GPU monitoring**: `start_sampler()` for continuous background metrics\n",
    "- [x] **Latency tracking**: Per-request timing with traced spans\n",
    "- [x] **Throughput analysis**: Tokens/sec across different configurations\n",
    "- [x] **Performance profiling**: Histogram and time-series visualization\n",
    "- [x] **Live dashboard**: Real-time Jupyter display with `clear_output()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.flush()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}