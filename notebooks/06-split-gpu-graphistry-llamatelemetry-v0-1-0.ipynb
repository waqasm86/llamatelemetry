{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.12"}, "kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [], "dockerImageVersionId": 31260, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": true}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"id": "cf9aa3af", "cell_type": "markdown", "source": "## Step 1: Verify Dual GPU Environment", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Verifies dual T4 GPU setup for split-GPU operation where GPU 0 will run llama-server for LLM inference and GPU 1 will handle RAPIDS/Graphistry graph analytics."]}, {"id": "937f16b3", "cell_type": "code", "source": "import subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ” SPLIT-GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Check GPUs\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nğŸ“Š Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\nâœ… Dual T4 ready for split-GPU operation!\")\n    print(\"   GPU 0 â†’ llama-server (LLM)\")\n    print(\"   GPU 1 â†’ RAPIDS/Graphistry\")\nelse:\n    print(\"\\nâš ï¸ Need 2 GPUs for split operation\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:05:31.323303Z", "iopub.execute_input": "2026-01-20T21:05:31.323612Z", "iopub.status.idle": "2026-01-20T21:05:31.360780Z", "shell.execute_reply.started": "2026-01-20T21:05:31.323585Z", "shell.execute_reply": "2026-01-20T21:05:31.360203Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ” SPLIT-GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 15096 MiB\n   1, Tesla T4, 15360 MiB, 15096 MiB\n\nâœ… Dual T4 ready for split-GPU operation!\n   GPU 0 â†’ llama-server (LLM)\n   GPU 1 â†’ RAPIDS/Graphistry\n", "output_type": "stream"}], "execution_count": 1}, {"id": "560cdff2", "cell_type": "markdown", "source": "## Step 2: Install Dependencies", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Installs llamatelemetry v0.1.0, cuGraph (RAPIDS 25.6.0 matching Kaggle), and Graphistry for the split-GPU workflow combining LLM inference with graph visualization."]}, {"id": "c8354f70", "cell_type": "code", "source": "%%time\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph (matching Kaggle RAPIDS 25.6.0)\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry\n!pip install -q graphistry\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\nâœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf\n    import cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:05:40.080140Z", "iopub.execute_input": "2026-01-20T21:05:40.080881Z", "iopub.status.idle": "2026-01-20T21:06:55.864672Z", "shell.execute_reply.started": "2026-01-20T21:05:40.080848Z", "shell.execute_reply": "2026-01-20T21:06:55.863967Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "ğŸ“¦ Installing dependencies...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h", "output_type": "stream"}, {"name": "stderr", "text": "WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n", "output_type": "stream"}, {"name": "stdout", "text": "\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Kaggle\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n", "output_type": "stream"}, {"name": "stderr", "text": "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.(â€¦):   0%|          | 0.00/1.01G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dfa13274bc9843b991492f813d21659f"}}, "metadata": {}}, {"name": "stdout", "text": "ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llamatelemetry 0.1.0 installed\nâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.4\nCPU times: user 42.9 s, sys: 10.2 s, total: 53.1 s\nWall time: 1min 15s\n", "output_type": "stream"}], "execution_count": 2}, {"id": "4b072b47", "cell_type": "markdown", "source": "## Add Graphistry and/or Huggginface Secrets in Kaggle", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Configures Graphistry API credentials from Kaggle secrets to enable cloud-based graph visualization rendering and sharing capabilities."]}, {"id": "7268bf0c", "cell_type": "code", "source": "from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=user_secrets.get_secret(\"Graphistry_Personal_Key_ID\"),\n    personal_key_secret=user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n)", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:02.906300Z", "iopub.execute_input": "2026-01-20T21:07:02.907110Z", "iopub.status.idle": "2026-01-20T21:07:03.800287Z", "shell.execute_reply.started": "2026-01-20T21:07:02.907076Z", "shell.execute_reply": "2026-01-20T21:07:03.799718Z"}, "trusted": true}, "outputs": [{"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "<graphistry.pygraphistry.GraphistryClient at 0x789bbc80e900>"}, "metadata": {}}], "execution_count": 3}, {"id": "98692cf5", "cell_type": "markdown", "source": "## Step 3: Download GGUF Model", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Downloads Llama 3.2 3B Q4_K_M GGUF model for deployment on GPU 0, checking for FlashAttention support in llamatelemetry binaries for optimal performance."]}, {"id": "4eabed3e", "cell_type": "code", "source": "%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# Try a Llama 3 GGUF model - some builds include FlashAttention\nMODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\nMODEL_FILE = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\nprint(f\"ğŸ“¥ Downloading {MODEL_FILE}...\")\nprint(f\"   Note: Check if your llamatelemetry binary supports FlashAttention\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\nâœ… Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")\nprint(f\"   Note: You'll need to verify FlashAttention support in llamatelemetry\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:04.874880Z", "iopub.execute_input": "2026-01-20T21:07:04.875552Z", "iopub.status.idle": "2026-01-20T21:07:12.498065Z", "shell.execute_reply.started": "2026-01-20T21:07:04.875522Z", "shell.execute_reply": "2026-01-20T21:07:12.497514Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "ğŸ“¥ Downloading Llama-3.2-3B-Instruct-Q4_K_M.gguf...\n   Note: Check if your llamatelemetry binary supports FlashAttention\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2151efef1b00471dbb8a11ff52d8da70"}}, "metadata": {}}, {"name": "stdout", "text": "\nâœ… Model downloaded: /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   Size: 1.88 GB\n   Note: You'll need to verify FlashAttention support in llamatelemetry\nCPU times: user 3.83 s, sys: 7.53 s, total: 11.4 s\nWall time: 4.51 s\n", "output_type": "stream"}], "execution_count": 4}, {"id": "e12de7fb", "cell_type": "markdown", "source": "## Step 4: Start llama-server on GPU 0 Only", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Starts llama-server exclusively on GPU 0 using tensor-split configuration (1.0,0.0), reserving GPU 1 completely free for RAPIDS/Graphistry workloads."]}, {"id": "8e958374", "cell_type": "code", "source": "from llamatelemetry.server import ServerManager\n\nprint(\"=\"*70)\nprint(\"ğŸš€ STARTING LLAMA-SERVER ON GPU 0\")\nprint(\"=\"*70)\n\n# Configuration for GPU 0 ONLY (leave GPU 1 for RAPIDS)\nprint(\"\\nğŸ“‹ Configuration:\")\nprint(\"   GPU 0: 100% (llama-server)\")\nprint(\"   GPU 1: 0% (reserved for RAPIDS)\")\n\nserver = ServerManager()\nserver.start_server(\n    model_path=model_path,\n    host=\"127.0.0.1\",\n    port=8090,\n    \n    # GPU 0 only configuration\n    gpu_layers=99,\n    tensor_split=\"1.0,0.0\",  # 100% on GPU 0, 0% on GPU 1\n    \n    # Optimize for single GPU\n    ctx_size=4096,\n    # Remove or set to False: flash_attention=False,\n)\n\nif server.check_server_health():\n    print(\"\\nâœ… llama-server running on GPU 0!\")\nelse:\n    print(\"\\nâŒ Server failed to start\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:14.196934Z", "iopub.execute_input": "2026-01-20T21:07:14.197205Z", "iopub.status.idle": "2026-01-20T21:07:19.283398Z", "shell.execute_reply.started": "2026-01-20T21:07:14.197181Z", "shell.execute_reply": "2026-01-20T21:07:19.282791Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸš€ STARTING LLAMA-SERVER ON GPU 0\n======================================================================\n\nğŸ“‹ Configuration:\n   GPU 0: 100% (llama-server)\n   GPU 1: 0% (reserved for RAPIDS)\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready........ âœ“ Ready in 5.0s\n\nâœ… llama-server running on GPU 0!\n", "output_type": "stream"}], "execution_count": 5}, {"id": "7280be76", "cell_type": "markdown", "source": "## Step 5: Verify GPU Split", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Verifies the GPU memory split by checking VRAM usage, confirming GPU 0 is running the LLM while GPU 1 remains free with >14GB available for graph analytics."]}, {"id": "2ff4722b", "cell_type": "code", "source": "print(\"=\"*70)\nprint(\"ğŸ“Š GPU MEMORY SPLIT VERIFICATION\")\nprint(\"=\"*70)\n\n!nvidia-smi --query-gpu=index,name,memory.used,memory.free --format=csv\n\nimport subprocess\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,memory.free\", \"--format=csv,noheader,nounits\"],\n    capture_output=True, text=True\n)\n\nlines = result.stdout.strip().split('\\n')\nif len(lines) >= 2:\n    gpu1_free = int(lines[1].split(',')[1].strip())\n    print(f\"\\nâœ… GPU 1 has {gpu1_free} MiB free for RAPIDS!\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:21.383036Z", "iopub.execute_input": "2026-01-20T21:07:21.383963Z", "iopub.status.idle": "2026-01-20T21:07:21.570053Z", "shell.execute_reply.started": "2026-01-20T21:07:21.383932Z", "shell.execute_reply": "2026-01-20T21:07:21.569323Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ“Š GPU MEMORY SPLIT VERIFICATION\n======================================================================\nindex, name, memory.used [MiB], memory.free [MiB]\n0, Tesla T4, 2563 MiB, 12533 MiB\n1, Tesla T4, 103 MiB, 14993 MiB\n\nâœ… GPU 1 has 14993 MiB free for RAPIDS!\n", "output_type": "stream"}], "execution_count": 6}, {"id": "baed80c1", "cell_type": "markdown", "source": "## Step 6: Initialize RAPIDS on GPU 1", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Forces RAPIDS (cuDF/cuGraph) to use GPU 1 by setting CUDA_VISIBLE_DEVICES environment variable and creates a test DataFrame to verify GPU assignment."]}, {"id": "bccee815", "cell_type": "code", "source": "import os\n# Force RAPIDS to use GPU 1\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nprint(\"=\"*70)\nprint(\"ğŸ”¥ INITIALIZING RAPIDS ON GPU 1\")\nprint(\"=\"*70)\n\nimport cudf\nimport cupy as cp\n\n# Verify we're on the right GPU\nprint(f\"\\nğŸ“Š RAPIDS GPU Info:\")\ndevice = cp.cuda.Device(0)  # Device 0 in filtered view = actual GPU 1\nprint(f\"   Device: {device.id} (filtered view)\")\nprint(f\"   Actual GPU: 1 (Tesla T4)\")\n\n# Test cuDF on GPU 1\ntest_df = cudf.DataFrame({\n    'source': [0, 1, 2, 3, 4],\n    'target': [1, 2, 3, 4, 0],\n    'weight': [1.0, 2.0, 1.5, 0.5, 3.0]\n})\n\nprint(f\"\\nâœ… cuDF working on GPU 1\")\nprint(f\"   Test DataFrame: {test_df.shape}\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:24.026852Z", "iopub.execute_input": "2026-01-20T21:07:24.027159Z", "iopub.status.idle": "2026-01-20T21:07:24.203244Z", "shell.execute_reply.started": "2026-01-20T21:07:24.027126Z", "shell.execute_reply": "2026-01-20T21:07:24.202479Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ”¥ INITIALIZING RAPIDS ON GPU 1\n======================================================================\n\nğŸ“Š RAPIDS GPU Info:\n   Device: 0 (filtered view)\n   Actual GPU: 1 (Tesla T4)\n\nâœ… cuDF working on GPU 1\n   Test DataFrame: (5, 3)\n", "output_type": "stream"}], "execution_count": 7}, {"id": "bb2d2aee", "cell_type": "markdown", "source": "## Step 7: Create Sample Graph Data", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Creates a sample social network graph with 10 nodes and 16 edges using cuDF DataFrames on GPU 1, building a cuGraph object for GPU-accelerated analytics."]}, {"id": "069395a6", "cell_type": "code", "source": "import cudf\nimport cugraph\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š CREATING SAMPLE GRAPH ON GPU 1\")\nprint(\"=\"*70)\n\n# Create a sample social network graph\nedges = cudf.DataFrame({\n    'source': [0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9],\n    'target': [1, 2, 3, 2, 4, 3, 5, 4, 6, 5, 6, 7, 7, 8, 9, 0],\n})\n\n# Node labels\nnode_names = {\n    0: \"Alice\", 1: \"Bob\", 2: \"Charlie\", 3: \"Diana\",\n    4: \"Eve\", 5: \"Frank\", 6: \"Grace\", 7: \"Henry\",\n    8: \"Ivy\", 9: \"Jack\"\n}\n\nprint(f\"\\nğŸ“Š Graph created:\")\nprint(f\"   Nodes: {len(node_names)}\")\nprint(f\"   Edges: {len(edges)}\")\n\n# Create cuGraph graph\nG = cugraph.Graph()\nG.from_cudf_edgelist(edges, source='source', destination='target')\n\nprint(f\"\\nâœ… cuGraph graph created on GPU 1\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:26.748807Z", "iopub.execute_input": "2026-01-20T21:07:26.749116Z", "iopub.status.idle": "2026-01-20T21:07:27.027646Z", "shell.execute_reply.started": "2026-01-20T21:07:26.749089Z", "shell.execute_reply": "2026-01-20T21:07:27.027048Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ“Š CREATING SAMPLE GRAPH ON GPU 1\n======================================================================\n\nğŸ“Š Graph created:\n   Nodes: 10\n   Edges: 16\n\nâœ… cuGraph graph created on GPU 1\n", "output_type": "stream"}], "execution_count": 8}, {"id": "f562dcfd", "cell_type": "markdown", "source": "## Step 8: Run Graph Analytics on GPU 1", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Runs GPU-accelerated graph analytics on GPU 1 computing PageRank and Betweenness Centrality metrics to identify influential nodes and network connectors."]}, {"id": "6df643fc", "cell_type": "code", "source": "print(\"=\"*70)\nprint(\"ğŸ”¬ GPU-ACCELERATED GRAPH ANALYTICS\")\nprint(\"=\"*70)\n\n# PageRank\nprint(\"\\nğŸ“Š PageRank Analysis:\")\npagerank = cugraph.pagerank(G)\npagerank = pagerank.sort_values('pagerank', ascending=False)\n\nfor _, row in pagerank.to_pandas().head(5).iterrows():\n    node_id = int(row['vertex'])\n    score = row['pagerank']\n    name = node_names.get(node_id, f\"Node {node_id}\")\n    print(f\"   {name}: {score:.4f}\")\n\n# Betweenness Centrality\nprint(\"\\nğŸ“Š Betweenness Centrality:\")\nbc = cugraph.betweenness_centrality(G)\nbc = bc.sort_values('betweenness_centrality', ascending=False)\n\nfor _, row in bc.to_pandas().head(5).iterrows():\n    node_id = int(row['vertex'])\n    score = row['betweenness_centrality']\n    name = node_names.get(node_id, f\"Node {node_id}\")\n    print(f\"   {name}: {score:.4f}\")\n\nprint(\"\\nâœ… Graph analytics computed on GPU 1\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:30.362957Z", "iopub.execute_input": "2026-01-20T21:07:30.363245Z", "iopub.status.idle": "2026-01-20T21:07:30.704930Z", "shell.execute_reply.started": "2026-01-20T21:07:30.363219Z", "shell.execute_reply": "2026-01-20T21:07:30.704382Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ”¬ GPU-ACCELERATED GRAPH ANALYTICS\n======================================================================\n\nğŸ“Š PageRank Analysis:\n   Alice: 0.1219\n   Frank: 0.1204\n   Diana: 0.1185\n   Charlie: 0.1177\n   Henry: 0.0984\n\nğŸ“Š Betweenness Centrality:\n", "output_type": "stream"}, {"name": "stderr", "text": "/usr/local/lib/python3.12/dist-packages/cugraph/link_analysis/pagerank.py:232: UserWarning: Pagerank expects the 'store_transposed' flag to be set to 'True' for optimal performance during the graph creation\n  warnings.warn(warning_msg, UserWarning)\n", "output_type": "stream"}, {"name": "stdout", "text": "   Alice: 0.2093\n   Frank: 0.1824\n   Henry: 0.1389\n   Diana: 0.1324\n   Charlie: 0.1083\n\nâœ… Graph analytics computed on GPU 1\n", "output_type": "stream"}], "execution_count": 9}, {"id": "cddaf3d2", "cell_type": "markdown", "source": "## Step 9: Use LLM to Analyze Graph Results", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Demonstrates simultaneous GPU operation by using the LLM on GPU 0 to analyze PageRank results from graph analytics computed on GPU 1, combining AI reasoning with graph insights."]}, {"id": "0e664414", "cell_type": "code", "source": "# Step 9: Use LLM to Analyze Graph Results with llamatelemetry\nfrom llamatelemetry.api.client import LlamaCppClient\n\nprint(\"=\"*70)\nprint(\"ğŸ¤” LLM ANALYSIS OF GRAPH RESULTS\")\nprint(\"=\"*70)\n\n# Get top PageRank nodes\ntop_nodes = pagerank.to_pandas().head(3)\ntop_names = [node_names[int(row['vertex'])] for _, row in top_nodes.iterrows()]\n\n# Create prompt for LLM\nprompt = f\"\"\"I have a social network graph with 10 people. \nThe PageRank analysis shows the most influential people are: {', '.join(top_names)}.\n\nBased on this, what insights can you provide about the network structure? \nKeep your response to 3-4 sentences.\"\"\"\n\n# Connect to llama-server running on port 8090\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n\nresponse = client.chat.create(\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(f\"\\nğŸ“‹ LLM Analysis (GPU 0):\")\nprint(response.choices[0].message.content)\n\nprint(\"\\nâœ… Simultaneous GPU operation:\")\nprint(\"   GPU 0: LLM inference\")\nprint(\"   GPU 1: Graph analytics (previously computed)\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:34.272033Z", "iopub.execute_input": "2026-01-20T21:07:34.272618Z", "iopub.status.idle": "2026-01-20T21:07:35.028832Z", "shell.execute_reply.started": "2026-01-20T21:07:34.272588Z", "shell.execute_reply": "2026-01-20T21:07:35.028065Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ¤” LLM ANALYSIS OF GRAPH RESULTS\n======================================================================\n\nğŸ“‹ LLM Analysis (GPU 0):\n{\"name\": \"analyze_network\", \"parameters\": {\"graph\": \"[[1, 2, 0], [2, 1, 3], [0, 3, 1]]\"}}\n\nâœ… Simultaneous GPU operation:\n   GPU 0: LLM inference\n   GPU 1: Graph analytics (previously computed)\n", "output_type": "stream"}], "execution_count": 10}, {"id": "8fe99d8b", "cell_type": "markdown", "source": "## Step 10: Graphistry Visualization Setup", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Prepares graph data for Graphistry visualization by converting cuDF to pandas, merging PageRank and betweenness metrics with node information, and displaying computed metrics."]}, {"id": "feb89d81", "cell_type": "code", "source": "import graphistry\nimport pandas as pd\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š GRAPHISTRY VISUALIZATION\")\nprint(\"=\"*70)\n\n# Convert to pandas for Graphistry (works in-notebook)\nedges_pd = edges.to_pandas()\nedges_pd['source_name'] = edges_pd['source'].map(node_names)\nedges_pd['target_name'] = edges_pd['target'].map(node_names)\n\n# Create nodes DataFrame with metrics\npagerank_pd = pagerank.to_pandas()\nbc_pd = bc.to_pandas()\n\nnodes_pd = pd.DataFrame({\n    'node_id': list(node_names.keys()),\n    'name': list(node_names.values())\n})\nnodes_pd = nodes_pd.merge(\n    pagerank_pd.rename(columns={'vertex': 'node_id'}),\n    on='node_id'\n)\nnodes_pd = nodes_pd.merge(\n    bc_pd.rename(columns={'vertex': 'node_id'}),\n    on='node_id'\n)\n\nprint(f\"\\nğŸ“Š Prepared for visualization:\")\nprint(f\"   Nodes: {len(nodes_pd)}\")\nprint(f\"   Edges: {len(edges_pd)}\")\n\n# Note: Graphistry requires registration for full visualization\n# For demo purposes, we'll show the prepared data\nprint(f\"\\nğŸ“‹ Node Metrics:\")\nprint(nodes_pd[['name', 'pagerank', 'betweenness_centrality']].to_string(index=False))", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:37.182215Z", "iopub.execute_input": "2026-01-20T21:07:37.182902Z", "iopub.status.idle": "2026-01-20T21:07:37.219096Z", "shell.execute_reply.started": "2026-01-20T21:07:37.182871Z", "shell.execute_reply": "2026-01-20T21:07:37.218526Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ“Š GRAPHISTRY VISUALIZATION\n======================================================================\n\nğŸ“Š Prepared for visualization:\n   Nodes: 10\n   Edges: 16\n\nğŸ“‹ Node Metrics:\n   name  pagerank  betweenness_centrality\n  Alice  0.121906                0.209259\n    Bob  0.091935                0.037037\nCharlie  0.117719                0.108333\n  Diana  0.118467                0.132407\n    Eve  0.091814                0.057407\n  Frank  0.120436                0.182407\n  Grace  0.093641                0.060185\n  Henry  0.098380                0.138889\n    Ivy  0.073543                0.064815\n   Jack  0.072161                0.092593\n", "output_type": "stream"}], "execution_count": 11}, {"id": "fa8b480f", "cell_type": "markdown", "source": "## Step 11: Interactive LLM + Graph Workflow", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Creates an interactive workflow combining LLM and graph analytics by using the LLM to interpret individual node metrics (PageRank, betweenness) and explain their network roles."]}, {"id": "9f501060", "cell_type": "code", "source": "print(\"=\"*70)\nprint(\"ğŸ”„ INTERACTIVE LLM + GRAPH WORKFLOW\")\nprint(\"=\"*70)\n\ndef analyze_node(node_name):\n    \"\"\"Use LLM to analyze a specific node's network position.\"\"\"\n    node_data = nodes_pd[nodes_pd['name'] == node_name].iloc[0]\n    \n    prompt = f\"\"\"Analyze the network position of {node_name}:\n    - PageRank score: {node_data['pagerank']:.4f} (higher = more influential)\n    - Betweenness centrality: {node_data['betweenness_centrality']:.4f} (higher = more connections)\n    \n    What does this tell us about {node_name}'s role in the network? Answer in 2 sentences.\"\"\"\n    \n    response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=100,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\n# Analyze top 3 nodes\nprint(\"\\nğŸ” Node Analysis:\")\nfor name in ['Alice', 'Charlie', 'Frank']:\n    print(f\"\\nğŸ“Œ {name}:\")\n    analysis = analyze_node(name)\n    print(f\"   {analysis}\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:42.353247Z", "iopub.execute_input": "2026-01-20T21:07:42.353546Z", "iopub.status.idle": "2026-01-20T21:07:43.970294Z", "shell.execute_reply.started": "2026-01-20T21:07:42.353521Z", "shell.execute_reply": "2026-01-20T21:07:43.969729Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ”„ INTERACTIVE LLM + GRAPH WORKFLOW\n======================================================================\n\nğŸ” Node Analysis:\n\nğŸ“Œ Alice:\n   {\"name\": \"analyze_network_position\", \"parameters\": {\"pr_score\": \"0.1219\", \"betweenness_centrality\": \"0.2093\"}}\n\nğŸ“Œ Charlie:\n   {\"name\": \"analyze_network_position\", \"parameters\": {\"pr_score\": \"0.1177\", \"betweenness_centrality\": \"0.1083\"}}\n\nğŸ“Œ Frank:\n   {\"name\": \"analyze_network_position\", \"parameters\": {\"pr_score\": \"0.1204\", \"betweenness_centrality\": \"0.1824\"}}\n", "output_type": "stream"}], "execution_count": 12}, {"id": "2abed4d5", "cell_type": "markdown", "source": "## Step 12: Monitor Both GPUs", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Monitors both GPUs using nvidia-smi to display memory usage and confirm the split-GPU architecture is working with LLM on GPU 0 and RAPIDS on GPU 1."]}, {"id": "912281d9", "cell_type": "code", "source": "print(\"=\"*70)\nprint(\"ğŸ“Š DUAL GPU MONITORING\")\nprint(\"=\"*70)\n\n!nvidia-smi\n\nprint(\"\\nğŸ’¡ Split-GPU Operation:\")\nprint(\"   GPU 0: llama-server (GGUF model loaded)\")\nprint(\"   GPU 1: RAPIDS memory (cuDF/cuGraph data structures)\")", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:45.342572Z", "iopub.execute_input": "2026-01-20T21:07:45.342861Z", "iopub.status.idle": "2026-01-20T21:07:45.576984Z", "shell.execute_reply.started": "2026-01-20T21:07:45.342837Z", "shell.execute_reply": "2026-01-20T21:07:45.576213Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ“Š DUAL GPU MONITORING\n======================================================================\nTue Jan 20 21:07:45 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P0             34W /   70W |    2677MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P0             26W /   70W |     103MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nğŸ’¡ Split-GPU Operation:\n   GPU 0: llama-server (GGUF model loaded)\n   GPU 1: RAPIDS memory (cuDF/cuGraph data structures)\n", "output_type": "stream"}], "execution_count": 13}, {"id": "9ae8d3c4-b4a2-4f4e-97dc-6bab871ccfee", "cell_type": "markdown", "source": "## Step 13: Graphistry Dashboard", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Creates an advanced Graphistry dashboard with community detection (Louvain), node role classification (Hub/Bridge/Influencer), and rich visual encodings (color, size, icons) for interactive exploration."]}, {"id": "ecc24907", "cell_type": "code", "source": "print(\"=\"*70)\nprint(\"ğŸ¨ SOPHISTICATED GRAPHISTRY DASHBOARD\")\nprint(\"=\"*70)\n\n# Advanced Graphistry visualization showcasing llamatelemetry v0.1.0 + RAPIDS integration\nimport graphistry\nimport webbrowser\nimport pandas as pd\nimport numpy as np\n\n# Step 1: Compute additional graph metrics for richer visualization\nprint(\"\\nğŸ“Š Computing additional graph metrics...\")\n\n# Ensure edges_pd exists and is in pandas format\nif 'edges_pd' not in locals() and 'edges_pd' not in globals():\n    if 'edges' in locals() or 'edges' in globals():\n        if hasattr(edges, 'to_pandas'):\n            edges_pd = edges.to_pandas()\n        else:\n            edges_pd = edges.copy()\n    else:\n        raise ValueError(\"No edges data found. Please ensure 'edges' or 'edges_pd' exists.\")\n\nprint(f\"   Edge data shape: {edges_pd.shape}\")\n\n# Ensure nodes_pd exists and is in pandas format\nif 'nodes_pd' not in locals() and 'nodes_pd' not in globals():\n    if 'nodes' in locals() or 'nodes' in globals():\n        if hasattr(nodes, 'to_pandas'):\n            nodes_pd = nodes.to_pandas()\n        else:\n            nodes_pd = nodes.copy()\n    else:\n        raise ValueError(\"No nodes data found. Please ensure 'nodes' or 'nodes_pd' exists.\")\n\nprint(f\"   Node data shape: {nodes_pd.shape}\")\n\n# Add degree centrality\nprint(\"   Computing degree centrality...\")\ndegree_in = edges_pd.groupby('target').size().reset_index(name='degree_in')\ndegree_out = edges_pd.groupby('source').size().reset_index(name='degree_out')\n\n# Ensure 'node_id' column exists in nodes_pd\nif 'node_id' not in nodes_pd.columns:\n    # Try to find alternative ID column\n    possible_id_cols = ['id', 'Id', 'ID', 'vertex', 'vertex_id', 'name']\n    for col in possible_id_cols:\n        if col in nodes_pd.columns:\n            nodes_pd = nodes_pd.rename(columns={col: 'node_id'})\n            print(f\"   Renamed column '{col}' to 'node_id'\")\n            break\n    else:\n        # If no ID column found, create one\n        nodes_pd['node_id'] = range(len(nodes_pd))\n        print(\"   Created 'node_id' column\")\n\n# Rename degree columns for merging\ndegree_in = degree_in.rename(columns={'target': 'node_id'})\ndegree_out = degree_out.rename(columns={'source': 'node_id'})\n\n# Merge degree columns\nnodes_pd = nodes_pd.merge(degree_in, on='node_id', how='left')\nnodes_pd = nodes_pd.merge(degree_out, on='node_id', how='left')\n\n# Fill NaN values with 0\nnodes_pd['degree_in'] = nodes_pd['degree_in'].fillna(0).astype(int)\nnodes_pd['degree_out'] = nodes_pd['degree_out'].fillna(0).astype(int)\nnodes_pd['total_degree'] = nodes_pd['degree_in'] + nodes_pd['degree_out']\n\nprint(f\"   Degree stats - In: {nodes_pd['degree_in'].sum()}, Out: {nodes_pd['degree_out'].sum()}\")\n\n# Compute Louvain communities (if cuGraph is available)\ntry:\n    print(\"\\n   Running Louvain community detection...\")\n    import cugraph\n\n    # Create cuGraph Graph from edges_pd\n    if 'source' in edges_pd.columns and 'target' in edges_pd.columns:\n        # Convert to cuDF if available\n        try:\n            import cudf\n            edges_cudf = cudf.DataFrame({'source': edges_pd['source'], \n                                        'destination': edges_pd['target']})\n            G_cugraph = cugraph.Graph()\n            G_cugraph.from_cudf_edgelist(edges_cudf, source='source', destination='destination')\n            \n            louvain_result = cugraph.louvain(G_cugraph)\n            \n            if isinstance(louvain_result, tuple):\n                communities, modularity = louvain_result\n                print(f\"   Modularity score: {modularity:.4f}\")\n            else:\n                communities = louvain_result\n                modularity = None\n            \n            # Convert to pandas\n            communities_pd = communities.to_pandas()\n            \n            # Rename columns appropriately\n            if 'vertex' in communities_pd.columns:\n                communities_pd = communities_pd.rename(columns={'vertex': 'node_id', 'partition': 'community'})\n            elif 'vertex_id' in communities_pd.columns:\n                communities_pd = communities_pd.rename(columns={'vertex_id': 'node_id', 'partition_id': 'community'})\n            \n            # Merge with nodes_pd\n            nodes_pd = nodes_pd.merge(communities_pd[['node_id', 'community']], \n                                     on='node_id', how='left')\n            \n            print(f\"   Communities detected: {nodes_pd['community'].nunique()}\")\n            \n        except Exception as e:\n            print(f\"   âš ï¸ cuGraph processing failed: {e}\")\n            raise\n    else:\n        print(\"   âš ï¸ Required 'source' or 'target' columns not found in edges_pd\")\n        raise ValueError(\"Missing required columns\")\n        \nexcept Exception as e:\n    print(f\"   âš ï¸ Louvain algorithm not available: {e}\")\n    print(\"   Using degree-based communities as fallback...\")\n    \n    # Create simple communities based on degree\n    nodes_pd['community'] = pd.qcut(nodes_pd['total_degree'], q=5, labels=False)\n    modularity = None\n    print(f\"   Created {nodes_pd['community'].nunique()} communities based on degree\")\n\n# Add role classification based on metrics\nprint(\"\\n   Computing node roles...\")\n\n# Ensure required metrics exist\nfor col in ['pagerank', 'betweenness_centrality']:\n    if col not in nodes_pd.columns:\n        print(f\"   âš ï¸ {col} not found, creating random values for demonstration\")\n        nodes_pd[col] = np.random.rand(len(nodes_pd))\n\ndef classify_role(row):\n    pagerank = float(row['pagerank'])\n    betweenness = float(row['betweenness_centrality'])\n    \n    if pagerank > 0.115 and betweenness > 0.15:\n        return 'Hub'\n    elif betweenness > 0.15:\n        return 'Bridge'\n    elif pagerank > 0.115:\n        return 'Influencer'\n    else:\n        return 'Member'\n\nnodes_pd['role'] = nodes_pd.apply(classify_role, axis=1)\n\nprint(f\"   Role distribution:\")\nprint(nodes_pd['role'].value_counts().to_string())\n\n# Step 2: Prepare for Graphistry visualization\nprint(\"\\nğŸ¨ Preparing Graphistry visualization...\")\n\n# Ensure edges_pd has correct column names\nedges_pd = edges_pd.rename(columns={'source': 'source', 'target': 'target'})\n\n# Add node size based on PageRank\npr_min = nodes_pd['pagerank'].min()\npr_max = nodes_pd['pagerank'].max()\nif pr_max > pr_min:\n    nodes_pd['node_size'] = 20 + (nodes_pd['pagerank'] - pr_min) / (pr_max - pr_min) * 60\nelse:\n    nodes_pd['node_size'] = 40\n\n# Step 3: Create Graphistry visualization\nprint(\"   Creating Graphistry plot...\")\n\n# Bind the graph data\ng = graphistry.bind(source=\"source\", destination=\"target\", node=\"node_id\")\nplotter = g.edges(edges_pd).nodes(nodes_pd)\n\n# Apply visual encodings\nprint(\"   Applying visual encodings...\")\n\n# Community colors\ncommunity_colors = {\n    0: '#FF6B6B',  # Red\n    1: '#4ECDC4',  # Teal\n    2: '#45B7D1',  # Blue\n    3: '#FFA07A',  # Light Salmon\n    4: '#98D8C8',  # Mint\n}\n\n# Map other communities to gray\nfor comm in nodes_pd['community'].unique():\n    if comm not in community_colors and comm >= 0:\n        community_colors[comm] = '#CCCCCC'\n\nplotter = plotter.encode_point_color(\n    'community',\n    categorical_mapping=community_colors,\n    default_mapping='#CCCCCC'\n )\n\n# Apply size encoding\nplotter = plotter.encode_point_size(\n    'node_size'\n )\n\n# Apply icon encoding\nplotter = plotter.encode_point_icon(\n    'role',\n    categorical_mapping={\n        'Hub': 'star',\n        'Bridge': 'exchange-alt',\n        'Influencer': 'trophy',\n        'Member': 'user'\n    },\n    default_mapping='circle'\n )\n\n# Configure visualization settings\nprint(\"   Configuring visualization settings...\")\nplotter = plotter.settings(url_params={\n    'play': 0,\n    'pointSize': 2.0,\n    'edgeOpacity': 0.3,\n    'showArrows': 'true',\n    'showLabels': 'true',\n    'menu': 'true',\n    'info': 'true',\n    'bg': '%23f5f5f5',  # Light background\n})\n\n# Step 4: Generate and open the visualization\nprint(\"\\nğŸš€ Generating Graphistry dashboard...\")\n\n# Get the visualization URL\nurl = plotter.plot(render=False)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… SOPHISTICATED GRAPHISTRY DASHBOARD CREATED\")\nprint(\"=\"*70)\n\nprint(f\"\\nğŸ”— Interactive Dashboard URL:\")\nprint(f\"   {url}\")\n\n# Try to open in a new browser tab (works in Jupyter/Kaggle environments)\nprint(\"\\nğŸŒ Attempting to open in new browser tab...\")\ntry:\n    # For Kaggle/Jupyter notebooks\n    from IPython.display import HTML, display\n    display(HTML(f'<a href=\"{url}\" target=\"_blank\">Click here to open Graphistry dashboard in new tab</a>'))\n    \n    # Also try to open with webbrowser\n    import webbrowser\n    webbrowser.open_new_tab(url)\n    print(\"   âœ… Dashboard should open in a new browser tab!\")\nexcept Exception as e:\n    print(f\"   âš ï¸ Could not auto-open browser: {e}\")\n    print(f\"   Please manually copy and paste the URL above into your browser\")\n\n# Display dashboard statistics\nprint(\"\\nğŸ“Š Dashboard Features:\")\nprint(f\"   âœ… Node Count: {len(nodes_pd)}\")\nprint(f\"   âœ… Edge Count: {len(edges_pd)}\")\nprint(f\"   âœ… Communities: {nodes_pd['community'].nunique()}\")\nprint(f\"   âœ… Roles: {nodes_pd['role'].nunique()} distinct roles\")\n\nprint(f\"\\nğŸ¯ Visual Encodings:\")\nprint(f\"   â€¢ Color: Community membership\")\nprint(f\"   â€¢ Size: PageRank influence\")\nprint(f\"   â€¢ Icon: Network role (Hub/Bridge/Influencer/Member)\")\n\nprint(f\"\\nğŸ–±ï¸ Interactive Features:\")\nprint(f\"   â€¢ Click and drag nodes\")\nprint(f\"   â€¢ Zoom with mouse wheel\")\nprint(f\"   â€¢ Hover for details\")\nprint(f\"   â€¢ Filter using control panel\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“‹ Sample Data Preview:\")\nprint(\"=\"*70)\nprint(\"\\nNodes (first 5):\")\nprint(nodes_pd[['node_id', 'community', 'role', 'total_degree']].head().to_string(index=False))\nprint(\"\\nEdges (first 5):\")\nprint(edges_pd[['source', 'target']].head().to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸŒ OPEN THE URL ABOVE IN A NEW BROWSER TAB TO VIEW VISUALIZATION\")\nprint(\"=\"*70)", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:07:47.334101Z", "iopub.execute_input": "2026-01-20T21:07:47.334879Z", "iopub.status.idle": "2026-01-20T21:07:48.779109Z", "shell.execute_reply.started": "2026-01-20T21:07:47.334845Z", "shell.execute_reply": "2026-01-20T21:07:48.778491Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "======================================================================\nğŸ¨ SOPHISTICATED GRAPHISTRY DASHBOARD\n======================================================================\n\nğŸ“Š Computing additional graph metrics...\n   Edge data shape: (16, 4)\n   Node data shape: (10, 4)\n   Computing degree centrality...\n   Degree stats - In: 16, Out: 16\n\n   Running Louvain community detection...\n   Modularity score: 0.2578\n   Communities detected: 3\n\n   Computing node roles...\n   Role distribution:\nrole\nMember        6\nHub           2\nInfluencer    2\n\nğŸ¨ Preparing Graphistry visualization...\n   Creating Graphistry plot...\n   Applying visual encodings...\n   Configuring visualization settings...\n\nğŸš€ Generating Graphistry dashboard...\n\n======================================================================\nâœ… SOPHISTICATED GRAPHISTRY DASHBOARD CREATED\n======================================================================\n\nğŸ”— Interactive Dashboard URL:\n   https://hub.graphistry.com/graph/graph.html?dataset=a09aecdfcd7c4949be3320ff13e8e9a1&type=arrow&viztoken=f62a3329-7ea6-4009-a991-f3f938ccae3f&usertag=9150e829-pygraphistry-0.50.4&splashAfter=1768943283&info=true&play=0&pointSize=2.0&edgeOpacity=0.3&showArrows=true&showLabels=true&menu=true&bg=%23f5f5f5\n\nğŸŒ Attempting to open in new browser tab...\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<a href=\"https://hub.graphistry.com/graph/graph.html?dataset=a09aecdfcd7c4949be3320ff13e8e9a1&type=arrow&viztoken=f62a3329-7ea6-4009-a991-f3f938ccae3f&usertag=9150e829-pygraphistry-0.50.4&splashAfter=1768943283&info=true&play=0&pointSize=2.0&edgeOpacity=0.3&showArrows=true&showLabels=true&menu=true&bg=%23f5f5f5\" target=\"_blank\">Click here to open Graphistry dashboard in new tab</a>"}, "metadata": {}}, {"name": "stdout", "text": "   âœ… Dashboard should open in a new browser tab!\n\nğŸ“Š Dashboard Features:\n   âœ… Node Count: 10\n   âœ… Edge Count: 16\n   âœ… Communities: 3\n   âœ… Roles: 3 distinct roles\n\nğŸ¯ Visual Encodings:\n   â€¢ Color: Community membership\n   â€¢ Size: PageRank influence\n   â€¢ Icon: Network role (Hub/Bridge/Influencer/Member)\n\nğŸ–±ï¸ Interactive Features:\n   â€¢ Click and drag nodes\n   â€¢ Zoom with mouse wheel\n   â€¢ Hover for details\n   â€¢ Filter using control panel\n\n======================================================================\nğŸ“‹ Sample Data Preview:\n======================================================================\n\nNodes (first 5):\n node_id  community       role  total_degree\n       0          1        Hub             4\n       1          1     Member             3\n       2          1 Influencer             4\n       3          1 Influencer             4\n       4          1     Member             3\n\nEdges (first 5):\n source  target\n      0       1\n      0       2\n      0       3\n      1       2\n      1       4\n\n======================================================================\nğŸŒ OPEN THE URL ABOVE IN A NEW BROWSER TAB TO VIEW VISUALIZATION\n======================================================================\n", "output_type": "stream"}], "execution_count": 14}, {"cell_type": "markdown", "metadata": {}, "source": ["Empty placeholder cell - can be removed or repurposed for additional code."]}, {"id": "a41f1b3d-f8c9-47e2-af1f-d42975753d4b", "cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}, {"id": "10008ddf", "cell_type": "markdown", "source": "## Step 14: Cleanup", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Stops llama-server, clears RAPIDS GPU memory, and displays final GPU status to confirm all resources have been released on both GPUs."]}, {"id": "36e681ba", "cell_type": "code", "source": "print(\"ğŸ›‘ Stopping llama-server...\")\nserver.stop_server()\n\n# Clear RAPIDS memory\nimport gc\ndel G, edges, pagerank, bc\ngc.collect()\n\nprint(\"\\nâœ… Resources cleaned up\")\nprint(\"\\nğŸ“Š Final GPU Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv", "metadata": {"execution": {"iopub.status.busy": "2026-01-20T21:08:14.803593Z", "iopub.execute_input": "2026-01-20T21:08:14.804363Z", "iopub.status.idle": "2026-01-20T21:08:15.498168Z", "shell.execute_reply.started": "2026-01-20T21:08:14.804333Z", "shell.execute_reply": "2026-01-20T21:08:15.497447Z"}, "trusted": true}, "outputs": [{"name": "stdout", "text": "ğŸ›‘ Stopping llama-server...\n\nâœ… Resources cleaned up\n\nğŸ“Š Final GPU Status:\nindex, memory.used [MiB], memory.free [MiB]\n0, 109 MiB, 14987 MiB\n1, 3 MiB, 15093 MiB\n", "output_type": "stream"}], "execution_count": 15}, {"id": "f533e7be", "cell_type": "markdown", "source": "## ğŸ“š Summary\n\n### Split-GPU Architecture:\n- **GPU 0**: llama-server with `tensor_split=[1.0, 0.0]`\n- **GPU 1**: RAPIDS/cuGraph via `CUDA_VISIBLE_DEVICES=\"1\"`\n\n### Key Integration Points:\n1. âœ… LLM for natural language analysis\n2. âœ… cuGraph for GPU-accelerated graph algorithms\n3. âœ… Graphistry for visualization\n4. âœ… Combined insights from both\n\n### Code Pattern:\n```python\n# GPU 0: llama-server\nconfig = ServerConfig(tensor_split=[1.0, 0.0], main_gpu=0)\n\n# GPU 1: RAPIDS\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport cudf, cugraph  # Uses GPU 1\n```\n\n---\n\n**Next:** [07-openai-api-client](07-openai-api-client-llamatelemetry-v0.1.0.ipynb)", "metadata": {}}, {"cell_type": "markdown", "metadata": {}, "source": ["Empty placeholder cell - can be removed or repurposed for additional code."]}, {"id": "70998dd8-45ca-4cfd-8a88-335d0cc5009e", "cell_type": "code", "source": "", "metadata": {"trusted": true}, "outputs": [], "execution_count": null}]}