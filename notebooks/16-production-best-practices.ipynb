{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Deployment & Best Practices\n",
    "\n",
    "**Duration:** ~25 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook covers **production-ready patterns** for LLM deployment —\n",
    "secret management, server lifecycle, error handling, health checks,\n",
    "session tracking, and graceful shutdown.\n",
    "\n",
    "### What you'll learn\n",
    "1. Secret management on Kaggle\n",
    "2. Server lifecycle with context managers\n",
    "3. Error handling patterns\n",
    "4. Health check loops and auto-restart\n",
    "5. Per-user session tracking\n",
    "6. Graceful shutdown procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "\n",
    "import llamatelemetry\n",
    "\n",
    "# Production-grade initialization\n",
    "llamatelemetry.init(\n",
    "    service_name=\"production-app\",\n",
    "    environment=\"kaggle\",\n",
    "    sampling=\"always_on\",    # Use \"ratio\" with sampling_ratio=0.1 in production\n",
    "    redact=False,            # Set True to redact prompts in traces\n",
    "    enable_gpu=True,\n",
    ")\n",
    "print(f\"llamatelemetry {llamatelemetry.version()} — production config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secret Management\n",
    "\n",
    "Use Kaggle secrets to securely manage API keys and tokens.\n",
    "Never hardcode credentials in notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.kaggle import (\n",
    "    auto_load_secrets,\n",
    "    setup_huggingface_auth,\n",
    "    auto_configure_grafana_cloud,\n",
    ")\n",
    "\n",
    "# Auto-load all available Kaggle secrets\n",
    "secrets = auto_load_secrets()\n",
    "print(\"Loaded secrets:\")\n",
    "for key, value in secrets.items():\n",
    "    status = \"set\" if value else \"not found\"\n",
    "    print(f\"  {key}: {status}\")\n",
    "\n",
    "# Setup HuggingFace auth for model downloads\n",
    "hf_ok = setup_huggingface_auth()\n",
    "print(f\"\\nHuggingFace auth: {'configured' if hf_ok else 'not available'}\")\n",
    "\n",
    "# Setup Grafana Cloud for telemetry export\n",
    "try:\n",
    "    grafana_ok = auto_configure_grafana_cloud()\n",
    "    print(f\"Grafana Cloud: {'configured' if grafana_ok else 'not available'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Grafana Cloud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server Lifecycle Management\n",
    "\n",
    "Use `ServerManager` as a context manager for automatic cleanup on errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# Pattern 1: Explicit lifecycle\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "print(\"Server started (explicit lifecycle)\")\n",
    "\n",
    "# Always wrap in try/finally for production\n",
    "try:\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=32,\n",
    "    )\n",
    "    print(f\"Response: {resp.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Keep server running for remaining cells\n",
    "print(\"Server running for remaining examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Patterns\n",
    "\n",
    "Use `@workflow` with proper error handling and span error recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "@llamatelemetry.workflow(name=\"resilient-inference\")\n",
    "def resilient_inference(client, prompt, max_retries=3):\n",
    "    \"\"\"Inference with retry and error recording.\"\"\"\n",
    "    last_error = None\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with llamatelemetry.span(\"attempt\", attempt=attempt + 1):\n",
    "                resp = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=64, temperature=0.7,\n",
    "                )\n",
    "                return resp.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            with llamatelemetry.span(\"retry-wait\", error=str(e)):\n",
    "                import time\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "    # All retries failed\n",
    "    return f\"[Error after {max_retries} retries: {last_error}]\"\n",
    "\n",
    "# Test resilient inference\n",
    "result = resilient_inference(client, \"What is error handling in production systems?\")\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Test with graceful degradation\n",
    "@llamatelemetry.workflow(name=\"degraded-inference\")\n",
    "def inference_with_fallback(client, prompt):\n",
    "    \"\"\"Try LLM first, fall back to a canned response.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=64, temperature=0.7,\n",
    "        )\n",
    "        return {\"source\": \"llm\", \"text\": resp.choices[0].message.content}\n",
    "    except Exception as e:\n",
    "        return {\"source\": \"fallback\", \"text\": \"I'm currently unable to process your request. Please try again later.\"}\n",
    "\n",
    "result = inference_with_fallback(client, \"Explain graceful degradation.\")\n",
    "print(f\"Source: {result['source']}\")\n",
    "print(f\"Text: {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check Loop\n",
    "\n",
    "Implement periodic health checks with automatic restart on failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def health_check_loop(client, mgr, model_path, check_interval=5, max_checks=5):\n",
    "    \"\"\"Periodic health check with auto-restart.\"\"\"\n",
    "    consecutive_failures = 0\n",
    "    max_failures = 3\n",
    "\n",
    "    for i in range(max_checks):\n",
    "        try:\n",
    "            health = client.health()\n",
    "            print(f\"  Check {i+1}: {health.status} (idle={health.slots_idle}, processing={health.slots_processing})\")\n",
    "            consecutive_failures = 0\n",
    "        except Exception as e:\n",
    "            consecutive_failures += 1\n",
    "            print(f\"  Check {i+1}: FAILED ({e})\")\n",
    "\n",
    "            if consecutive_failures >= max_failures:\n",
    "                print(f\"  {max_failures} consecutive failures — restarting server...\")\n",
    "                try:\n",
    "                    mgr.stop_server()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                mgr.start_server(model_path=model_path, gpu_layers=99, ctx_size=2048)\n",
    "                mgr.wait_until_ready(timeout=60)\n",
    "                consecutive_failures = 0\n",
    "                print(\"  Server restarted successfully\")\n",
    "\n",
    "        time.sleep(check_interval)\n",
    "\n",
    "    print(\"Health check loop complete\")\n",
    "\n",
    "health_check_loop(client, mgr, model_path, check_interval=2, max_checks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Tracking\n",
    "\n",
    "Use `session()` to group requests by user or conversation for per-user observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple user sessions\n",
    "users = [\n",
    "    (\"user-123\", [\"What is Python?\", \"How do I install pip?\"]),\n",
    "    (\"user-456\", [\"Explain Docker containers.\", \"What is Kubernetes?\"]),\n",
    "    (\"user-789\", [\"What is GPU computing?\"]),\n",
    "]\n",
    "\n",
    "for user_id, questions in users:\n",
    "    with llamatelemetry.session(f\"session-{user_id}\", user_id=user_id):\n",
    "        print(f\"\\n--- {user_id} ---\")\n",
    "        for q in questions:\n",
    "            with llamatelemetry.span(\"user-query\", question=q[:50]):\n",
    "                resp = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": q}],\n",
    "                    max_tokens=32, temperature=0.7,\n",
    "                )\n",
    "                print(f\"  Q: {q}\")\n",
    "                print(f\"  A: {resp.choices[0].message.content[:80]}...\")\n",
    "\n",
    "print(\"\\nAll sessions complete — traces grouped by session_id in your backend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graceful Shutdown\n",
    "\n",
    "Always follow the proper shutdown sequence to ensure all telemetry is exported\n",
    "and resources are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "def graceful_shutdown(mgr, timeout_s=5.0):\n",
    "    \"\"\"Production shutdown sequence.\"\"\"\n",
    "    print(\"Starting graceful shutdown...\")\n",
    "\n",
    "    # Step 1: Stop accepting new requests\n",
    "    print(\"  1. Stopping server...\")\n",
    "    try:\n",
    "        mgr.stop_server()\n",
    "        print(\"     Server stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"     Server stop warning: {e}\")\n",
    "\n",
    "    # Step 2: Flush pending telemetry\n",
    "    print(\"  2. Flushing telemetry...\")\n",
    "    llamatelemetry.flush(timeout_s=timeout_s)\n",
    "    print(\"     Telemetry flushed\")\n",
    "\n",
    "    # Step 3: Shutdown SDK\n",
    "    print(\"  3. Shutting down SDK...\")\n",
    "    llamatelemetry.shutdown(timeout_s=timeout_s)\n",
    "    print(\"     SDK shutdown complete\")\n",
    "\n",
    "    # Step 4: Verify GPU resources released\n",
    "    print(\"  4. Verifying resource cleanup...\")\n",
    "    try:\n",
    "        from llamatelemetry.gpu import snapshot\n",
    "        snaps = snapshot()\n",
    "        for s in snaps:\n",
    "            print(f\"     GPU {s.gpu_id}: {s.mem_used_mb} MB used\")\n",
    "    except Exception:\n",
    "        print(\"     GPU check skipped (SDK shut down)\")\n",
    "\n",
    "    print(\"\\nGraceful shutdown complete.\")\n",
    "\n",
    "graceful_shutdown(mgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Deployment Checklist\n",
    "\n",
    "### Before Deployment\n",
    "- [ ] Model validated with `parse_gguf_header()` and `validate_gguf()`\n",
    "- [ ] VRAM requirements calculated for target hardware\n",
    "- [ ] Tensor split configured for multi-GPU\n",
    "- [ ] Context window sized for workload\n",
    "\n",
    "### Secrets & Auth\n",
    "- [ ] `HF_TOKEN` set for model downloads\n",
    "- [ ] `GRAFANA_CLOUD_*` set for telemetry export\n",
    "- [ ] `GRAPHISTRY_*` set for visualization (optional)\n",
    "- [ ] No hardcoded credentials in code\n",
    "\n",
    "### Observability\n",
    "- [ ] Sampling strategy configured (`ratio` with 1-10%)\n",
    "- [ ] Prompt redaction enabled if handling user data\n",
    "- [ ] Custom metrics defined for business KPIs\n",
    "- [ ] Session tracking for per-user observability\n",
    "\n",
    "### Reliability\n",
    "- [ ] Error handling with retries and fallbacks\n",
    "- [ ] Health check loop with auto-restart\n",
    "- [ ] Graceful shutdown with `flush()` + `shutdown()`\n",
    "- [ ] GPU monitoring with `start_sampler()`\n",
    "\n",
    "### Performance\n",
    "- [ ] Inference latency benchmarked\n",
    "- [ ] Throughput tested under load\n",
    "- [ ] Memory usage profiled over time\n",
    "- [ ] Flash attention enabled for memory savings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
