{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start with llamatelemetry v1.2.0\n",
    "\n",
    "**Duration:** ~10 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook walks you through the fundamentals of **llamatelemetry** — a CUDA-first\n",
    "OpenTelemetry Python SDK for LLM inference on Kaggle.\n",
    "\n",
    "### What you'll learn\n",
    "1. Install and initialize the SDK\n",
    "2. Verify GPU availability\n",
    "3. Download a GGUF model\n",
    "4. Start a llama-server and run inference\n",
    "5. Monitor GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llamatelemetry v1.2.0 from GitHub\n",
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the SDK\n",
    "\n",
    "A single `init()` call configures tracing, GPU monitoring, and the llama-server runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamatelemetry\n",
    "\n",
    "llamatelemetry.init(service_name=\"quickstart\")\n",
    "print(f\"llamatelemetry {llamatelemetry.version()}\")\n",
    "\n",
    "# Verify GPU availability\n",
    "devices = llamatelemetry.gpu.list_devices()\n",
    "for d in devices:\n",
    "    print(f\"  GPU {d.id}: {d.name} — {d.memory_total_mb} MB VRAM (SM {d.compute_capability})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a GGUF Model\n",
    "\n",
    "We'll use **Gemma-3 1B Q4_K_M** — a compact model that loads instantly on a single T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "print(f\"Model downloaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Server\n",
    "\n",
    "`ServerManager` wraps the bundled llama-server binary. It handles process lifecycle,\n",
    "health checks, and readiness polling automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import ServerManager\n",
    "\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "print(\"Server is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "`LlamaCppClient` provides both the native llama.cpp completion API and an\n",
    "OpenAI-compatible chat API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import LlamaCppClient\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# --- Chat completion ---\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain GPU tensor parallelism in two sentences.\"}],\n",
    "    max_tokens=128,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(\"Chat response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# --- Streaming ---\n",
    "print(\"\\nStreaming response:\")\n",
    "for chunk in client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is GGUF format?\"}],\n",
    "    max_tokens=128,\n",
    "    stream=True,\n",
    "):\n",
    "    if chunk.choices[0].text:\n",
    "        print(chunk.choices[0].text, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor GPU\n",
    "\n",
    "`gpu.snapshot()` returns a point-in-time reading of every GPU's utilization,\n",
    "memory, power, and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = llamatelemetry.gpu.snapshot()\n",
    "for s in snapshots:\n",
    "    print(\n",
    "        f\"GPU {s.gpu_id}: {s.mem_used_mb}/{s.mem_total_mb} MB \"\n",
    "        f\"({s.utilization_pct}% util, {s.temp_c}°C, {s.power_w:.0f} W)\"\n",
    "    )\n",
    "\n",
    "# Graceful shutdown — stops the server and flushes telemetry\n",
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"\\nDone — all resources released.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}