{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"81a43b95","cell_type":"markdown","source":"## Step 0.5: Verify Dual GPU Environment","metadata":{}},{"id":"ed8f6f37-d668-496a-91dd-7cad030e4678","cell_type":"code","source":"import subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ” DUAL GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Get GPU info\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nğŸ“Š Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\nâœ… Dual T4 environment confirmed!\")\n    print(\"   Total VRAM: 30GB (15GB Ã— 2)\")\nelse:\n    print(\"\\nâš ï¸ Only 1 GPU detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle settings.\")\n\n# CUDA version\nprint(\"\\nğŸ“Š CUDA Version:\")\n!nvcc --version | grep release","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T06:58:22.244886Z","iopub.execute_input":"2026-02-04T06:58:22.245308Z","iopub.status.idle":"2026-02-04T06:58:22.473222Z","shell.execute_reply.started":"2026-02-04T06:58:22.245256Z","shell.execute_reply":"2026-02-04T06:58:22.472166Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ” DUAL GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 7.5\n   1, Tesla T4, 15360 MiB, 7.5\n\nâœ… Dual T4 environment confirmed!\n   Total VRAM: 30GB (15GB Ã— 2)\n\nğŸ“Š CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n","output_type":"stream"}],"execution_count":1},{"id":"d63066d3-be13-42c7-b8e2-8c972929a00b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e562baf6-3aee-43df-9db3-e12f13ba2eef","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd3029bb-e055-4bca-b59d-64a0e1dc3290","cell_type":"markdown","source":"## Step 1: Install llamatelemetry and Check Environment","metadata":{}},{"id":"f4d5fd30-1eed-4383-9c73-be5010d65836","cell_type":"code","source":"%%time\n\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n!pip install -q sseclient-py\n\nimport llamatelemetry\nprint(f\"âœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy\n\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:01:26.490327Z","iopub.execute_input":"2026-02-04T07:01:26.490650Z","iopub.status.idle":"2026-02-04T07:03:53.694139Z","shell.execute_reply.started":"2026-02-04T07:01:26.490601Z","shell.execute_reply":"2026-02-04T07:03:53.693196Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing dependencies...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m232.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m248.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m309.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m321.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m283.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m237.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m318.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m316.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m334.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m228.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m250.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m242.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m279.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m259.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m343.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m180.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m295.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m216.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m290.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m286.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  cache_dir = str(cache_dir)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9237bb860194c32a1eb535910d9ea81"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\nâœ… llamatelemetry 0.1.0 installed\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\nCPU times: user 59.9 s, sys: 13.4 s, total: 1min 13s\nWall time: 2min 27s\n","output_type":"stream"}],"execution_count":7},{"id":"3f491189-ca25-4718-82d8-ceec97b389ef","cell_type":"code","source":"!pip install -q huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:04:05.030919Z","iopub.execute_input":"2026-02-04T07:04:05.031760Z","iopub.status.idle":"2026-02-04T07:04:08.302106Z","shell.execute_reply.started":"2026-02-04T07:04:05.031726Z","shell.execute_reply":"2026-02-04T07:04:08.301101Z"}},"outputs":[],"execution_count":8},{"id":"6d3e8557-fdcd-4c77-87d7-56866f09a324","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nhf_token = user_secrets.get_secret(\"HF_TOKEN_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:04:08.303879Z","iopub.execute_input":"2026-02-04T07:04:08.304164Z","iopub.status.idle":"2026-02-04T07:04:08.534204Z","shell.execute_reply.started":"2026-02-04T07:04:08.304133Z","shell.execute_reply":"2026-02-04T07:04:08.533526Z"}},"outputs":[],"execution_count":9},{"id":"3a217155-0cad-4686-a1ac-c494c503506c","cell_type":"code","source":"# First, set up Kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nhf_token = user_secrets.get_secret(\"HF_TOKEN_2\")\n\n# Then login to Hugging Face Hub using the token from secrets\nfrom huggingface_hub import login\n\nlogin(token=hf_token)  # Pass the token from Kaggle secrets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:04:10.359352Z","iopub.execute_input":"2026-02-04T07:04:10.359692Z","iopub.status.idle":"2026-02-04T07:04:10.667353Z","shell.execute_reply.started":"2026-02-04T07:04:10.359665Z","shell.execute_reply":"2026-02-04T07:04:10.666563Z"}},"outputs":[],"execution_count":10},{"id":"90338ebb","cell_type":"code","source":"%%time\n# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n#!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\nimport llamatelemetry\nprint(f\"âœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\n# Check GPU\nprint(\"\\nğŸ“Š GPU Info:\")\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:04:12.923344Z","iopub.execute_input":"2026-02-04T07:04:12.923666Z","iopub.status.idle":"2026-02-04T07:04:13.092318Z","shell.execute_reply.started":"2026-02-04T07:04:12.923631Z","shell.execute_reply":"2026-02-04T07:04:13.091419Z"}},"outputs":[{"name":"stdout","text":"âœ… llamatelemetry 0.1.0 installed\n\nğŸ“Š GPU Info:\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\nCPU times: user 970 Âµs, sys: 16.3 ms, total: 17.3 ms\nWall time: 164 ms\n","output_type":"stream"}],"execution_count":11},{"id":"f8008eb9","cell_type":"markdown","source":"## Step 2: Understanding Quantization Types\n\nGGUF supports multiple quantization types, organized into families:","metadata":{}},{"id":"b0e7f9b7","cell_type":"code","source":"from llamatelemetry.api.gguf import QUANT_TYPE_INFO, QuantTypeInfo\n\nprint(\"=\"*80)\nprint(\"ğŸ“‹ GGUF QUANTIZATION TYPES\")\nprint(\"=\"*80)\n\n# Group by family\nfamilies = {\n    \"Legacy (Basic)\": [\"Q4_0\", \"Q4_1\", \"Q5_0\", \"Q5_1\", \"Q8_0\"],\n    \"K-Quants (Recommended)\": [\"Q2_K\", \"Q3_K_S\", \"Q3_K_M\", \"Q3_K_L\", \"Q4_K_S\", \"Q4_K_M\", \"Q5_K_S\", \"Q5_K_M\", \"Q6_K\"],\n    \"I-Quants (Ultra-Low)\": [\"IQ2_XXS\", \"IQ2_XS\", \"IQ2_S\", \"IQ3_XXS\", \"IQ3_XS\", \"IQ3_S\", \"IQ3_M\", \"IQ4_XS\", \"IQ4_NL\"],\n    \"Full Precision\": [\"F16\", \"F32\", \"BF16\"],\n}\n\nfor family_name, types in families.items():\n    print(f\"\\nğŸ”¹ {family_name}:\")\n    print(f\"   {'Type':<12} {'Bits/Weight':<12} {'Quality':<10} {'Notes'}\")\n    print(f\"   {'-'*60}\")\n    \n    for qtype in types:\n        if qtype in QUANT_TYPE_INFO:\n            info = QUANT_TYPE_INFO[qtype]\n            quality_stars = \"â˜…\" * int(info.quality_score * 5)\n            notes = \"Needs imatrix\" if info.requires_imatrix else \"\"\n            print(f\"   {qtype:<12} {info.bits_per_weight:<12.2f} {quality_stars:<10} {notes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:04:15.952457Z","iopub.execute_input":"2026-02-04T07:04:15.953049Z","iopub.status.idle":"2026-02-04T07:04:15.983228Z","shell.execute_reply.started":"2026-02-04T07:04:15.953001Z","shell.execute_reply":"2026-02-04T07:04:15.982658Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“‹ GGUF QUANTIZATION TYPES\n================================================================================\n\nğŸ”¹ Legacy (Basic):\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n   Q4_0         4.00         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q4_1         4.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q5_0         5.00         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q5_1         5.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q8_0         8.00         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n\nğŸ”¹ K-Quants (Recommended):\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n   Q2_K         2.60         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   Q3_K_S       3.40         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   Q3_K_M       3.90         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   Q3_K_L       4.30         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q4_K_S       4.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q4_K_M       4.80         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q5_K_S       5.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q5_K_M       5.70         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   Q6_K         6.60         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n\nğŸ”¹ I-Quants (Ultra-Low):\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n   IQ2_XXS      2.00         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   IQ2_XS       2.30         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   IQ2_S        2.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   IQ3_XXS      3.00         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   IQ3_XS       3.30         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   IQ3_S        3.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Needs imatrix\n   IQ4_XS       4.00         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n   IQ4_NL       4.50         â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… \n\nğŸ”¹ Full Precision:\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n","output_type":"stream"}],"execution_count":12},{"id":"e1b9033a","cell_type":"markdown","source":"## Step 3: Quantization Size Calculator","metadata":{}},{"id":"4634fc0f","cell_type":"code","source":"from llamatelemetry.api.gguf import estimate_gguf_size\n\nprint(\"=\"*80)\nprint(\"ğŸ“Š GGUF SIZE CALCULATOR\")\nprint(\"=\"*80)\n\n# Common model sizes\nmodel_sizes = {\n    \"Gemma-3 1B\": 1,\n    \"Gemma-3 4B\": 4,\n    \"Llama-3.2 3B\": 3,\n    \"Llama-3.1 8B\": 8,\n    \"Qwen2.5 14B\": 14,\n    \"Llama-3.1 70B\": 70,\n}\n\n# Quantization types to compare\nquant_types = [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\", \"IQ3_XS\", \"F16\"]\n\nprint(f\"\\n{'Model':<18} | \", end=\"\")\nfor qt in quant_types:\n    print(f\"{qt:<10}\", end=\"\")\nprint()\nprint(\"-\" * 80)\n\nfor model_name, params_b in model_sizes.items():\n    print(f\"{model_name:<18} | \", end=\"\")\n    for qt in quant_types:\n        size_gb = estimate_gguf_size(params_b, qt)\n        print(f\"{size_gb:<10.1f}\", end=\"\")\n    print(\" GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:06:53.503546Z","iopub.execute_input":"2026-02-04T07:06:53.503902Z","iopub.status.idle":"2026-02-04T07:06:53.511188Z","shell.execute_reply.started":"2026-02-04T07:06:53.503875Z","shell.execute_reply":"2026-02-04T07:06:53.510458Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“Š GGUF SIZE CALCULATOR\n================================================================================\n\nModel              | Q4_K_M    Q5_K_M    Q6_K      Q8_0      IQ3_XS    F16       \n--------------------------------------------------------------------------------\nGemma-3 1B         | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nGemma-3 4B         | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nLlama-3.2 3B       | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nLlama-3.1 8B       | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nQwen2.5 14B        | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nLlama-3.1 70B      | 0.0       0.0       0.0       0.0       0.0       0.0        GB\n","output_type":"stream"}],"execution_count":13},{"id":"607b25c2","cell_type":"markdown","source":"## Step 4: Kaggle T4 Recommendations","metadata":{}},{"id":"4a035f19-500d-476a-8b3e-4d033d49f803","cell_type":"code","source":"from llamatelemetry.api.gguf import recommend_quant_for_kaggle\n\nprint(\"=\"*80)\nprint(\"ğŸ¯ KAGGLE T4 QUANTIZATION RECOMMENDATIONS\")\nprint(\"=\"*80)\n\n# Test various model sizes (in billions)\ntest_models = [\n    (\"Gemma-3 1B\", 1),\n    (\"Llama-3.2 3B\", 3),\n    (\"Gemma-3 4B\", 4),\n    (\"Llama-3.1 8B\", 8),\n    (\"Qwen2.5 14B\", 14),\n    (\"Llama-3.1 70B\", 70),\n]\n\nCTX = 4096  # context size passed into recommend_quant_for_kaggle\n\nprint(\"\\nğŸ”¹ Single T4 (15GB VRAM):\")\nprint(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Context':<8} {'Fits'}\")\nprint(f\"   {'-'*70}\")\n\nfor model_name, params_b in test_models:\n    rec = recommend_quant_for_kaggle(int(params_b * 1e9), dual_t4=False, context_size=CTX)\n    size = rec[\"estimated_size_gb\"]\n    print(f\"   {model_name:<18} {rec['quant_type']:<12} {size:<10.1f} {CTX:<8} {rec['fits']}\")\n\nprint(\"\\nğŸ”¹ Dual T4 (30GB VRAM):\")\nprint(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Context':<8} {'Fits'}\")\nprint(f\"   {'-'*70}\")\n\nfor model_name, params_b in test_models:\n    rec = recommend_quant_for_kaggle(int(params_b * 1e9), dual_t4=True, context_size=CTX)\n    size = rec[\"estimated_size_gb\"]\n    print(f\"   {model_name:<18} {rec['quant_type']:<12} {size:<10.1f} {CTX:<8} {rec['fits']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:13:41.327892Z","iopub.execute_input":"2026-02-04T07:13:41.328515Z","iopub.status.idle":"2026-02-04T07:13:41.335487Z","shell.execute_reply.started":"2026-02-04T07:13:41.328484Z","shell.execute_reply":"2026-02-04T07:13:41.334860Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ¯ KAGGLE T4 QUANTIZATION RECOMMENDATIONS\n================================================================================\n\nğŸ”¹ Single T4 (15GB VRAM):\n   Model              Recommended  Est. Size  Context  Fits\n   ----------------------------------------------------------------------\n   Gemma-3 1B         Q8_0         1.1        4096     True\n   Llama-3.2 3B       Q8_0         3.3        4096     True\n   Gemma-3 4B         Q8_0         4.4        4096     True\n   Llama-3.1 8B       Q8_0         8.8        4096     True\n   Qwen2.5 14B        Q5_K_M       11.0       4096     True\n   Llama-3.1 70B      IQ2_XS       22.1       4096     False\n\nğŸ”¹ Dual T4 (30GB VRAM):\n   Model              Recommended  Est. Size  Context  Fits\n   ----------------------------------------------------------------------\n   Gemma-3 1B         Q8_0         1.1        4096     True\n   Llama-3.2 3B       Q8_0         3.3        4096     True\n   Gemma-3 4B         Q8_0         4.4        4096     True\n   Llama-3.1 8B       Q8_0         8.8        4096     True\n   Qwen2.5 14B        Q8_0         15.4       4096     True\n   Llama-3.1 70B      IQ2_XS       22.1       4096     False\n","output_type":"stream"}],"execution_count":16},{"id":"7001e562","cell_type":"markdown","source":"## Step 5: K-Quants Deep Dive\n\nK-Quants are the recommended choice for most use cases.","metadata":{}},{"id":"59f961c5","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ“˜ K-QUANTS DETAILED GUIDE\")\nprint(\"=\"*80)\n\nk_quant_guide = \"\"\"\nK-Quants use a sophisticated mixed-precision approach:\n- Attention layers: Higher precision (more important for quality)\n- Feed-forward layers: Lower precision (less sensitive)\n\nğŸ”¹ Naming Convention:\n   Q{bits}_K_{size}\n   â””â”€ bits: Base quantization (2,3,4,5,6)\n      â””â”€ K: K-quant family marker\n         â””â”€ size: S=Small, M=Medium, L=Large\n\nğŸ”¹ Recommended K-Quants:\n\n   Q4_K_M (4.85 bits/weight) â­ RECOMMENDED\n   â”œâ”€â”€ Best balance of size and quality\n   â”œâ”€â”€ ~30% smaller than FP16\n   â””â”€â”€ Minimal quality loss\n\n   Q5_K_M (5.69 bits/weight) - HIGH QUALITY\n   â”œâ”€â”€ Better quality than Q4_K_M\n   â”œâ”€â”€ Good for creative writing\n   â””â”€â”€ ~20% larger than Q4_K_M\n\n   Q6_K (6.59 bits/weight) - NEAR LOSSLESS\n   â”œâ”€â”€ Almost FP16 quality\n   â”œâ”€â”€ Good for technical tasks\n   â””â”€â”€ ~35% larger than Q4_K_M\n\n   Q3_K_M (3.89 bits/weight) - MEMORY SAVER\n   â”œâ”€â”€ For larger models on limited VRAM\n   â”œâ”€â”€ Some quality degradation\n   â””â”€â”€ ~20% smaller than Q4_K_M\n\"\"\"\nprint(k_quant_guide)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:13:55.895977Z","iopub.execute_input":"2026-02-04T07:13:55.896549Z","iopub.status.idle":"2026-02-04T07:13:55.901788Z","shell.execute_reply.started":"2026-02-04T07:13:55.896517Z","shell.execute_reply":"2026-02-04T07:13:55.901078Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“˜ K-QUANTS DETAILED GUIDE\n================================================================================\n\nK-Quants use a sophisticated mixed-precision approach:\n- Attention layers: Higher precision (more important for quality)\n- Feed-forward layers: Lower precision (less sensitive)\n\nğŸ”¹ Naming Convention:\n   Q{bits}_K_{size}\n   â””â”€ bits: Base quantization (2,3,4,5,6)\n      â””â”€ K: K-quant family marker\n         â””â”€ size: S=Small, M=Medium, L=Large\n\nğŸ”¹ Recommended K-Quants:\n\n   Q4_K_M (4.85 bits/weight) â­ RECOMMENDED\n   â”œâ”€â”€ Best balance of size and quality\n   â”œâ”€â”€ ~30% smaller than FP16\n   â””â”€â”€ Minimal quality loss\n\n   Q5_K_M (5.69 bits/weight) - HIGH QUALITY\n   â”œâ”€â”€ Better quality than Q4_K_M\n   â”œâ”€â”€ Good for creative writing\n   â””â”€â”€ ~20% larger than Q4_K_M\n\n   Q6_K (6.59 bits/weight) - NEAR LOSSLESS\n   â”œâ”€â”€ Almost FP16 quality\n   â”œâ”€â”€ Good for technical tasks\n   â””â”€â”€ ~35% larger than Q4_K_M\n\n   Q3_K_M (3.89 bits/weight) - MEMORY SAVER\n   â”œâ”€â”€ For larger models on limited VRAM\n   â”œâ”€â”€ Some quality degradation\n   â””â”€â”€ ~20% smaller than Q4_K_M\n\n","output_type":"stream"}],"execution_count":17},{"id":"e5a9e958","cell_type":"markdown","source":"## Step 6: I-Quants for Large Models\n\nI-Quants enable running 70B+ models on limited hardware.","metadata":{}},{"id":"c7f2704f","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ“˜ I-QUANTS FOR LARGE MODELS\")\nprint(\"=\"*80)\n\ni_quant_guide = \"\"\"\nI-Quants (Importance-Matrix Quants) use importance matrices\nto determine which weights are most critical for quality.\n\nğŸ”¹ Key Requirements:\n   âš ï¸  Require importance matrix (imatrix) for good quality\n   âš ï¸  Without imatrix, quality suffers significantly\n   âœ… Pre-made imatrix GGUFs are available on HuggingFace\n\nğŸ”¹ I-Quant Types:\n\n   IQ3_XS (3.30 bits/weight) â­ BEST FOR 70B\n   â”œâ”€â”€ Fits 70B models in ~25GB VRAM\n   â”œâ”€â”€ Surprisingly good quality with imatrix\n   â””â”€â”€ Ideal for Kaggle dual T4 (30GB)\n\n   IQ4_XS (4.25 bits/weight) - HIGH QUALITY LOW SIZE\n   â”œâ”€â”€ Better than Q4_K_M in some benchmarks\n   â”œâ”€â”€ Slightly smaller file size\n   â””â”€â”€ Great for 13B-34B models\n\n   IQ2_XS (2.31 bits/weight) - EXTREME COMPRESSION\n   â”œâ”€â”€ For 70B+ when VRAM is very limited\n   â”œâ”€â”€ Noticeable quality degradation\n   â””â”€â”€ Use only when necessary\n\nğŸ”¹ 70B Model on Kaggle Dual T4:\n   Model: Llama-3.1-70B-Instruct-GGUF\n   Quant: IQ3_XS (~25GB)\n   Config: tensor-split 0.5,0.5\n   Context: 2048 (limited by VRAM)\n\"\"\"\nprint(i_quant_guide)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:14:00.959164Z","iopub.execute_input":"2026-02-04T07:14:00.959822Z","iopub.status.idle":"2026-02-04T07:14:00.964643Z","shell.execute_reply.started":"2026-02-04T07:14:00.959792Z","shell.execute_reply":"2026-02-04T07:14:00.963788Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“˜ I-QUANTS FOR LARGE MODELS\n================================================================================\n\nI-Quants (Importance-Matrix Quants) use importance matrices\nto determine which weights are most critical for quality.\n\nğŸ”¹ Key Requirements:\n   âš ï¸  Require importance matrix (imatrix) for good quality\n   âš ï¸  Without imatrix, quality suffers significantly\n   âœ… Pre-made imatrix GGUFs are available on HuggingFace\n\nğŸ”¹ I-Quant Types:\n\n   IQ3_XS (3.30 bits/weight) â­ BEST FOR 70B\n   â”œâ”€â”€ Fits 70B models in ~25GB VRAM\n   â”œâ”€â”€ Surprisingly good quality with imatrix\n   â””â”€â”€ Ideal for Kaggle dual T4 (30GB)\n\n   IQ4_XS (4.25 bits/weight) - HIGH QUALITY LOW SIZE\n   â”œâ”€â”€ Better than Q4_K_M in some benchmarks\n   â”œâ”€â”€ Slightly smaller file size\n   â””â”€â”€ Great for 13B-34B models\n\n   IQ2_XS (2.31 bits/weight) - EXTREME COMPRESSION\n   â”œâ”€â”€ For 70B+ when VRAM is very limited\n   â”œâ”€â”€ Noticeable quality degradation\n   â””â”€â”€ Use only when necessary\n\nğŸ”¹ 70B Model on Kaggle Dual T4:\n   Model: Llama-3.1-70B-Instruct-GGUF\n   Quant: IQ3_XS (~25GB)\n   Config: tensor-split 0.5,0.5\n   Context: 2048 (limited by VRAM)\n\n","output_type":"stream"}],"execution_count":18},{"id":"def3b025","cell_type":"markdown","source":"## Step 7: Interactive Quant Selector","metadata":{}},{"id":"91968e13","cell_type":"code","source":"from llamatelemetry.api.gguf import print_quant_guide\n\nprint(\"=\"*80)\nprint(\"ğŸ¯ INTERACTIVE QUANTIZATION GUIDE\")\nprint(\"=\"*80)\n\n# Print the full guide\nprint_quant_guide()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:14:07.368269Z","iopub.execute_input":"2026-02-04T07:14:07.368548Z","iopub.status.idle":"2026-02-04T07:14:07.373268Z","shell.execute_reply.started":"2026-02-04T07:14:07.368523Z","shell.execute_reply":"2026-02-04T07:14:07.372741Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ¯ INTERACTIVE QUANTIZATION GUIDE\n================================================================================\n======================================================================\nGGUF Quantization Guide for Kaggle T4\n======================================================================\n\nLEGEND:\n  BPW = Bits Per Weight (lower = smaller file)\n  Quality = Relative quality score (1-10, higher = better)\n  Imatrix = Whether importance matrix is recommended\n\n----------------------------------------------------------------------\nType         Gen      BPW    Quality  Imatrix  Description\n----------------------------------------------------------------------\nQ4_0         legacy   4.0    5        No       4-bit symmetric quantization\nQ4_1         legacy   4.5    6        No       4-bit asymmetric quantization with offset\nQ5_0         legacy   5.0    6        No       5-bit symmetric quantization\nQ5_1         legacy   5.5    7        No       5-bit asymmetric quantization with offset\nQ8_0         legacy   8.0    9        No       8-bit symmetric quantization (near-lossless)\nQ8_1         legacy   8.5    9        No       8-bit asymmetric quantization\nQ2_K         k-quant  2.6    3        Yes      2-bit K-quant with super-blocks\nQ3_K_S       k-quant  3.4    5        Yes      3-bit K-quant, small variant\nQ3_K_M       k-quant  3.9    6        Yes      3-bit K-quant, medium variant\nQ3_K_L       k-quant  4.3    6        No       3-bit K-quant, large variant\nQ4_K_S       k-quant  4.5    7        No       4-bit K-quant, small variant\nQ4_K_M       k-quant  4.8    8        No       4-bit K-quant, medium (RECOMMENDED)\nQ5_K_S       k-quant  5.5    8        No       5-bit K-quant, small variant\nQ5_K_M       k-quant  5.7    9        No       5-bit K-quant, medium variant\nQ6_K         k-quant  6.6    9        No       6-bit K-quant (best K-quant quality)\nIQ1_S        i-quant  1.5    1        Yes      ~1.5 bpw vector quant (experimental)\nIQ2_XXS      i-quant  2.0    2        Yes      ~2.0 bpw, extra extra small\nIQ2_XS       i-quant  2.3    3        Yes      ~2.3 bpw, extra small\nIQ2_S        i-quant  2.5    4        Yes      ~2.5 bpw, small\nIQ3_XXS      i-quant  3.0    5        Yes      ~3.0 bpw, extra extra small\nIQ3_XS       i-quant  3.3    6        Yes      ~3.3 bpw, extra small (good for 70B)\nIQ3_S        i-quant  3.5    6        Yes      ~3.5 bpw, small\nIQ4_XS       i-quant  4.0    7        No       ~4.0 bpw, near Q4_K quality\nIQ4_NL       i-quant  4.5    8        No       ~4.5 bpw, non-linear\n----------------------------------------------------------------------\n\nKAGGLE RECOMMENDATIONS:\n  Single T4 (15GB): Q4_K_M for 7B, Q3_K_M for 13B\n  Dual T4 (30GB):   Q6_K for 7B, Q4_K_M for 13B, IQ3_XS for 70B\n\nSee: https://github.com/iuliaturc/gguf-docs\n======================================================================\n","output_type":"stream"}],"execution_count":19},{"id":"5936cbe2","cell_type":"markdown","source":"## Step 8: Download and Test Different Quantizations","metadata":{}},{"id":"d785df8e","cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport os\n\nprint(\"=\"*80)\nprint(\"ğŸ“¥ DOWNLOAD GGUF MODELS FOR COMPARISON\")\nprint(\"=\"*80)\n\n# Available Gemma-3 1B quantizations from Unsloth\nmodels_to_test = {\n    \"Q4_K_M\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n    \"Q5_K_M\": \"gemma-3-1b-it-Q5_K_M.gguf\",\n    \"Q8_0\": \"gemma-3-1b-it-Q8_0.gguf\",\n}\n\nREPO = \"unsloth/gemma-3-1b-it-GGUF\"\nMODEL_DIR = \"/kaggle/working/models\"\n\nprint(f\"\\nğŸ“¥ Downloading from {REPO}...\\n\")\n\ndownloaded = {}\nfor quant, filename in models_to_test.items():\n    print(f\"   Downloading {quant}...\", end=\" \")\n    try:\n        path = hf_hub_download(\n            repo_id=REPO,\n            filename=filename,\n            local_dir=MODEL_DIR\n        )\n        size_mb = os.path.getsize(path) / (1024**2)\n        downloaded[quant] = path\n        print(f\"âœ… {size_mb:.0f} MB\")\n    except Exception as e:\n        print(f\"âŒ {e}\")\n\nprint(f\"\\nâœ… Downloaded {len(downloaded)} models for comparison\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:14:13.466580Z","iopub.execute_input":"2026-02-04T07:14:13.466885Z","iopub.status.idle":"2026-02-04T07:14:23.389248Z","shell.execute_reply.started":"2026-02-04T07:14:13.466860Z","shell.execute_reply":"2026-02-04T07:14:23.388692Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“¥ DOWNLOAD GGUF MODELS FOR COMPARISON\n================================================================================\n\nğŸ“¥ Downloading from unsloth/gemma-3-1b-it-GGUF...\n\n   Downloading Q4_K_M... ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9036891513c644e482f2d5336b29e292"}},"metadata":{}},{"name":"stdout","text":"âœ… 769 MB\n   Downloading Q5_K_M... ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q5_K_M.gguf:   0%|          | 0.00/851M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744023080c9a4bbfb57b07dced62d091"}},"metadata":{}},{"name":"stdout","text":"âœ… 812 MB\n   Downloading Q8_0... ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q8_0.gguf:   0%|          | 0.00/1.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b504d045b4c4390a22782dd60c958da"}},"metadata":{}},{"name":"stdout","text":"âœ… 1020 MB\n\nâœ… Downloaded 3 models for comparison\n","output_type":"stream"}],"execution_count":20},{"id":"223a69b6","cell_type":"markdown","source":"## Step 9: Benchmark Different Quantizations","metadata":{}},{"id":"1ede471f-6d14-4f17-810f-a887aa30af14","cell_type":"code","source":"# Step 9 (fixed + size guard + flash_attn auto)\n\nfrom llamatelemetry.server import ServerManager\nfrom llamatelemetry.api.client import LlamaCppClient\nimport time, os\n\nprint(\"=\"*80)\nprint(\"ğŸ“Š QUANTIZATION BENCHMARK\")\nprint(\"=\"*80)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nMAX_GB_SINGLE_T4 = 12.0\n\ntest_prompt = \"Explain what CUDA is in exactly 3 sentences.\"\nresults = {}\n\nfor quant, model_path in downloaded.items():\n    try:\n        size_gb = os.path.getsize(model_path) / (1024**3)\n        if size_gb > MAX_GB_SINGLE_T4:\n            print(f\"\\nğŸ”¹ Skipping {quant} (size {size_gb:.1f} GB > {MAX_GB_SINGLE_T4} GB)\")\n            continue\n    except Exception:\n        pass\n\n    print(f\"\\nğŸ”¹ Testing {quant}...\")\n\n    server = ServerManager(server_url=\"http://127.0.0.1:8090\")\n\n    try:\n        server.start_server(\n            model_path=model_path,\n            host=\"127.0.0.1\",\n            port=8090,\n            gpu_layers=99,\n            ctx_size=2048,\n            flash_attn=\"auto\",  # âœ… adapts based on GPU/kernel support\n            timeout=90,\n            verbose=True,\n        )\n\n        if not server.check_server_health(timeout=60):\n            print(\"   âŒ Failed to start\")\n            server.stop_server()\n            time.sleep(2)\n            continue\n\n        client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n\n        client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n            max_tokens=10\n        )\n\n        start = time.time()\n        response = client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": test_prompt}],\n            max_tokens=100,\n            temperature=0.7\n        )\n        elapsed = time.time() - start\n\n        tokens = response.usage.completion_tokens if response.usage else 0\n        tok_per_sec = tokens / elapsed if elapsed > 0 else 0\n\n        results[quant] = {\n            \"tokens\": tokens,\n            \"time\": elapsed,\n            \"tok_per_sec\": tok_per_sec,\n            \"response\": response.choices[0].message.content[:100]\n        }\n\n        print(f\"   Tokens: {tokens}, Time: {elapsed:.2f}s, Speed: {tok_per_sec:.1f} tok/s\")\n\n    except Exception as e:\n        print(f\"   âŒ Failed: {e}\")\n\n    finally:\n        server.stop_server()\n        time.sleep(2)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“Š BENCHMARK SUMMARY\")\nprint(\"=\"*80)\n\nfor quant, info in results.items():\n    print(f\"{quant:<10} | {info['tok_per_sec']:.1f} tok/s | {info['time']:.2f}s | {info['tokens']} tokens\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:30:11.850202Z","iopub.execute_input":"2026-02-04T07:30:11.850955Z","iopub.status.idle":"2026-02-04T07:30:28.010098Z","shell.execute_reply.started":"2026-02-04T07:30:11.850924Z","shell.execute_reply":"2026-02-04T07:30:28.009349Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“Š QUANTIZATION BENCHMARK\n================================================================================\n\nğŸ”¹ Testing Q4_K_M...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... âœ“ Ready in 2.0s\n   Tokens: 90, Time: 1.04s, Speed: 86.2 tok/s\n\nğŸ”¹ Testing Q5_K_M...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q5_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... âœ“ Ready in 2.0s\n   Tokens: 71, Time: 0.84s, Speed: 84.6 tok/s\n\nğŸ”¹ Testing Q8_0...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q8_0.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... âœ“ Ready in 2.0s\n   Tokens: 78, Time: 1.00s, Speed: 78.2 tok/s\n\n================================================================================\nğŸ“Š BENCHMARK SUMMARY\n================================================================================\nQ4_K_M     | 86.2 tok/s | 1.04s | 90 tokens\nQ5_K_M     | 84.6 tok/s | 0.84s | 71 tokens\nQ8_0       | 78.2 tok/s | 1.00s | 78 tokens\n","output_type":"stream"}],"execution_count":25},{"id":"3e2605aa-a8d8-4a6c-8dd6-343c13cae3d2","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"363cce9f-bad6-4b78-9555-38da7098f4d9","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"724c73a8-a8ea-46fb-bc13-2608473c0af0","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1b2e6c06","cell_type":"markdown","source":"## Step 10: Creating Custom Quantizations\n\nUse llama-quantize to create your own quantized models.","metadata":{}},{"id":"997e52aa","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ”§ CREATING CUSTOM QUANTIZATIONS\")\nprint(\"=\"*80)\n\nquantize_guide = \"\"\"\nTo quantize a model yourself, use llama-quantize:\n\nğŸ”¹ Basic Usage:\n   llama-quantize input.gguf output.gguf Q4_K_M\n\nğŸ”¹ With Importance Matrix (for I-quants):\n   # First, generate importance matrix\n   llama-imatrix -m model.gguf -f calibration.txt -o imatrix.dat\n   \n   # Then quantize with imatrix\n   llama-quantize --imatrix imatrix.dat input.gguf output.gguf IQ3_XS\n\nğŸ”¹ Available in llamatelemetry:\n   from llamatelemetry.quantization import quantize_model\n   \n   quantize_model(\n       input_path=\"model-f16.gguf\",\n       output_path=\"model-q4km.gguf\",\n       quant_type=\"Q4_K_M\"\n   )\n\nğŸ”¹ From Unsloth/HuggingFace:\n   Most models on HuggingFace are already pre-quantized.\n   Look for repos with '-GGUF' suffix:\n   - unsloth/gemma-3-4b-it-GGUF\n   - unsloth/Llama-3.2-3B-Instruct-GGUF\n   - bartowski/Qwen2.5-14B-Instruct-GGUF\n\"\"\"\nprint(quantize_guide)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:31:09.700593Z","iopub.execute_input":"2026-02-04T07:31:09.701182Z","iopub.status.idle":"2026-02-04T07:31:09.705984Z","shell.execute_reply.started":"2026-02-04T07:31:09.701151Z","shell.execute_reply":"2026-02-04T07:31:09.705377Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ”§ CREATING CUSTOM QUANTIZATIONS\n================================================================================\n\nTo quantize a model yourself, use llama-quantize:\n\nğŸ”¹ Basic Usage:\n   llama-quantize input.gguf output.gguf Q4_K_M\n\nğŸ”¹ With Importance Matrix (for I-quants):\n   # First, generate importance matrix\n   llama-imatrix -m model.gguf -f calibration.txt -o imatrix.dat\n   \n   # Then quantize with imatrix\n   llama-quantize --imatrix imatrix.dat input.gguf output.gguf IQ3_XS\n\nğŸ”¹ Available in llamatelemetry:\n   from llamatelemetry.quantization import quantize_model\n   \n   quantize_model(\n       input_path=\"model-f16.gguf\",\n       output_path=\"model-q4km.gguf\",\n       quant_type=\"Q4_K_M\"\n   )\n\nğŸ”¹ From Unsloth/HuggingFace:\n   Most models on HuggingFace are already pre-quantized.\n   Look for repos with '-GGUF' suffix:\n   - unsloth/gemma-3-4b-it-GGUF\n   - unsloth/Llama-3.2-3B-Instruct-GGUF\n   - bartowski/Qwen2.5-14B-Instruct-GGUF\n\n","output_type":"stream"}],"execution_count":26},{"id":"3a376df4","cell_type":"markdown","source":"## Step 11: Quick Reference Table","metadata":{}},{"id":"35089e61","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ“‹ QUICK REFERENCE: MODEL + QUANT â†’ KAGGLE FEASIBILITY\")\nprint(\"=\"*80)\n\nreference = \"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Model       â”‚ Quant     â”‚ Size (GB) â”‚ Kaggle T4  â”‚ Notes                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1B params   â”‚ Q4_K_M    â”‚ 0.6       â”‚ âœ… Single  â”‚ Fast, high quality     â”‚\nâ”‚ 1B params   â”‚ Q8_0      â”‚ 1.1       â”‚ âœ… Single  â”‚ Best quality for 1B    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 3B params   â”‚ Q4_K_M    â”‚ 1.8       â”‚ âœ… Single  â”‚ Recommended            â”‚\nâ”‚ 3B params   â”‚ Q5_K_M    â”‚ 2.1       â”‚ âœ… Single  â”‚ Higher quality         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 4B params   â”‚ Q4_K_M    â”‚ 2.4       â”‚ âœ… Single  â”‚ â­ Sweet spot          â”‚\nâ”‚ 4B params   â”‚ Q6_K      â”‚ 3.3       â”‚ âœ… Single  â”‚ Near lossless          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 7-8B params â”‚ Q4_K_M    â”‚ 4.5       â”‚ âœ… Single  â”‚ Popular choice         â”‚\nâ”‚ 7-8B params â”‚ Q5_K_M    â”‚ 5.3       â”‚ âœ… Single  â”‚ Better for coding      â”‚\nâ”‚ 7-8B params â”‚ Q6_K      â”‚ 6.0       â”‚ âš ï¸ Single  â”‚ Tight fit, 4K ctx      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 13-14B      â”‚ Q4_K_M    â”‚ 8.0       â”‚ âš ï¸ Single  â”‚ 2K context max         â”‚\nâ”‚ 13-14B      â”‚ Q4_K_M    â”‚ 8.0       â”‚ âœ… Dual    â”‚ Split across GPUs      â”‚\nâ”‚ 13-14B      â”‚ IQ3_XS    â”‚ 5.5       â”‚ âœ… Single  â”‚ Quality trade-off      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 30-34B      â”‚ Q4_K_M    â”‚ 19        â”‚ âœ… Dual    â”‚ tensor-split 0.5,0.5   â”‚\nâ”‚ 30-34B      â”‚ IQ3_XS    â”‚ 12        â”‚ âš ï¸ Single  â”‚ Low context            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 70B params  â”‚ IQ3_XS    â”‚ 25        â”‚ âœ… Dual    â”‚ â­ 70B on Kaggle!      â”‚\nâ”‚ 70B params  â”‚ IQ2_XS    â”‚ 19        â”‚ âœ… Dual    â”‚ Lower quality          â”‚\nâ”‚ 70B params  â”‚ Q4_K_M    â”‚ 40        â”‚ âŒ         â”‚ Too large              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nLegend:\n  âœ… Works well    âš ï¸ Possible with limits    âŒ Won't fit\n\"\"\"\nprint(reference)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T07:31:15.131717Z","iopub.execute_input":"2026-02-04T07:31:15.132043Z","iopub.status.idle":"2026-02-04T07:31:15.138764Z","shell.execute_reply.started":"2026-02-04T07:31:15.132000Z","shell.execute_reply":"2026-02-04T07:31:15.137914Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“‹ QUICK REFERENCE: MODEL + QUANT â†’ KAGGLE FEASIBILITY\n================================================================================\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Model       â”‚ Quant     â”‚ Size (GB) â”‚ Kaggle T4  â”‚ Notes                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1B params   â”‚ Q4_K_M    â”‚ 0.6       â”‚ âœ… Single  â”‚ Fast, high quality     â”‚\nâ”‚ 1B params   â”‚ Q8_0      â”‚ 1.1       â”‚ âœ… Single  â”‚ Best quality for 1B    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 3B params   â”‚ Q4_K_M    â”‚ 1.8       â”‚ âœ… Single  â”‚ Recommended            â”‚\nâ”‚ 3B params   â”‚ Q5_K_M    â”‚ 2.1       â”‚ âœ… Single  â”‚ Higher quality         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 4B params   â”‚ Q4_K_M    â”‚ 2.4       â”‚ âœ… Single  â”‚ â­ Sweet spot          â”‚\nâ”‚ 4B params   â”‚ Q6_K      â”‚ 3.3       â”‚ âœ… Single  â”‚ Near lossless          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 7-8B params â”‚ Q4_K_M    â”‚ 4.5       â”‚ âœ… Single  â”‚ Popular choice         â”‚\nâ”‚ 7-8B params â”‚ Q5_K_M    â”‚ 5.3       â”‚ âœ… Single  â”‚ Better for coding      â”‚\nâ”‚ 7-8B params â”‚ Q6_K      â”‚ 6.0       â”‚ âš ï¸ Single  â”‚ Tight fit, 4K ctx      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 13-14B      â”‚ Q4_K_M    â”‚ 8.0       â”‚ âš ï¸ Single  â”‚ 2K context max         â”‚\nâ”‚ 13-14B      â”‚ Q4_K_M    â”‚ 8.0       â”‚ âœ… Dual    â”‚ Split across GPUs      â”‚\nâ”‚ 13-14B      â”‚ IQ3_XS    â”‚ 5.5       â”‚ âœ… Single  â”‚ Quality trade-off      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 30-34B      â”‚ Q4_K_M    â”‚ 19        â”‚ âœ… Dual    â”‚ tensor-split 0.5,0.5   â”‚\nâ”‚ 30-34B      â”‚ IQ3_XS    â”‚ 12        â”‚ âš ï¸ Single  â”‚ Low context            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 70B params  â”‚ IQ3_XS    â”‚ 25        â”‚ âœ… Dual    â”‚ â­ 70B on Kaggle!      â”‚\nâ”‚ 70B params  â”‚ IQ2_XS    â”‚ 19        â”‚ âœ… Dual    â”‚ Lower quality          â”‚\nâ”‚ 70B params  â”‚ Q4_K_M    â”‚ 40        â”‚ âŒ         â”‚ Too large              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nLegend:\n  âœ… Works well    âš ï¸ Possible with limits    âŒ Won't fit\n\n","output_type":"stream"}],"execution_count":27},{"id":"837c8bdb","cell_type":"markdown","source":"## ğŸ“š Summary\n\n### Key Takeaways:\n\n1. **Q4_K_M** is the recommended default - best balance of size and quality\n2. **Q5_K_M** for better quality when VRAM allows\n3. **IQ3_XS** for fitting large models (70B) on limited hardware\n4. Always check HuggingFace for pre-quantized models (faster than doing it yourself)\n\n### Kaggle T4 Guidelines:\n- Single T4 (15GB): Up to 8B Q4_K_M comfortably\n- Dual T4 (30GB): Up to 70B IQ3_XS or 34B Q4_K_M\n\n---\n\n**Next:** [05-unsloth-integration](05-unsloth-integration-llamatelemetry-v0.1.0.ipynb)","metadata":{}}]}