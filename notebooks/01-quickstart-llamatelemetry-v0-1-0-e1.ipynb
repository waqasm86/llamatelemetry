{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "b394706d",
   "cell_type": "markdown",
   "source": "## Step 1: Verify Kaggle GPU Environment\n\nValidates dual Tesla T4 GPU availability, CUDA version, and VRAM capacity to ensure the environment meets llamatelemetry v0.1.0 requirements for multi-GPU inference.",
   "metadata": {}
  },
  {
   "id": "e5e56c1a",
   "cell_type": "code",
   "source": "# Verify we have 2√ó T4 GPUs\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"üîç KAGGLE GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Check nvidia-smi\nresult = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\ngpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\nprint(f\"\\nüìä Detected GPUs: {len(gpu_lines)}\")\nfor line in gpu_lines:\n    print(f\"   {line}\")\n\n# Check CUDA version\nprint(\"\\nüìä CUDA Version:\")\n!nvcc --version | grep release\n\n# Check total VRAM\nprint(\"\\nüìä VRAM Summary:\")\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n\n# Verify we have 2 GPUs\nif len(gpu_lines) >= 2:\n    print(\"\\n‚úÖ Multi-GPU environment confirmed! Ready for llamatelemetry v0.1.0.\")\nelse:\n    print(\"\\n‚ö†Ô∏è WARNING: Less than 2 GPUs detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:11:38.452447Z",
     "iopub.execute_input": "2026-02-03T21:11:38.452881Z",
     "iopub.status.idle": "2026-02-03T21:11:38.860631Z",
     "shell.execute_reply.started": "2026-02-03T21:11:38.452842Z",
     "shell.execute_reply": "2026-02-03T21:11:38.859705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "======================================================================\nüîç KAGGLE GPU ENVIRONMENT CHECK\n======================================================================\n\nüìä Detected GPUs: 2\n   GPU 0: Tesla T4 (UUID: GPU-7ba2c248-3b76-b125-7f27-7ac05c7faf42)\n   GPU 1: Tesla T4 (UUID: GPU-27208f88-2843-f816-0b43-cf8b64926aac)\n\nüìä CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\nüìä VRAM Summary:\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\n\n‚úÖ Multi-GPU environment confirmed! Ready for llamatelemetry v0.1.0.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "id": "b734856a",
   "cell_type": "markdown",
   "source": "## Step 2: Install llamatelemetry v0.1.0 and Dependencies\n\nInstalls llamatelemetry from GitHub with pre-built CUDA binaries optimized for Kaggle's dual T4 GPUs, including FlashAttention support and automatic binary verification.",
   "metadata": {}
  },
  {
   "id": "8f5d3990",
   "cell_type": "code",
   "source": "%%time\n# Install llamatelemetry v0.1.0 from GitHub (force fresh install, no cache)\nprint(\"üì¶ Installing llamatelemetry v0.1.0...\")\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Verify installation\nimport llamatelemetry\nprint(f\"\\n‚úÖ llamatelemetry {llamatelemetry.__version__} installed!\")\n\n# Check llamatelemetry status using available APIs\nfrom llamatelemetry import check_cuda_available, get_cuda_device_info\nfrom llamatelemetry.api.multigpu import gpu_count\n\ncuda_info = get_cuda_device_info()\nprint(f\"\\nüìä llamatelemetry Status:\")\nprint(f\"   CUDA Available: {check_cuda_available()}\")\nprint(f\"   GPUs: {gpu_count()}\")\nif cuda_info:\n    print(f\"   CUDA Version: {cuda_info.get('cuda_version', 'N/A')}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:11:43.103847Z",
     "iopub.execute_input": "2026-02-03T21:11:43.104207Z",
     "iopub.status.idle": "2026-02-03T21:13:43.135581Z",
     "shell.execute_reply.started": "2026-02-03T21:11:43.104170Z",
     "shell.execute_reply": "2026-02-03T21:13:43.134515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üì¶ Installing llamatelemetry v0.1.0...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m244.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m265.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m330.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m309.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m300.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m316.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m346.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m351.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m324.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m293.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m311.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m266.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m269.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m356.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m231.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m310.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m207.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m274.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m294.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n======================================================================\nüéØ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(‚Ä¶):   0%|          | 0.00/1.40G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1cded72fc7043af93f5d06b28874ceb"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\n‚úÖ Binaries installed successfully!\n\n\n‚úÖ llamatelemetry 0.1.0 installed!\n\nüìä llamatelemetry Status:\n   CUDA Available: True\n   GPUs: 2\n   CUDA Version: 12.5\nCPU times: user 52 s, sys: 11.9 s, total: 1min 3s\nWall time: 2min\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "ae3bcc86-e9b8-42ac-ab0f-7acd9d4e738e",
   "cell_type": "code",
   "source": "!pip install -q huggingface-hub",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:13:43.138015Z",
     "iopub.execute_input": "2026-02-03T21:13:43.138643Z",
     "iopub.status.idle": "2026-02-03T21:13:46.800146Z",
     "shell.execute_reply.started": "2026-02-03T21:13:43.138593Z",
     "shell.execute_reply": "2026-02-03T21:13:46.799069Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "id": "e6d6ba18-be7c-438f-854c-a88119558c42",
   "cell_type": "code",
   "source": "#Add Secrets\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\n# Get token and set as environment variable\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ['HF_TOKEN'] = hf_token\n\n# Login using environment variable\nlogin()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:09.655676Z",
     "iopub.execute_input": "2026-02-03T21:14:09.656437Z",
     "iopub.status.idle": "2026-02-03T21:14:09.750195Z",
     "shell.execute_reply.started": "2026-02-03T21:14:09.656386Z",
     "shell.execute_reply": "2026-02-03T21:14:09.749606Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "id": "707f8b6d-e3ed-4bf1-aec3-92453475c524",
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "7892d082",
   "cell_type": "markdown",
   "source": "## Step 3: Download GGUF Model\n\nDownloads Gemma 3-4B Instruct in Q4_K_M quantization from HuggingFace Hub, optimized for Kaggle T4 GPUs with balanced quality and memory efficiency (~2.5GB).",
   "metadata": {}
  },
  {
   "id": "fd35a857",
   "cell_type": "code",
   "source": "%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# Model selection - optimized for 15GB VRAM\nMODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\nMODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n\nprint(f\"üì• Downloading {MODEL_FILE}...\")\nprint(f\"   Repository: {MODEL_REPO}\")\nprint(f\"   Expected size: ~2.5GB\")\n\n# Download to Kaggle working directory\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nprint(f\"\\n‚úÖ Model downloaded: {model_path}\")\n\n# Show model size\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"   Size: {size_gb:.2f} GB\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:13.420750Z",
     "iopub.execute_input": "2026-02-03T21:14:13.421108Z",
     "iopub.status.idle": "2026-02-03T21:14:18.861217Z",
     "shell.execute_reply.started": "2026-02-03T21:14:13.421080Z",
     "shell.execute_reply": "2026-02-03T21:14:18.860578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üì• Downloading gemma-3-4b-it-Q4_K_M.gguf...\n   Repository: unsloth/gemma-3-4b-it-GGUF\n   Expected size: ~2.5GB\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "gemma-3-4b-it-Q4_K_M.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcbe8b6d44924fbf8ad346b5832c564f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\n‚úÖ Model downloaded: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n   Size: 2.32 GB\nCPU times: user 5.63 s, sys: 8.75 s, total: 14.4 s\nWall time: 5.44 s\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "id": "c8710abc",
   "cell_type": "markdown",
   "source": "## Step 4: Start llama-server with Multi-GPU Configuration\n\nLaunches llama-server using dual-GPU tensor-split configuration (50/50) with FlashAttention enabled, maximizing throughput and model capacity across both T4 GPUs.",
   "metadata": {}
  },
  {
   "id": "d5bbe24c",
   "cell_type": "code",
   "source": "from llamatelemetry.server import ServerManager\nfrom llamatelemetry.api.multigpu import kaggle_t4_dual_config\n\n# Get optimized configuration for Kaggle T4√ó2\nconfig = kaggle_t4_dual_config()\n\nprint(\"üöÄ Starting llama-server with Multi-GPU configuration...\")\nprint(f\"   Model: {model_path}\")\nprint(f\"   GPU Layers: {config.n_gpu_layers} (all layers)\")\nprint(f\"   Context Size: {config.ctx_size}\")\nprint(f\"   Tensor Split: {config.tensor_split} (equal across 2 GPUs)\")\nprint(f\"   Flash Attention: {config.flash_attention}\")\n\n# Create server manager\nserver = ServerManager(server_url=\"http://127.0.0.1:8080\")\n\n# Start server with multi-GPU configuration\n# Pass tensor_split as comma-separated string for --tensor-split flag\ntensor_split_str = \",\".join(str(x) for x in config.tensor_split) if config.tensor_split else None\n\ntry:\n    server.start_server(\n        model_path=model_path,\n        host=\"127.0.0.1\",\n        port=8080,\n        gpu_layers=config.n_gpu_layers,\n        ctx_size=config.ctx_size,\n        timeout=120,\n        verbose=True,\n        # Multi-GPU parameters (passed via **kwargs)\n        flash_attn=1 if config.flash_attention else 0,\n        split_mode=\"layer\",\n        tensor_split=tensor_split_str,\n    )\n    print(\"\\n‚úÖ llama-server is ready with dual T4 GPUs!\")\n    print(f\"   API endpoint: http://127.0.0.1:8080\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Server failed to start: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:31.525108Z",
     "iopub.execute_input": "2026-02-03T21:14:31.526054Z",
     "iopub.status.idle": "2026-02-03T21:14:36.670729Z",
     "shell.execute_reply.started": "2026-02-03T21:14:31.526021Z",
     "shell.execute_reply": "2026-02-03T21:14:36.670009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üöÄ Starting llama-server with Multi-GPU configuration...\n   Model: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n   GPU Layers: -1 (all layers)\n   Context Size: 8192\n   Tensor Split: [0.5, 0.5] (equal across 2 GPUs)\n   Flash Attention: True\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: -1\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready........ ‚úì Ready in 5.1s\n\n‚úÖ llama-server is ready with dual T4 GPUs!\n   API endpoint: http://127.0.0.1:8080\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "id": "3ad24c79",
   "cell_type": "markdown",
   "source": "## Step 5: Test Chat Completion\n\nDemonstrates OpenAI-compatible chat completion API using llamatelemetry client, testing model inference with token usage tracking and response quality validation.",
   "metadata": {}
  },
  {
   "id": "249c0176",
   "cell_type": "code",
   "source": "from llamatelemetry.api.client import LlamaCppClient\n\n# Create client\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n\n# Test simple completion using OpenAI-compatible API\nprint(\"üí¨ Testing inference...\\n\")\n\nresponse = client.chat.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is CUDA? Explain in 2 sentences.\"}\n    ],\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(\"üìù Response:\")\nprint(response.choices[0].message.content)\n\nprint(f\"\\nüìä Stats:\")\nprint(f\"   Tokens generated: {response.usage.completion_tokens}\")\nprint(f\"   Total tokens: {response.usage.total_tokens}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:39.331571Z",
     "iopub.execute_input": "2026-02-03T21:14:39.332164Z",
     "iopub.status.idle": "2026-02-03T21:14:41.279064Z",
     "shell.execute_reply.started": "2026-02-03T21:14:39.332134Z",
     "shell.execute_reply": "2026-02-03T21:14:41.278422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üí¨ Testing inference...\n\nüìù Response:\nCUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA that allows developers to utilize the massive processing power of their GPUs for general-purpose computations ‚Äì essentially turning graphics cards into powerful accelerators. It enables you to write code that runs simultaneously on many GPU cores, dramatically speeding up tasks like machine learning, scientific simulations, and image/video processing.\n\nüìä Stats:\n   Tokens generated: 76\n   Total tokens: 95\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "id": "cab76caa",
   "cell_type": "markdown",
   "source": "## Step 6: Test Streaming Responses\n\nDemonstrates streaming completion API for real-time token generation, enabling progressive display of responses with chunk-by-chunk delivery.",
   "metadata": {}
  },
  {
   "id": "8f6c7c3e",
   "cell_type": "code",
   "source": "# Streaming example using OpenAI-compatible API\nprint(\"üí¨ Streaming response...\\n\")\n\nfor chunk in client.chat.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to calculate factorial.\"}\n    ],\n    max_tokens=200,\n    temperature=0.3,\n    stream=True  # Enable streaming\n):\n    if hasattr(chunk, 'choices') and chunk.choices:\n        delta = chunk.choices[0].delta\n        if hasattr(delta, 'content') and delta.content:\n            print(delta.content, end=\"\", flush=True)\n\nprint(\"\\n\\n‚úÖ Streaming complete!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:43.643496Z",
     "iopub.execute_input": "2026-02-03T21:14:43.644094Z",
     "iopub.status.idle": "2026-02-03T21:14:48.312650Z",
     "shell.execute_reply.started": "2026-02-03T21:14:43.644065Z",
     "shell.execute_reply": "2026-02-03T21:14:48.312038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üí¨ Streaming response...\n\n\n\n‚úÖ Streaming complete!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "7eb40cde",
   "cell_type": "markdown",
   "source": "## Step 7: Monitor GPU Memory Usage\n\nChecks VRAM consumption across both GPUs using nvidia-smi to verify model loading, track memory allocation, and confirm GPU 1 remains available for RAPIDS/Graphistry workloads.",
   "metadata": {}
  },
  {
   "id": "7f3b2939",
   "cell_type": "code",
   "source": "# Check GPU memory usage\nprint(\"üìä GPU Memory Usage:\")\nprint(\"=\"*60)\n!nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv\n\nprint(\"\\nüí° Note:\")\nprint(\"   GPU 0: llama-server (LLM inference)\")\nprint(\"   GPU 1: Available for RAPIDS/Graphistry\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:51.096777Z",
     "iopub.execute_input": "2026-02-03T21:14:51.097396Z",
     "iopub.status.idle": "2026-02-03T21:14:51.267189Z",
     "shell.execute_reply.started": "2026-02-03T21:14:51.097368Z",
     "shell.execute_reply": "2026-02-03T21:14:51.266420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üìä GPU Memory Usage:\n============================================================\nindex, name, memory.used [MiB], memory.total [MiB], utilization.gpu [%]\n0, Tesla T4, 1307 MiB, 15360 MiB, 0 %\n1, Tesla T4, 1797 MiB, 15360 MiB, 0 %\n\nüí° Note:\n   GPU 0: llama-server (LLM inference)\n   GPU 1: Available for RAPIDS/Graphistry\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "id": "dd0b4582",
   "cell_type": "markdown",
   "source": "## Step 8: Cleanup and Release Resources\n\nStops llama-server gracefully, releases GPU memory, and verifies complete resource cleanup with nvidia-smi to ensure clean state for subsequent operations.",
   "metadata": {}
  },
  {
   "id": "5c3cd97d",
   "cell_type": "code",
   "source": "# Stop the server\nprint(\"üõë Stopping llama-server...\")\nserver.stop_server()\nprint(\"\\n‚úÖ Server stopped. Resources freed.\")\n\n# Verify GPU memory is released\nprint(\"\\nüìä GPU Memory After Cleanup:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-03T21:14:54.749888Z",
     "iopub.execute_input": "2026-02-03T21:14:54.750586Z",
     "iopub.status.idle": "2026-02-03T21:14:55.783934Z",
     "shell.execute_reply.started": "2026-02-03T21:14:54.750550Z",
     "shell.execute_reply": "2026-02-03T21:14:55.783222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üõë Stopping llama-server...\n\n‚úÖ Server stopped. Resources freed.\n\nüìä GPU Memory After Cleanup:\nindex, memory.used [MiB], memory.total [MiB]\n0, 0 MiB, 15360 MiB\n1, 0 MiB, 15360 MiB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "id": "2ad536df",
   "cell_type": "markdown",
   "source": "## üéâ Quick Start Complete!\n\nYou've successfully:\n1. ‚úÖ Verified Kaggle GPU environment\n2. ‚úÖ Installed llamatelemetry v0.1.0\n3. ‚úÖ Downloaded a GGUF model\n4. ‚úÖ Started llama-server\n5. ‚úÖ Ran inference with chat completion\n6. ‚úÖ Used streaming responses\n\n## Next Steps\n\nExplore more tutorials:\n- üìò [02-llama-server-setup](02-llama-server-setup-llamatelemetry-v0.1.0.ipynb) - Advanced server configuration\n- üìò [03-multi-gpu-inference](03-multi-gpu-inference-llamatelemetry-v0.1.0.ipynb) - Dual T4 inference\n- üìò [04-gguf-quantization](04-gguf-quantization-llamatelemetry-v0.1.0.ipynb) - Quantization guide\n- üìò [05-unsloth-integration](05-unsloth-integration-llamatelemetry-v0.1.0.ipynb) - Unsloth training ‚Üí llamatelemetry\n- üìò [06-split-gpu-graphistry](06-split-gpu-graphistry-llamatelemetry-v0.1.0.ipynb) - LLM + Graph visualization\n\n---\n\n**llamatelemetry v0.1.0** | CUDA 12 Inference Backend for Unsloth",
   "metadata": {}
  },
  {
   "id": "9d96eb66-9671-4a2a-8e92-54775fd82365",
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}