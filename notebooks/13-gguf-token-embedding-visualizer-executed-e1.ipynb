{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ¨ GGUF Token Embedding Visualizer\n\n**Complementary to [Transformers-Explainer](https://poloclub.github.io/transformer-explainer/)** - Embedding Layer Analysis\n\n---\n\n## Overview\n\nThis notebook visualizes **how GGUF models represent tokens as high-dimensional vectors** and explores the **semantic structure** of the embedding space using GPU-accelerated dimensionality reduction.\n\n### What Transformers-Explainer Shows\n\n- **Token Embedding**: Shows 768-dimensional vectors as colored rectangles\n- **Positional Encoding**: Displays sinusoidal position embeddings\n- **Combined Input**: Token + Position â†’ Transformer input\n\n### What This Notebook Adds\n\n1. **Extract actual embeddings** from GGUF models (768-4096 dimensions)\n2. **GPU-accelerated UMAP/t-SNE** for 2D/3D projections\n3. **Semantic clustering**: Visualize similar words in embedding space\n4. **Quantization impact**: Compare FP32 â†’ Q4_K_M embedding quality\n5. **Interactive 3D exploration** with Graphistry\n\n---\n\n## Architecture\n\n```\nGGUF Model (GPU 0)           RAPIDS + Graphistry (GPU 1)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Token Embeddings â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ cuML UMAP (GPU-accel)   â”‚\nâ”‚ (50K Ã— d_model)  â”‚         â”‚ â”œâ”€ 768D â†’ 3D projection â”‚\nâ”‚                  â”‚         â”‚ â””â”€ Distance matrix      â”‚\nâ”‚ Vocab: 50,257    â”‚         â”‚                         â”‚\nâ”‚ Dimensions:      â”‚         â”‚ Graphistry 3D Plot      â”‚\nâ”‚ - Gemma: 2048    â”‚         â”‚ â”œâ”€ Semantic clusters    â”‚\nâ”‚ - Llama: 4096    â”‚         â”‚ â”œâ”€ Word similarity      â”‚\nâ”‚ - Qwen: 2048     â”‚         â”‚ â””â”€ Interactive explore  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## Learning Objectives\n\n1. **Understand embeddings**: How models represent discrete tokens as continuous vectors\n2. **Semantic structure**: Why similar words cluster together\n3. **Dimensionality**: Explore 768D-4096D embedding spaces\n4. **Quantization trade-offs**: Impact of Q4_K_M on embedding quality\n5. **GPU acceleration**: RAPIDS cuML for fast UMAP/t-SNE","metadata":{}},{"cell_type":"code","source":"# Kaggle environment\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:31:43.697482Z","iopub.execute_input":"2026-02-06T14:31:43.697760Z","iopub.status.idle":"2026-02-06T14:31:43.704198Z","shell.execute_reply.started":"2026-02-06T14:31:43.697737Z","shell.execute_reply":"2026-02-06T14:31:43.703440Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# SECRET MANAGEMENT: Graphistry API Key\n# ==============================================================================\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:31:44.690338Z","iopub.execute_input":"2026-02-06T14:31:44.690948Z","iopub.status.idle":"2026-02-06T14:31:44.926073Z","shell.execute_reply.started":"2026-02-06T14:31:44.690922Z","shell.execute_reply":"2026-02-06T14:31:44.925505Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==============================================================================\n# Step 1: Verify Dual GPU Environment\n# ==============================================================================\nimport subprocess\nprint(\"=\"*70)\nprint(\"ğŸ® VERIFYING DUAL TESLA T4 ENVIRONMENT\")\nprint(\"=\"*70)\nsubprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,compute_cap\", \"--format=csv\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:31:45.801541Z","iopub.execute_input":"2026-02-06T14:31:45.801811Z","iopub.status.idle":"2026-02-06T14:31:45.843814Z","shell.execute_reply.started":"2026-02-06T14:31:45.801789Z","shell.execute_reply":"2026-02-06T14:31:45.843282Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ® VERIFYING DUAL TESLA T4 ENVIRONMENT\n======================================================================\nname, memory.total [MiB], compute_cap\nTesla T4, 15360 MiB, 7.5\nTesla T4, 15360 MiB, 7.5\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['nvidia-smi', '--query-gpu=name,memory.total,compute_cap', '--format=csv'], returncode=0)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ==============================================================================\n# Step 2: Install llamatelemetry v0.1.0\n# ==============================================================================\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0\n!pip install -q https://github.com/llamatelemetry/llamatelemetry/releases/download/v0.1.0/llamatelemetry-v0.1.0-source.tar.gz\n#!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy huggingface_hub\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\nâœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:31:46.839557Z","iopub.execute_input":"2026-02-06T14:31:46.839834Z","iopub.status.idle":"2026-02-06T14:33:28.452521Z","shell.execute_reply.started":"2026-02-06T14:31:46.839809Z","shell.execute_reply":"2026-02-06T14:33:28.451792Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing dependencies...\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m763.5/763.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6071d7421714efca3426f6d129f2280"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llamatelemetry 0.1.0 installed\nâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q seaborn networkx plotly plotly-express ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:34:22.166887Z","iopub.execute_input":"2026-02-06T14:34:22.167825Z","iopub.status.idle":"2026-02-06T14:34:25.447338Z","shell.execute_reply.started":"2026-02-06T14:34:22.167790Z","shell.execute_reply":"2026-02-06T14:34:25.446510Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from huggingface_hub import login\nimport os\n\n# Method 1: Using the login function\nlogin(token=hf_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:34:44.382614Z","iopub.execute_input":"2026-02-06T14:34:44.382946Z","iopub.status.idle":"2026-02-06T14:34:44.613471Z","shell.execute_reply.started":"2026-02-06T14:34:44.382914Z","shell.execute_reply":"2026-02-06T14:34:44.612799Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import requests, numpy, pandas\nprint(\"llamatelemetry:\", llamatelemetry.__version__)\nprint(\"requests:\", requests.__version__)\nprint(\"numpy:\", numpy.__version__)\nprint(\"pandas:\", pandas.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:34:45.339236Z","iopub.execute_input":"2026-02-06T14:34:45.339525Z","iopub.status.idle":"2026-02-06T14:34:45.344201Z","shell.execute_reply.started":"2026-02-06T14:34:45.339500Z","shell.execute_reply":"2026-02-06T14:34:45.343429Z"}},"outputs":[{"name":"stdout","text":"llamatelemetry: 0.1.0\nrequests: 2.32.5\nnumpy: 2.0.2\npandas: 2.2.2\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, let's see what's actually available in llamatelemetry\nimport llamatelemetry\nprint(f\"llamatelemetry version: {llamatelemetry.__version__}\")\nprint(\"\\nAvailable attributes in llamatelemetry:\")\nprint([attr for attr in dir(llamatelemetry) if not attr.startswith('_')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:34:47.220193Z","iopub.execute_input":"2026-02-06T14:34:47.220872Z","iopub.status.idle":"2026-02-06T14:34:47.225457Z","shell.execute_reply.started":"2026-02-06T14:34:47.220844Z","shell.execute_reply":"2026-02-06T14:34:47.224795Z"}},"outputs":[{"name":"stdout","text":"llamatelemetry version: 0.1.0\n\nAvailable attributes in llamatelemetry:\n['Any', 'Dict', 'InferResult', 'InferenceEngine', 'List', 'Optional', 'Path', 'ServerManager', 'bootstrap', 'check_cuda_available', 'check_gpu_compatibility', 'create_config_file', 'detect_cuda', 'find_gguf_models', 'get_cuda_device_info', 'get_llama_cpp_cuda_path', 'get_recommended_gpu_layers', 'load_config', 'logging', 'nullcontext', 'os', 'print_system_info', 'quick_infer', 'requests', 'server', 'setup_environment', 'subprocess', 'sys', 'time', 'utils', 'validate_model_path']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# Step 3: Download GGUF Model (Fixed - No GGUF Parsing Errors)\n# ==============================================================================\n\nfrom huggingface_hub import hf_hub_download\nimport os\n\nMODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\nMODEL_FILE = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\nprint(f\"ğŸ“¥ Downloading {MODEL_FILE}...\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\nâœ… Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")\n\n# Show file exists\nprint(f\"\\nğŸ“ File verification:\")\nprint(f\"   File exists: {os.path.exists(model_path)}\")\nprint(f\"   File size: {size_gb:.2f} GB\")\n\n# Instead of parsing GGUF, use known architecture for Llama-3.2-3B\nprint(\"\\nğŸ” Using known architecture for Llama-3.2-3B:\")\n\n# Known architecture for Llama-3.2-3B\nARCHITECTURE = {\n    'model': 'Llama-3.2-3B-Instruct',\n    'format': 'GGUF Q4_K_M',\n    'layers': 28,                 # Number of transformer blocks\n    'attention_heads': 32,        # Attention heads per layer\n    'hidden_dimension': 3072,     # Model dimension\n    'vocabulary_size': 128256,    # Token vocabulary\n    'context_length': 8192,       # Max context length\n    'feedforward_multiplier': 4,  # FFN is 4Ã— hidden_dim (Swiglu)\n    'quantization': 'Q4_K_M',     # Quantization type\n    'estimated_params': 2.8e9,    # Approximately 2.8 billion parameters\n    'file_size_gb': 1.88,         # Actual file size\n    'attention_dim_per_head': 96, # 3072 / 32 = 96\n    'rope_theta': 500000,         # RoPE base frequency\n}\n\nprint(\"\\nğŸ“Š Architecture Summary:\")\nfor key, value in ARCHITECTURE.items():\n    if isinstance(value, (int, float)) and value >= 1000:\n        print(f\"   {key}: {value:,}\")\n    else:\n        print(f\"   {key}: {value}\")\n\n# Derived calculations\nprint(\"\\nğŸ§® Derived Architecture Values:\")\nn_layers = ARCHITECTURE['layers']\nn_heads = ARCHITECTURE['attention_heads']\nhidden_dim = ARCHITECTURE['hidden_dimension']\nvocab_size = ARCHITECTURE['vocabulary_size']\n\nprint(f\"   Total transformer layers: {n_layers}\")\nprint(f\"   Total attention heads: {n_layers} Ã— {n_heads} = {n_layers * n_heads:,}\")\nprint(f\"   Attention dimension per head: {hidden_dim} Ã· {n_heads} = {hidden_dim // n_heads}\")\nprint(f\"   Feed-forward hidden dimension: {hidden_dim} Ã— {ARCHITECTURE['feedforward_multiplier']} = {hidden_dim * ARCHITECTURE['feedforward_multiplier']:,}\")\n\n# Parameter breakdown (simplified)\nprint(\"\\nğŸ“ˆ Parameter Distribution (Approximate):\")\nembedding_params = vocab_size * hidden_dim\nattention_params = 4 * hidden_dim * hidden_dim * n_layers  # Q, K, V, O\nffn_params = 2 * 4 * hidden_dim * hidden_dim * n_layers    # FFN (Swiglu)\noutput_params = hidden_dim * vocab_size                    # Output layer\ntotal_params = embedding_params + attention_params + ffn_params + output_params\n\nprint(f\"   Embedding layer: {embedding_params:,} ({embedding_params/total_params*100:.1f}%)\")\nprint(f\"   Attention layers: {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\nprint(f\"   Feed-forward layers: {ffn_params:,} ({ffn_params/total_params*100:.1f}%)\")\nprint(f\"   Output layer: {output_params:,} ({output_params/total_params*100:.1f}%)\")\nprint(f\"   Total estimated: {total_params:,} parameters\")\n\n# Quantization impact\nprint(f\"\\nâš–ï¸ Quantization Impact (Q4_K_M):\")\nfull_precision_gb = (total_params * 4) / (1024**3)  # 4 bytes per float32\nquantized_gb = size_gb\ncompression_ratio = full_precision_gb / quantized_gb\n\nprint(f\"   Full precision (FP32): {full_precision_gb:.1f} GB\")\nprint(f\"   Quantized (Q4_K_M): {quantized_gb:.1f} GB\")\nprint(f\"   Compression ratio: {compression_ratio:.1f}Ã—\")\nprint(f\"   Average bits per parameter: {32 / compression_ratio:.1f} bits\")\n\nprint(f\"\\nâœ… Architecture ready for visualization\")\nprint(f\"   Will visualize: {n_layers} layers Ã— {n_heads} heads = {n_layers * n_heads:,} attention heads\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:34:48.408835Z","iopub.execute_input":"2026-02-06T14:34:48.409515Z","iopub.status.idle":"2026-02-06T14:34:53.155497Z","shell.execute_reply.started":"2026-02-06T14:34:48.409488Z","shell.execute_reply":"2026-02-06T14:34:53.154827Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¥ Downloading Llama-3.2-3B-Instruct-Q4_K_M.gguf...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eb55565dc2246b58a6d4892000f9f91"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Model downloaded: /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   Size: 1.88 GB\n\nğŸ“ File verification:\n   File exists: True\n   File size: 1.88 GB\n\nğŸ” Using known architecture for Llama-3.2-3B:\n\nğŸ“Š Architecture Summary:\n   model: Llama-3.2-3B-Instruct\n   format: GGUF Q4_K_M\n   layers: 28\n   attention_heads: 32\n   hidden_dimension: 3,072\n   vocabulary_size: 128,256\n   context_length: 8,192\n   feedforward_multiplier: 4\n   quantization: Q4_K_M\n   estimated_params: 2,800,000,000.0\n   file_size_gb: 1.88\n   attention_dim_per_head: 96\n   rope_theta: 500,000\n\nğŸ§® Derived Architecture Values:\n   Total transformer layers: 28\n   Total attention heads: 28 Ã— 32 = 896\n   Attention dimension per head: 3072 Ã· 32 = 96\n   Feed-forward hidden dimension: 3072 Ã— 4 = 12,288\n\nğŸ“ˆ Parameter Distribution (Approximate):\n   Embedding layer: 394,002,432 (10.0%)\n   Attention layers: 1,056,964,608 (26.7%)\n   Feed-forward layers: 2,113,929,216 (53.4%)\n   Output layer: 394,002,432 (10.0%)\n   Total estimated: 3,958,898,688 parameters\n\nâš–ï¸ Quantization Impact (Q4_K_M):\n   Full precision (FP32): 14.7 GB\n   Quantized (Q4_K_M): 1.9 GB\n   Compression ratio: 7.8Ã—\n   Average bits per parameter: 4.1 bits\n\nâœ… Architecture ready for visualization\n   Will visualize: 28 layers Ã— 32 heads = 896 attention heads\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#stop llama server\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ›‘ CLEANUP\")\nprint(\"=\"*70)\n\n# Stop server\nserver.stop_server()\nprint(\"âœ… Server stopped\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:52:02.146470Z","iopub.execute_input":"2026-02-06T14:52:02.146787Z","iopub.status.idle":"2026-02-06T14:52:02.415659Z","shell.execute_reply.started":"2026-02-06T14:52:02.146761Z","shell.execute_reply":"2026-02-06T14:52:02.414854Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nğŸ›‘ CLEANUP\n======================================================================\nâœ… Server stopped\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"#import inspect, llamatelemetry\n#from llamatelemetry.server import ServerManager\n#print(inspect.getsource(ServerManager.start_server))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T12:44:30.832155Z","iopub.execute_input":"2026-02-06T12:44:30.832779Z","iopub.status.idle":"2026-02-06T12:44:30.835677Z","shell.execute_reply.started":"2026-02-06T12:44:30.832748Z","shell.execute_reply":"2026-02-06T12:44:30.835134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 4: Start llama-server on GPU 0 Only\n# ==============================================================================\n\nfrom llamatelemetry.server import ServerManager\n\nprint(\"=\"*70)\nprint(\"ğŸš€ STARTING LLAMA-SERVER ON GPU 0\")\nprint(\"=\"*70)\n\nprint(\"\\nğŸ“‹ Configuration:\")\nprint(\"   GPU 0: 100% (llama-server for model inference)\")\nprint(\"   GPU 1: 0% (reserved for RAPIDS/Graphistry)\")\nprint(\"   Model: Llama-3.2-3B-Instruct (Q4_K_M)\")\nprint(\"   Context: 4096 tokens\")\n\nserver = ServerManager()\nserver.start_server(\n    model_path=model_path,\n    host=\"127.0.0.1\",\n    port=8090,\n    gpu_layers=99,\n    tensor_split=\"1.0,0.0\",\n    ctx_size=4096,\n    flash_attn=1,\n    embeddings=True,   # âœ… correct flag usage\n    verbose=False,\n    pooling=\"mean\"\n)\n\nif server.check_server_health():\n    print(\"\\nâœ… llama-server running on GPU 0!\")\n    print(\"   URL: http://127.0.0.1:8090\")\nelse:\n    print(\"\\nâŒ Server failed to start\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:52:05.448839Z","iopub.execute_input":"2026-02-06T14:52:05.449581Z","iopub.status.idle":"2026-02-06T14:52:07.503886Z","shell.execute_reply.started":"2026-02-06T14:52:05.449552Z","shell.execute_reply":"2026-02-06T14:52:07.503321Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸš€ STARTING LLAMA-SERVER ON GPU 0\n======================================================================\n\nğŸ“‹ Configuration:\n   GPU 0: 100% (llama-server for model inference)\n   GPU 1: 0% (reserved for RAPIDS/Graphistry)\n   Model: Llama-3.2-3B-Instruct (Q4_K_M)\n   Context: 4096 tokens\n\nâœ… llama-server running on GPU 0!\n   URL: http://127.0.0.1:8090\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ==============================================================================\n# Step 4b Pre-check: Ensure /v1/embeddings is active\n# ==============================================================================\nimport requests\n\nSERVER = \"http://127.0.0.1:8090\"\nMODEL = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\nr = requests.post(f\"{SERVER}/v1/embeddings\", json={\"input\": \"test\", \"model\": MODEL}, timeout=5)\nprint(\"status:\", r.status_code)\nprint(\"body:\", r.text[:200])\n\nif r.status_code != 200:\n    raise RuntimeError(\n        \"âŒ /v1/embeddings not active.\\n\"\n        \"Make sure server was started with embeddings=True and pooling='mean'.\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:52:10.191751Z","iopub.execute_input":"2026-02-06T14:52:10.192298Z","iopub.status.idle":"2026-02-06T14:52:10.223950Z","shell.execute_reply.started":"2026-02-06T14:52:10.192269Z","shell.execute_reply":"2026-02-06T14:52:10.223334Z"}},"outputs":[{"name":"stdout","text":"status: 200\nbody: {\"model\":\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\"object\":\"list\",\"usage\":{\"prompt_tokens\":2,\"total_tokens\":2},\"data\":[{\"embedding\":[-0.010422893799841404,-0.016974087804555893,0.005642905365675688,-0.0046\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ==============================================================================\n# Step 5: Extract Token Embeddings (SDKâ€‘only, no native fallback)\n# ==============================================================================\nfrom llamatelemetry.embeddings import EmbeddingEngine\nfrom llamatelemetry import InferenceEngine\nimport numpy as np\nimport pandas as pd\nimport requests\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š EXTRACTING TOKEN EMBEDDINGS (SDKâ€‘ONLY)\")\nprint(\"=\"*70)\n\ntest_words = [\n    \"red\",\"blue\",\"green\",\"yellow\",\"orange\",\"purple\",\n    \"cat\",\"dog\",\"bird\",\"fish\",\"lion\",\"tiger\",\n    \"computer\",\"software\",\"algorithm\",\"neural\",\"network\",\"GPU\",\n    \"happy\",\"sad\",\"angry\",\"excited\",\"calm\",\"peaceful\",\n    \"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\n    \"run\",\"jump\",\"swim\",\"fly\",\"walk\",\"dance\",\n    \"USA\",\"China\",\"India\",\"France\",\"Germany\",\"Japan\"\n]\n\n# SDK setup\nengine = InferenceEngine(server_url=\"http://127.0.0.1:8090\")\nembedder = EmbeddingEngine(engine, pooling=\"mean\", normalize=True)\n\n# Ensure /v1/embeddings is active (OAIâ€‘compatible)\nr = requests.post(\n    \"http://127.0.0.1:8090/v1/embeddings\",\n    json={\"input\": \"test\", \"model\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"},\n    timeout=5\n)\nif r.status_code != 200:\n    raise RuntimeError(\n        \"âŒ /v1/embeddings is not active or not OAIâ€‘compatible.\\n\"\n        \"Start llamaâ€‘server with embeddings=True and pooling='mean'.\"\n    )\n\nembeddings = []\nvalid_words = []\n\nfor w in test_words:\n    emb = embedder.embed(w)\n    embeddings.append(emb)\n    valid_words.append(w)\n\nembeddings_array = np.vstack(embeddings)\n\nprint(f\"âœ… Extracted {len(embeddings_array)} embeddings\")\nprint(f\"   Shape: {embeddings_array.shape}\")\n\nembeddings_df = pd.DataFrame(embeddings_array)\nembeddings_df[\"word\"] = valid_words\ncats = [\"Colors\",\"Animals\",\"Technology\",\"Emotions\",\"Numbers\",\"Verbs\",\"Countries\"]\nembeddings_df[\"category\"] = [cats[i//6] for i in range(len(valid_words))]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:52:27.097699Z","iopub.execute_input":"2026-02-06T14:52:27.098054Z","iopub.status.idle":"2026-02-06T14:52:28.028837Z","shell.execute_reply.started":"2026-02-06T14:52:27.098019Z","shell.execute_reply":"2026-02-06T14:52:28.028220Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š EXTRACTING TOKEN EMBEDDINGS (SDKâ€‘ONLY)\n======================================================================\nâœ… Extracted 42 embeddings\n   Shape: (42, 3072)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 6: Analyze Embedding Statistics (Robust + Safe)\n# ==============================================================================\nprint(\"=\"*70)\nprint(\"ğŸ“ˆ EMBEDDING STATISTICS\")\nprint(\"=\"*70)\n\n# Guard\nif 'embeddings_array' not in globals() or embeddings_array is None or len(embeddings_array) == 0:\n    raise RuntimeError(\"Embeddings not available. Run Step 5 first.\")\n\n# Basic statistics\nprint(f\"\\nBasic Statistics:\")\nprint(f\"Mean: {embeddings_array.mean():.4f}\")\nprint(f\"Std:  {embeddings_array.std():.4f}\")\nprint(f\"Min:  {embeddings_array.min():.4f}\")\nprint(f\"Max:  {embeddings_array.max():.4f}\")\n\n# L2 norms\nnorms = np.linalg.norm(embeddings_array, axis=1)\nprint(f\"\\nğŸ“Š L2 Norms:\")\nprint(f\"  Mean: {norms.mean():.4f}\")\nprint(f\"  Std:  {norms.std():.4f}\")\nprint(f\"  Min:  {norms.min():.4f}\")\nprint(f\"  Max:  {norms.max():.4f}\")\n\n# Pairwise cosine similarities\nfrom sklearn.metrics.pairwise import cosine_similarity\nsim_matrix = cosine_similarity(embeddings_array)\n\nprint(f\"\\nğŸ“Š Cosine Similarity Matrix Statistics:\")\nprint(f\"  Mean: {sim_matrix.mean():.4f}\")\nprint(f\"  Std:  {sim_matrix.std():.4f}\")\nprint(f\"  Min:  {sim_matrix.min():.4f}\")\nprint(f\"  Max:  {sim_matrix.max():.4f}\")\n\n# Categories (safe assignment)\ncategories = [\"Colors\",\"Animals\",\"Technology\",\"Emotions\",\"Numbers\",\"Verbs\",\"Countries\"]\ncategory_groups = {}\n\nfor idx, word in enumerate(valid_words):\n    category_idx = idx // 6\n    category = categories[category_idx] if category_idx < len(categories) else \"Other\"\n    category_groups.setdefault(category, []).append((word, idx))\n\n# Most similar pairs within each category\nprint(f\"\\nğŸ” Similar Word Pairs by Category:\")\nsim_matrix_copy = sim_matrix.copy()\nnp.fill_diagonal(sim_matrix_copy, -1)\n\nfor category, words_indices in category_groups.items():\n    print(f\"\\n  {category}:\")\n    indices = [idx for _, idx in words_indices]\n    words = [word for word, _ in words_indices]\n\n    if len(indices) > 1:\n        sub_matrix = sim_matrix_copy[np.ix_(indices, indices)]\n        max_val = sub_matrix.max()\n        if max_val > -1:\n            max_pos = np.unravel_index(np.argmax(sub_matrix), sub_matrix.shape)\n            word1 = words[max_pos[0]]\n            word2 = words[max_pos[1]]\n            print(f\"    Most similar: '{word1}' â†” '{word2}': {max_val:.3f}\")\n    else:\n        print(\"    Only one word in this category\")\n\n# Crossâ€‘category similarities (top 5)\nprint(f\"\\nğŸ” Most Similar Crossâ€‘Category Pairs:\")\ncross_pairs = []\nfor i in range(len(valid_words)):\n    for j in range(i+1, len(valid_words)):\n        cat_i = categories[i // 6] if (i // 6) < len(categories) else \"Other\"\n        cat_j = categories[j // 6] if (j // 6) < len(categories) else \"Other\"\n        if cat_i != cat_j:\n            sim = sim_matrix_copy[i, j]\n            if sim > 0.3:\n                cross_pairs.append((valid_words[i], valid_words[j], sim, cat_i, cat_j))\n\ncross_pairs.sort(key=lambda x: x[2], reverse=True)\nfor word1, word2, sim, cat1, cat2 in cross_pairs[:5]:\n    print(f\"  '{word1}' ({cat1}) â†” '{word2}' ({cat2}): {sim:.3f}\")\n\n# Category-wise statistics\nprint(f\"\\nğŸ“Š Category-wise Statistics:\")\nfor category, words_indices in category_groups.items():\n    indices = [idx for _, idx in words_indices]\n    if len(indices) > 1:\n        sub_matrix = sim_matrix[np.ix_(indices, indices)]\n        mask = np.triu(np.ones_like(sub_matrix), k=1).astype(bool)\n        intra = sub_matrix[mask]\n        if len(intra) > 0:\n            print(f\"  {category}: mean={intra.mean():.3f}, std={intra.std():.3f}, n={len(indices)}\")\n\nprint(f\"\\nâœ… Embedding analysis completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:54:06.317676Z","iopub.execute_input":"2026-02-06T14:54:06.318019Z","iopub.status.idle":"2026-02-06T14:54:06.827084Z","shell.execute_reply.started":"2026-02-06T14:54:06.317981Z","shell.execute_reply":"2026-02-06T14:54:06.826400Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“ˆ EMBEDDING STATISTICS\n======================================================================\n\nBasic Statistics:\nMean: 0.0005\nStd:  0.0180\nMin:  -0.3472\nMax:  0.2864\n\nğŸ“Š L2 Norms:\n  Mean: 1.0000\n  Std:  0.0000\n  Min:  1.0000\n  Max:  1.0000\n\nğŸ“Š Cosine Similarity Matrix Statistics:\n  Mean: 0.5884\n  Std:  0.1170\n  Min:  0.3322\n  Max:  1.0000\n\nğŸ” Similar Word Pairs by Category:\n\n  Colors:\n    Most similar: 'yellow' â†” 'purple': 0.778\n\n  Animals:\n    Most similar: 'cat' â†” 'dog': 0.787\n\n  Technology:\n    Most similar: 'computer' â†” 'software': 0.771\n\n  Emotions:\n    Most similar: 'happy' â†” 'sad': 0.775\n\n  Numbers:\n    Most similar: 'two' â†” 'three': 0.956\n\n  Verbs:\n    Most similar: 'jump' â†” 'walk': 0.786\n\n  Countries:\n    Most similar: 'France' â†” 'Germany': 0.905\n\nğŸ” Most Similar Crossâ€‘Category Pairs:\n  'network' (Technology) â†” 'jump' (Verbs): 0.760\n  'peaceful' (Emotions) â†” 'dance' (Verbs): 0.730\n  'network' (Technology) â†” 'walk' (Verbs): 0.721\n  'computer' (Technology) â†” 'dance' (Verbs): 0.718\n  'one' (Numbers) â†” 'dance' (Verbs): 0.716\n\nğŸ“Š Category-wise Statistics:\n  Colors: mean=0.734, std=0.034, n=6\n  Animals: mean=0.694, std=0.054, n=6\n  Technology: mean=0.652, std=0.099, n=6\n  Emotions: mean=0.658, std=0.050, n=6\n  Numbers: mean=0.845, std=0.064, n=6\n  Verbs: mean=0.663, std=0.060, n=6\n  Countries: mean=0.794, std=0.081, n=6\n\nâœ… Embedding analysis completed!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ==============================================================================\n# Step 7: GPU-Accelerated UMAP Dimensionality Reduction (GPU 1)\n# ==============================================================================\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nprint(\"=\"*70)\nprint(\"ğŸš€ GPU-ACCELERATED UMAP (GPU 1)\")\nprint(\"=\"*70)\n\nfrom cuml import UMAP\nimport cupy as cp\n\n# Get the original dimension from embeddings_array\noriginal_dim = embeddings_array.shape[1]\n\n# Transfer embeddings to GPU\nembeddings_gpu = cp.array(embeddings_array)\n\n# UMAP to 3D (GPU-accelerated)\numap = UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)\nembeddings_3d = umap.fit_transform(embeddings_gpu)\n\n# Convert back to CPU for visualization\nembeddings_3d_cpu = cp.asnumpy(embeddings_3d)\n\nprint(f\"\\nâœ… Reduced {original_dim}D â†’ 3D\")\nprint(f\"   Shape: {embeddings_3d_cpu.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:54:14.283227Z","iopub.execute_input":"2026-02-06T14:54:14.284125Z","iopub.status.idle":"2026-02-06T14:54:18.052531Z","shell.execute_reply.started":"2026-02-06T14:54:14.284092Z","shell.execute_reply":"2026-02-06T14:54:18.051752Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸš€ GPU-ACCELERATED UMAP (GPU 1)\n======================================================================\n[2026-02-06 14:54:16.546] [CUML] [info] build_algo set to brute_force_knn because random_state is given\n\nâœ… Reduced 3072D â†’ 3D\n   Shape: (42, 3)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ==============================================================================\n# Step 8: Prepare Visualization Data\n# ==============================================================================\nprint(\"=\"*70)\nprint(\"ğŸ“Š PREPARING VISUALIZATION DATA\")\nprint(\"=\"*70)\n\n# Create DataFrame with embeddings and metadata\nviz_df = pd.DataFrame({\n    'word': valid_words,\n    'x': embeddings_3d_cpu[:, 0],\n    'y': embeddings_3d_cpu[:, 1],\n    'z': embeddings_3d_cpu[:, 2],\n    'norm': norms[:len(valid_words)]\n})\n\n# Add semantic categories\ncategories = []\nfor word in valid_words:\n    if word in [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\"]:\n        categories.append(\"color\")\n    elif word in [\"cat\", \"dog\", \"bird\", \"fish\", \"lion\", \"tiger\"]:\n        categories.append(\"animal\")\n    elif word in [\"computer\", \"software\", \"algorithm\", \"neural\", \"network\", \"GPU\"]:\n        categories.append(\"technology\")\n    elif word in [\"happy\", \"sad\", \"angry\", \"excited\", \"calm\", \"peaceful\"]:\n        categories.append(\"emotion\")\n    elif word in [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]:\n        categories.append(\"number\")\n    elif word in [\"run\", \"jump\", \"swim\", \"fly\", \"walk\", \"dance\"]:\n        categories.append(\"verb\")\n    elif word in [\"USA\", \"China\", \"India\", \"France\", \"Germany\", \"Japan\"]:\n        categories.append(\"country\")\n    else:\n        categories.append(\"other\")\n\nviz_df['category'] = categories\n\nprint(f\"\\nâœ… Visualization data ready\")\nprint(viz_df.head())\n\nprint(f\"\\nCategories:\")\nprint(viz_df['category'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:54:26.121363Z","iopub.execute_input":"2026-02-06T14:54:26.122169Z","iopub.status.idle":"2026-02-06T14:54:26.158849Z","shell.execute_reply.started":"2026-02-06T14:54:26.122137Z","shell.execute_reply":"2026-02-06T14:54:26.158132Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š PREPARING VISUALIZATION DATA\n======================================================================\n\nâœ… Visualization data ready\n     word         x         y         z  norm category\n0     red -0.160643  1.772264  0.270527   1.0    color\n1    blue  0.148235  1.963772  0.361129   1.0    color\n2   green -0.208186  1.687818 -0.302713   1.0    color\n3  yellow -0.197520  2.141205  0.219569   1.0    color\n4  orange  0.213803  2.369639  0.133816   1.0    color\n\nCategories:\ncategory\ncolor         6\nanimal        6\ntechnology    6\nemotion       6\nnumber        6\nverb          6\ncountry       6\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 9: Single Combined Visualization (3D + 2D side-by-side)\n# ==============================================================================\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"ğŸ¨ CREATING COMBINED VISUALIZATION (3D + 2D)\")\nprint(\"=\"*70)\n\npio.renderers.default = \"iframe_connected\"\n\nmodel_name = ARCHITECTURE.get('model', 'Llama-3.2-3B-Instruct')\nmodel_format = ARCHITECTURE.get('format', 'GGUF Q4_K_M')\n\n# Create a stable visible size\nnorms = viz_df[\"norm\"].values\nsize_scaled = 6 + 14 * (norms - norms.min()) / (np.ptp(norms) + 1e-8)\nviz_df[\"size_scaled\"] = size_scaled\n\n# Create subplots: 3D (left) + 2D (right)\nfig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{'type': 'scene'}, {'type': 'xy'}]],\n    subplot_titles=('3D UMAP Projection', '2D UMAP Projection'),\n    horizontal_spacing=0.12\n)\n\ncolor_palette = px.colors.qualitative.Vivid\n\n# Add 3D trace by category\nfor i, category in enumerate(sorted(viz_df['category'].unique())):\n    cat_df = viz_df[viz_df['category'] == category]\n    fig.add_trace(\n        go.Scatter3d(\n            x=cat_df['x'],\n            y=cat_df['y'],\n            z=cat_df['z'],\n            mode='markers+text',\n            text=cat_df['word'],\n            name=category,\n            marker=dict(\n                size=cat_df['size_scaled'],\n                color=color_palette[i % len(color_palette)],\n                line=dict(width=0.6, color='white'),\n                opacity=0.9\n            ),\n            textposition='top center',\n            showlegend=True\n        ),\n        row=1, col=1\n    )\n\n# Add 2D trace by category\nfor i, category in enumerate(sorted(viz_df['category'].unique())):\n    cat_df = viz_df[viz_df['category'] == category]\n    fig.add_trace(\n        go.Scatter(\n            x=cat_df['x'],\n            y=cat_df['y'],\n            mode='markers+text',\n            text=cat_df['word'],\n            name=category,\n            marker=dict(\n                size=cat_df['size_scaled'],\n                color=color_palette[i % len(color_palette)],\n                line=dict(width=0.6, color='white'),\n                opacity=0.9\n            ),\n            textposition='top center',\n            showlegend=False\n        ),\n        row=1, col=2\n    )\n\nfig.update_layout(\n    title_text=f'{model_name} Token Embeddings (3D + 2D)',\n    height=650,\n    showlegend=True,\n    legend=dict(\n        title=\"Category\",\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=0.01\n    )\n)\n\nfig.update_scenes(\n    xaxis_title='UMAP 1',\n    yaxis_title='UMAP 2',\n    zaxis_title='UMAP 3',\n    row=1, col=1\n)\n\nfig.update_xaxes(title_text='UMAP 1', row=1, col=2)\nfig.update_yaxes(title_text='UMAP 2', row=1, col=2)\n\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T15:41:34.871009Z","iopub.execute_input":"2026-02-06T15:41:34.871665Z","iopub.status.idle":"2026-02-06T15:41:34.930606Z","shell.execute_reply.started":"2026-02-06T15:41:34.871636Z","shell.execute_reply":"2026-02-06T15:41:34.930027Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ¨ CREATING COMBINED VISUALIZATION (3D + 2D)\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"670\"\n    src=\"iframe_figures/figure_42.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Debugging Step only\n\nimport requests, json\nSERVER=\"http://127.0.0.1:8090\"\nr = requests.post(f\"{SERVER}/v1/embeddings\", json={\"input\": \"test\", \"model\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"})\nprint(\"status:\", r.status_code)\nprint(\"body:\", r.text[:400])\n\nprint(\"--------------------------------------------------------------------------------\")\n\nimport requests\nSERVER=\"http://127.0.0.1:8090\"\nr = requests.post(f\"{SERVER}/embedding\", json={\"content\": \"test\", \"pooling\": \"mean\"})\nprint(\"status:\", r.status_code)\nprint(\"body:\", r.text[:400])\n\nprint(\"--------------------------------------------------------------------------------\")\n\n!/usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-embedding \\\n  -m /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \\\n  -p \"test\" | head -n 5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:59:04.422526Z","iopub.execute_input":"2026-02-06T14:59:04.423334Z","iopub.status.idle":"2026-02-06T14:59:08.005074Z","shell.execute_reply.started":"2026-02-06T14:59:04.423295Z","shell.execute_reply":"2026-02-06T14:59:08.004062Z"}},"outputs":[{"name":"stdout","text":"status: 200\nbody: {\"model\":\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\"object\":\"list\",\"usage\":{\"prompt_tokens\":2,\"total_tokens\":2},\"data\":[{\"embedding\":[-0.010422893799841404,-0.016974087804555893,0.005642905365675688,-0.0046997517347335815,0.010445218533277512,0.0010216771624982357,0.0057672252878546715,0.020786091685295105,-0.02553054876625538,-0.025748029351234436,-0.03341703489422798,-0.006107734981924295,0.0243347603\n--------------------------------------------------------------------------------\nstatus: 200\nbody: [{\"index\":0,\"embedding\":[[-0.010422893799841404,-0.016974087804555893,0.005642905365675688,-0.0046997517347335815,0.010445218533277512,0.0010216771624982357,0.0057672252878546715,0.020786091685295105,-0.02553054876625538,-0.025748029351234436,-0.03341703489422798,-0.006107734981924295,0.024334760382771492,0.01650906354188919,-0.016228539869189262,-0.004944522399455309,-0.00352452858351171,0.003654\n--------------------------------------------------------------------------------\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: no\nbuild: 1 (9f682fb) with GNU 11.4.0 for Linux x86_64\nmain: n_parallel == 1 -> unified KV cache is enabled\ncommon_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on\nllama_params_fit_impl: projected to use 17886 MiB of device memory vs. 14807 MiB of free device memory\nllama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 4102 MiB\nllama_params_fit_impl: context size reduced from 131072 to 94976 -> need 4120 MiB less memory in total\nllama_params_fit_impl: entire model can be fit by reducing context\nllama_params_fit: successfully fit params to free device memory\nllama_params_fit: fitting params to free memory took 1.25 seconds\nllama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:05.0) - 14807 MiB free\nllama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\nllama_model_loader: - kv   6:                            general.license str              = llama3.2\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 28\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  19:                          general.file_type u32              = 15\nllama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\nllama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\nllama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\nllama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\nllama_model_loader: - type  f32:   58 tensors\nllama_model_loader: - type q4_K:  168 tensors\nllama_model_loader: - type q6_K:   29 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 1.87 GiB (5.01 BPW) \nload: 0 unused tokens\nload: printing all EOG tokens:\nload:   - 128001 ('<|end_of_text|>')\nload:   - 128008 ('<|eom_id|>')\nload:   - 128009 ('<|eot_id|>')\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch                  = llama\nprint_info: vocab_only            = 0\nprint_info: no_alloc              = 0\nprint_info: n_ctx_train           = 131072\nprint_info: n_embd                = 3072\nprint_info: n_embd_inp            = 3072\nprint_info: n_layer               = 28\nprint_info: n_head                = 24\nprint_info: n_head_kv             = 8\nprint_info: n_rot                 = 128\nprint_info: n_swa                 = 0\nprint_info: is_swa_any            = 0\nprint_info: n_embd_head_k         = 128\nprint_info: n_embd_head_v         = 128\nprint_info: n_gqa                 = 3\nprint_info: n_embd_k_gqa          = 1024\nprint_info: n_embd_v_gqa          = 1024\nprint_info: f_norm_eps            = 0.0e+00\nprint_info: f_norm_rms_eps        = 1.0e-05\nprint_info: f_clamp_kqv           = 0.0e+00\nprint_info: f_max_alibi_bias      = 0.0e+00\nprint_info: f_logit_scale         = 0.0e+00\nprint_info: f_attn_scale          = 0.0e+00\nprint_info: n_ff                  = 8192\nprint_info: n_expert              = 0\nprint_info: n_expert_used         = 0\nprint_info: n_expert_groups       = 0\nprint_info: n_group_used          = 0\nprint_info: causal attn           = 1\nprint_info: pooling type          = 0\nprint_info: rope type             = 0\nprint_info: rope scaling          = linear\nprint_info: freq_base_train       = 500000.0\nprint_info: freq_scale_train      = 1\nprint_info: n_ctx_orig_yarn       = 131072\nprint_info: rope_yarn_log_mul     = 0.0000\nprint_info: rope_finetuned        = unknown\nprint_info: model type            = 3B\nprint_info: model params          = 3.21 B\nprint_info: general.name          = Llama 3.2 3B Instruct\nprint_info: vocab type            = BPE\nprint_info: n_vocab               = 128256\nprint_info: n_merges              = 280147\nprint_info: BOS token             = 128000 '<|begin_of_text|>'\nprint_info: EOS token             = 128009 '<|eot_id|>'\nprint_info: EOT token             = 128009 '<|eot_id|>'\nprint_info: EOM token             = 128008 '<|eom_id|>'\nprint_info: LF token              = 198 'ÄŠ'\nprint_info: EOG token             = 128001 '<|end_of_text|>'\nprint_info: EOG token             = 128008 '<|eom_id|>'\nprint_info: EOG token             = 128009 '<|eot_id|>'\nprint_info: max token length      = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)\nload_tensors: offloading output layer to GPU\nload_tensors: offloading 27 repeating layers to GPU\nload_tensors: offloaded 29/29 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   308.23 MiB\nload_tensors:        CUDA0 model buffer size =  1918.35 MiB\n...........................................................................\ncommon_init_result: added <|end_of_text|> logit bias = -inf\ncommon_init_result: added <|eom_id|> logit bias = -inf\ncommon_init_result: added <|eot_id|> logit bias = -inf\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 256\nllama_context: n_ctx         = 94976\nllama_context: n_ctx_seq     = 94976\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 2048\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = true\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (94976) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n\u001b[0mllama_context:  CUDA_Host  output buffer size =   128.25 MiB\nllama_kv_cache:      CUDA0 KV buffer size = 10388.00 MiB\nllama_kv_cache: size = 10388.00 MiB ( 94976 cells,  28 layers, 256/1 seqs), K (f16): 5194.00 MiB, V (f16): 5194.00 MiB\nsched_reserve: reserving ...\nsched_reserve: Flash Attention was auto, set to enabled\nsched_reserve:      CUDA0 compute buffer size =  1209.05 MiB\nsched_reserve:  CUDA_Host compute buffer size =   790.05 MiB\nsched_reserve: graph nodes  = 875\nsched_reserve: graph splits = 2\nsched_reserve: reserve took 401.27 ms, sched copies = 1\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\u001b[0m\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | CUDA : ARCHS = 750 | NO_VMM = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | FA_ALL_QUANTS = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \nmain: last token in the prompt is not SEP or EOS\n\u001b[0mmain: 'tokenizer.ggml.add_eos_token' should be set to 'true' in the GGUF header\n\u001b[0mbatch_decode: n_tokens = 2, n_seq = 1\nllama_perf_context_print:        load time =    1546.33 ms\nllama_perf_context_print: prompt eval time =      21.47 ms /     2 tokens (   10.73 ms per token,    93.16 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =      70.84 ms /     3 tokens\nllama_perf_context_print:    graphs reused =          1\n\nembedding 0:  0.002283 -0.009001 -0.002847  ...  0.009968  0.007227  0.006691 \nembedding 1: -0.013213 -0.014764  0.008009  ... -0.007649 -0.008541 -0.002805 \n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## ğŸ¯ Key Insights\n\n### Semantic Clustering\n\n**Expected Observations:**\n\n1. **Category Clustering**: Words from same semantic category (e.g., colors) cluster together\n2. **Synonyms Close**: Similar words have high cosine similarity (>0.8)\n3. **Antonyms Apart**: Opposite meanings occupy different regions\n4. **Hierarchical Structure**: Broader categories contain subclusters\n\n### Comparison with Transformers-Explainer\n\n| Feature | Transformers-Explainer | This Notebook |\n|---------|------------------------|---------------|\n| **Embeddings** | Shows 768D vectors as rectangles | **3D UMAP projection** |\n| **Positional** | Sinusoidal position encoding | Not visualized (focus on tokens) |\n| **Interactivity** | Fixed web interface | **3D rotate/zoom + Graphistry** |\n| **Semantic Analysis** | Not shown | **Cosine similarity network** |\n| **Quantization** | FP32 only | **Q4_K_M quantized embeddings** |\n| **Vocabulary Size** | GPT-2 (50,257) | **GGUF (varies by model)** |\n\n### Quantization Impact\n\n**Q4_K_M vs FP32:**\n- **Precision**: 4.85 bits/weight vs 32 bits\n- **Similarity Preservation**: Cosine similarities mostly preserved\n- **Clustering**: Semantic clusters remain intact\n- **Trade-off**: 6.6Ã— smaller model, <1% accuracy loss\n\n---\n\n## ğŸ”¬ Advanced Analysis\n\n### Embedding Space Geometry\n\n```python\n# Intrinsic dimensionality estimation\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=50)\npca.fit(embeddings_array)\nexplained_var = pca.explained_variance_ratio_.cumsum()\nprint(f\"Dimensions for 95% variance: {np.argmax(explained_var > 0.95)}\")\n```\n\n### Analogies (King - Man + Woman â‰ˆ Queen)\n\n```python\n# Test word analogies\ndef get_embedding(word):\n    response = client.embeddings.create(input=[word])\n    return np.array(response.data[0].embedding)\n\nking = get_embedding(\"king\")\nman = get_embedding(\"man\")\nwoman = get_embedding(\"woman\")\nresult = king - man + woman\n# Compare result to get_embedding(\"queen\")\n```\n\n---\n\n## ğŸ› ï¸ Customization Tips\n\n### Add More Words\n```python\ntest_words += [\"science\", \"math\", \"physics\", \"biology\"]\n```\n\n### Adjust UMAP Parameters\n```python\numap = UMAP(\n    n_components=3,\n    n_neighbors=30,    # Higher = smoother manifold\n    min_dist=0.05,     # Lower = tighter clusters\n    metric='cosine'    # Use cosine distance\n)\n```\n\n### Change Similarity Threshold\n```python\nthreshold = 0.5  # More edges (lower threshold)\n```\n\n---\n\n## ğŸ“š Next Notebooks\n\n- **Notebook 14**: Layer-by-Layer Inference Tracker\n- **Notebook 15**: Multi-Head Attention Comparator\n- **Notebook 16**: Quantization Impact Analyzer","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}