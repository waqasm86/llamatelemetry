{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a43b95",
   "metadata": {},
   "source": [
    "## Step 1: Install llamatelemetry and Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90338ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n",
    "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n",
    "\n",
    "import llamatelemetry\n",
    "print(f\"âœ… llamatelemetry {llamatelemetry.__version__} installed\")\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\nðŸ“Š GPU Info:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8008eb9",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Quantization Types\n",
    "\n",
    "GGUF supports multiple quantization types, organized into families:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.gguf import QUANT_TYPE_INFO, QuantTypeInfo\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ GGUF QUANTIZATION TYPES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group by family\n",
    "families = {\n",
    "    \"Legacy (Basic)\": [\"Q4_0\", \"Q4_1\", \"Q5_0\", \"Q5_1\", \"Q8_0\"],\n",
    "    \"K-Quants (Recommended)\": [\"Q2_K\", \"Q3_K_S\", \"Q3_K_M\", \"Q3_K_L\", \"Q4_K_S\", \"Q4_K_M\", \"Q5_K_S\", \"Q5_K_M\", \"Q6_K\"],\n",
    "    \"I-Quants (Ultra-Low)\": [\"IQ2_XXS\", \"IQ2_XS\", \"IQ2_S\", \"IQ3_XXS\", \"IQ3_XS\", \"IQ3_S\", \"IQ3_M\", \"IQ4_XS\", \"IQ4_NL\"],\n",
    "    \"Full Precision\": [\"F16\", \"F32\", \"BF16\"],\n",
    "}\n",
    "\n",
    "for family_name, types in families.items():\n",
    "    print(f\"\\nðŸ”¹ {family_name}:\")\n",
    "    print(f\"   {'Type':<12} {'Bits/Weight':<12} {'Quality':<10} {'Notes'}\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "    \n",
    "    for qtype in types:\n",
    "        if qtype in QUANT_TYPE_INFO:\n",
    "            info = QUANT_TYPE_INFO[qtype]\n",
    "            quality_stars = \"â˜…\" * int(info.quality_score * 5)\n",
    "            notes = \"Needs imatrix\" if info.requires_imatrix else \"\"\n",
    "            print(f\"   {qtype:<12} {info.bits_per_weight:<12.2f} {quality_stars:<10} {notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9033a",
   "metadata": {},
   "source": [
    "## Step 3: Quantization Size Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.gguf import estimate_gguf_size\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š GGUF SIZE CALCULATOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Common model sizes\n",
    "model_sizes = {\n",
    "    \"Gemma-3 1B\": 1,\n",
    "    \"Gemma-3 4B\": 4,\n",
    "    \"Llama-3.2 3B\": 3,\n",
    "    \"Llama-3.1 8B\": 8,\n",
    "    \"Qwen2.5 14B\": 14,\n",
    "    \"Llama-3.1 70B\": 70,\n",
    "}\n",
    "\n",
    "# Quantization types to compare\n",
    "quant_types = [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\", \"IQ3_XS\", \"F16\"]\n",
    "\n",
    "print(f\"\\n{'Model':<18} | \", end=\"\")\n",
    "for qt in quant_types:\n",
    "    print(f\"{qt:<10}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, params_b in model_sizes.items():\n",
    "    print(f\"{model_name:<18} | \", end=\"\")\n",
    "    for qt in quant_types:\n",
    "        size_gb = estimate_gguf_size(params_b, qt)\n",
    "        print(f\"{size_gb:<10.1f}\", end=\"\")\n",
    "    print(\" GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b25c2",
   "metadata": {},
   "source": [
    "## Step 4: Kaggle T4 Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.gguf import recommend_quant_for_kaggle\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ KAGGLE T4 QUANTIZATION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test various model sizes\n",
    "test_models = [\n",
    "    (\"Gemma-3 1B\", 1),\n",
    "    (\"Llama-3.2 3B\", 3),\n",
    "    (\"Gemma-3 4B\", 4),\n",
    "    (\"Llama-3.1 8B\", 8),\n",
    "    (\"Qwen2.5 14B\", 14),\n",
    "    (\"Llama-3.1 70B\", 70),\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ”¹ Single T4 (15GB VRAM):\")\n",
    "print(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Context'}\")\n",
    "print(f\"   {'-'*55}\")\n",
    "\n",
    "for model_name, params in test_models:\n",
    "    rec = recommend_quant_for_kaggle(params, dual_gpu=False)\n",
    "    if rec:\n",
    "        size = estimate_gguf_size(params, rec['quant_type'])\n",
    "        print(f\"   {model_name:<18} {rec['quant_type']:<12} {size:.1f} GB     {rec['context_size']}\")\n",
    "    else:\n",
    "        print(f\"   {model_name:<18} {'Too large':<12} -          -\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Dual T4 (30GB VRAM):\")\n",
    "print(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Context'}\")\n",
    "print(f\"   {'-'*55}\")\n",
    "\n",
    "for model_name, params in test_models:\n",
    "    rec = recommend_quant_for_kaggle(params, dual_gpu=True)\n",
    "    if rec:\n",
    "        size = estimate_gguf_size(params, rec['quant_type'])\n",
    "        print(f\"   {model_name:<18} {rec['quant_type']:<12} {size:.1f} GB     {rec['context_size']}\")\n",
    "    else:\n",
    "        print(f\"   {model_name:<18} {'Too large':<12} -          -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001e562",
   "metadata": {},
   "source": [
    "## Step 5: K-Quants Deep Dive\n",
    "\n",
    "K-Quants are the recommended choice for most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f961c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“˜ K-QUANTS DETAILED GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "k_quant_guide = \"\"\"\n",
    "K-Quants use a sophisticated mixed-precision approach:\n",
    "- Attention layers: Higher precision (more important for quality)\n",
    "- Feed-forward layers: Lower precision (less sensitive)\n",
    "\n",
    "ðŸ”¹ Naming Convention:\n",
    "   Q{bits}_K_{size}\n",
    "   â””â”€ bits: Base quantization (2,3,4,5,6)\n",
    "      â””â”€ K: K-quant family marker\n",
    "         â””â”€ size: S=Small, M=Medium, L=Large\n",
    "\n",
    "ðŸ”¹ Recommended K-Quants:\n",
    "\n",
    "   Q4_K_M (4.85 bits/weight) â­ RECOMMENDED\n",
    "   â”œâ”€â”€ Best balance of size and quality\n",
    "   â”œâ”€â”€ ~30% smaller than FP16\n",
    "   â””â”€â”€ Minimal quality loss\n",
    "\n",
    "   Q5_K_M (5.69 bits/weight) - HIGH QUALITY\n",
    "   â”œâ”€â”€ Better quality than Q4_K_M\n",
    "   â”œâ”€â”€ Good for creative writing\n",
    "   â””â”€â”€ ~20% larger than Q4_K_M\n",
    "\n",
    "   Q6_K (6.59 bits/weight) - NEAR LOSSLESS\n",
    "   â”œâ”€â”€ Almost FP16 quality\n",
    "   â”œâ”€â”€ Good for technical tasks\n",
    "   â””â”€â”€ ~35% larger than Q4_K_M\n",
    "\n",
    "   Q3_K_M (3.89 bits/weight) - MEMORY SAVER\n",
    "   â”œâ”€â”€ For larger models on limited VRAM\n",
    "   â”œâ”€â”€ Some quality degradation\n",
    "   â””â”€â”€ ~20% smaller than Q4_K_M\n",
    "\"\"\"\n",
    "print(k_quant_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9e958",
   "metadata": {},
   "source": [
    "## Step 6: I-Quants for Large Models\n",
    "\n",
    "I-Quants enable running 70B+ models on limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“˜ I-QUANTS FOR LARGE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "i_quant_guide = \"\"\"\n",
    "I-Quants (Importance-Matrix Quants) use importance matrices\n",
    "to determine which weights are most critical for quality.\n",
    "\n",
    "ðŸ”¹ Key Requirements:\n",
    "   âš ï¸  Require importance matrix (imatrix) for good quality\n",
    "   âš ï¸  Without imatrix, quality suffers significantly\n",
    "   âœ… Pre-made imatrix GGUFs are available on HuggingFace\n",
    "\n",
    "ðŸ”¹ I-Quant Types:\n",
    "\n",
    "   IQ3_XS (3.30 bits/weight) â­ BEST FOR 70B\n",
    "   â”œâ”€â”€ Fits 70B models in ~25GB VRAM\n",
    "   â”œâ”€â”€ Surprisingly good quality with imatrix\n",
    "   â””â”€â”€ Ideal for Kaggle dual T4 (30GB)\n",
    "\n",
    "   IQ4_XS (4.25 bits/weight) - HIGH QUALITY LOW SIZE\n",
    "   â”œâ”€â”€ Better than Q4_K_M in some benchmarks\n",
    "   â”œâ”€â”€ Slightly smaller file size\n",
    "   â””â”€â”€ Great for 13B-34B models\n",
    "\n",
    "   IQ2_XS (2.31 bits/weight) - EXTREME COMPRESSION\n",
    "   â”œâ”€â”€ For 70B+ when VRAM is very limited\n",
    "   â”œâ”€â”€ Noticeable quality degradation\n",
    "   â””â”€â”€ Use only when necessary\n",
    "\n",
    "ðŸ”¹ 70B Model on Kaggle Dual T4:\n",
    "   Model: Llama-3.1-70B-Instruct-GGUF\n",
    "   Quant: IQ3_XS (~25GB)\n",
    "   Config: tensor-split 0.5,0.5\n",
    "   Context: 2048 (limited by VRAM)\n",
    "\"\"\"\n",
    "print(i_quant_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3b025",
   "metadata": {},
   "source": [
    "## Step 7: Interactive Quant Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91968e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.gguf import print_quant_guide\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ INTERACTIVE QUANTIZATION GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Print the full guide\n",
    "print_quant_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936cbe2",
   "metadata": {},
   "source": [
    "## Step 8: Download and Test Different Quantizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“¥ DOWNLOAD GGUF MODELS FOR COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Available Gemma-3 1B quantizations from Unsloth\n",
    "models_to_test = {\n",
    "    \"Q4_K_M\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    \"Q5_K_M\": \"gemma-3-1b-it-Q5_K_M.gguf\",\n",
    "    \"Q8_0\": \"gemma-3-1b-it-Q8_0.gguf\",\n",
    "}\n",
    "\n",
    "REPO = \"unsloth/gemma-3-1b-it-GGUF\"\n",
    "MODEL_DIR = \"/kaggle/working/models\"\n",
    "\n",
    "print(f\"\\nðŸ“¥ Downloading from {REPO}...\\n\")\n",
    "\n",
    "downloaded = {}\n",
    "for quant, filename in models_to_test.items():\n",
    "    print(f\"   Downloading {quant}...\", end=\" \")\n",
    "    try:\n",
    "        path = hf_hub_download(\n",
    "            repo_id=REPO,\n",
    "            filename=filename,\n",
    "            local_dir=MODEL_DIR\n",
    "        )\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        downloaded[quant] = path\n",
    "        print(f\"âœ… {size_mb:.0f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Downloaded {len(downloaded)} models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a69b6",
   "metadata": {},
   "source": [
    "## Step 9: Benchmark Different Quantizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.server import ServerManager\n",
    "from llamatelemetry.api.client import LlamaCppClient\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š QUANTIZATION BENCHMARK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_prompt = \"Explain what CUDA is in exactly 3 sentences.\"\n",
    "results = {}\n",
    "\n",
    "for quant, model_path in downloaded.items():\n",
    "    print(f\"\\nðŸ”¹ Testing {quant}...\")\n",
    "    \n",
    "    # Start server\n",
    "    server = ServerManager()\n",
    "    server.start_server(\n",
    "        model_path=model_path,\n",
    "        host=\"127.0.0.1\",\n",
    "        port=8080,\n",
    "        gpu_layers=99,\n",
    "        ctx_size=2048,\n",
    "        flash_attention=True,\n",
    "    )\n",
    "    \n",
    "    if not server.check_server_health(timeout=60):\n",
    "        print(f\"   âŒ Failed to start\")\n",
    "        continue\n",
    "    \n",
    "    # Benchmark\n",
    "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "    \n",
    "    # Warm-up\n",
    "    client.chat.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    # Actual test\n",
    "    start = time.time()\n",
    "    response = client.chat.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": test_prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    tokens = response.usage.completion_tokens\n",
    "    tok_per_sec = tokens / elapsed\n",
    "    \n",
    "    results[quant] = {\n",
    "        \"tokens\": tokens,\n",
    "        \"time\": elapsed,\n",
    "        \"tok_per_sec\": tok_per_sec,\n",
    "        \"response\": response.choices[0].message.content[:100]\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tokens: {tokens}, Time: {elapsed:.2f}s, Speed: {tok_per_sec:.1f} tok/s\")\n",
    "    \n",
    "    # Stop server\n",
    "    server.stop_server()\n",
    "    time.sleep(2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e6c06",
   "metadata": {},
   "source": [
    "## Step 10: Creating Custom Quantizations\n",
    "\n",
    "Use llama-quantize to create your own quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”§ CREATING CUSTOM QUANTIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "quantize_guide = \"\"\"\n",
    "To quantize a model yourself, use llama-quantize:\n",
    "\n",
    "ðŸ”¹ Basic Usage:\n",
    "   llama-quantize input.gguf output.gguf Q4_K_M\n",
    "\n",
    "ðŸ”¹ With Importance Matrix (for I-quants):\n",
    "   # First, generate importance matrix\n",
    "   llama-imatrix -m model.gguf -f calibration.txt -o imatrix.dat\n",
    "   \n",
    "   # Then quantize with imatrix\n",
    "   llama-quantize --imatrix imatrix.dat input.gguf output.gguf IQ3_XS\n",
    "\n",
    "ðŸ”¹ Available in llamatelemetry:\n",
    "   from llamatelemetry.quantization import quantize_model\n",
    "   \n",
    "   quantize_model(\n",
    "       input_path=\"model-f16.gguf\",\n",
    "       output_path=\"model-q4km.gguf\",\n",
    "       quant_type=\"Q4_K_M\"\n",
    "   )\n",
    "\n",
    "ðŸ”¹ From Unsloth/HuggingFace:\n",
    "   Most models on HuggingFace are already pre-quantized.\n",
    "   Look for repos with '-GGUF' suffix:\n",
    "   - unsloth/gemma-3-4b-it-GGUF\n",
    "   - unsloth/Llama-3.2-3B-Instruct-GGUF\n",
    "   - bartowski/Qwen2.5-14B-Instruct-GGUF\n",
    "\"\"\"\n",
    "print(quantize_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a376df4",
   "metadata": {},
   "source": [
    "## Step 11: Quick Reference Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35089e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ QUICK REFERENCE: MODEL + QUANT â†’ KAGGLE FEASIBILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reference = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model       â”‚ Quant     â”‚ Size (GB) â”‚ Kaggle T4  â”‚ Notes                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 1B params   â”‚ Q4_K_M    â”‚ 0.6       â”‚ âœ… Single  â”‚ Fast, high quality     â”‚\n",
    "â”‚ 1B params   â”‚ Q8_0      â”‚ 1.1       â”‚ âœ… Single  â”‚ Best quality for 1B    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 3B params   â”‚ Q4_K_M    â”‚ 1.8       â”‚ âœ… Single  â”‚ Recommended            â”‚\n",
    "â”‚ 3B params   â”‚ Q5_K_M    â”‚ 2.1       â”‚ âœ… Single  â”‚ Higher quality         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 4B params   â”‚ Q4_K_M    â”‚ 2.4       â”‚ âœ… Single  â”‚ â­ Sweet spot          â”‚\n",
    "â”‚ 4B params   â”‚ Q6_K      â”‚ 3.3       â”‚ âœ… Single  â”‚ Near lossless          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 7-8B params â”‚ Q4_K_M    â”‚ 4.5       â”‚ âœ… Single  â”‚ Popular choice         â”‚\n",
    "â”‚ 7-8B params â”‚ Q5_K_M    â”‚ 5.3       â”‚ âœ… Single  â”‚ Better for coding      â”‚\n",
    "â”‚ 7-8B params â”‚ Q6_K      â”‚ 6.0       â”‚ âš ï¸ Single  â”‚ Tight fit, 4K ctx      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 13-14B      â”‚ Q4_K_M    â”‚ 8.0       â”‚ âš ï¸ Single  â”‚ 2K context max         â”‚\n",
    "â”‚ 13-14B      â”‚ Q4_K_M    â”‚ 8.0       â”‚ âœ… Dual    â”‚ Split across GPUs      â”‚\n",
    "â”‚ 13-14B      â”‚ IQ3_XS    â”‚ 5.5       â”‚ âœ… Single  â”‚ Quality trade-off      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 30-34B      â”‚ Q4_K_M    â”‚ 19        â”‚ âœ… Dual    â”‚ tensor-split 0.5,0.5   â”‚\n",
    "â”‚ 30-34B      â”‚ IQ3_XS    â”‚ 12        â”‚ âš ï¸ Single  â”‚ Low context            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 70B params  â”‚ IQ3_XS    â”‚ 25        â”‚ âœ… Dual    â”‚ â­ 70B on Kaggle!      â”‚\n",
    "â”‚ 70B params  â”‚ IQ2_XS    â”‚ 19        â”‚ âœ… Dual    â”‚ Lower quality          â”‚\n",
    "â”‚ 70B params  â”‚ Q4_K_M    â”‚ 40        â”‚ âŒ         â”‚ Too large              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Legend:\n",
    "  âœ… Works well    âš ï¸ Possible with limits    âŒ Won't fit\n",
    "\"\"\"\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c8bdb",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Q4_K_M** is the recommended default - best balance of size and quality\n",
    "2. **Q5_K_M** for better quality when VRAM allows\n",
    "3. **IQ3_XS** for fitting large models (70B) on limited hardware\n",
    "4. Always check HuggingFace for pre-quantized models (faster than doing it yourself)\n",
    "\n",
    "### Kaggle T4 Guidelines:\n",
    "- Single T4 (15GB): Up to 8B Q4_K_M comfortably\n",
    "- Dual T4 (30GB): Up to 70B IQ3_XS or 34B Q4_K_M\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [05-unsloth-integration](05-unsloth-integration-llamatelemetry-v0.1.0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
