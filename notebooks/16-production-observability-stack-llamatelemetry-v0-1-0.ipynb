{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 16: End-to-End Production Observability Stack\n",
    "\n",
    "**Complete Integration: CUDA Inference + OpenTelemetry + Unified Visualizations**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives Demonstrated\n",
    "\n",
    "‚úÖ **CUDA Inference** (GPU 0) - Production-grade inference pipeline\n",
    "\n",
    "‚úÖ **LLM Observability** (GPU 0) - Full OpenTelemetry + llama.cpp metrics\n",
    "\n",
    "‚úÖ **Unified Visualizations** (GPU 1) - Graphistry 2D + Plotly 3D/2D integrated dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is the **flagship comprehensive notebook** that integrates all three core objectives of llamatelemetry into a unified production observability stack. It combines:\n",
    "- CUDA-optimized LLM inference on GPU 0\n",
    "- Multi-layer observability (OpenTelemetry + llama.cpp + GPU metrics)\n",
    "- Unified visualization dashboard mixing Graphistry graph viz + Plotly charts\n",
    "\n",
    "**What You'll Build:**\n",
    "- Production-ready inference pipeline with full instrumentation\n",
    "- Multi-source telemetry collection (traces, metrics, logs, GPU stats)\n",
    "- Unified dashboard showing:\n",
    "  - Request trace graphs (Graphistry 2D)\n",
    "  - Performance metrics charts (Plotly 2D)\n",
    "  - 3D model internals visualization (Plotly 3D)\n",
    "  - Real-time monitoring panels\n",
    "- Complete observability stack deployment\n",
    "\n",
    "**Time:** 45 minutes\n",
    "\n",
    "**Difficulty:** Expert\n",
    "\n",
    "**VRAM:** GPU 0: 6-10 GB, GPU 1: 3-5 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup (5 min)\n",
    "\n",
    "### Cell 1: Install llamatelemetry v0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llamatelemetry v0.1.0\n",
    "!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n",
    "\n",
    "import llamatelemetry\n",
    "print(f\"‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Install Observability Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenTelemetry and monitoring tools\n",
    "!pip install -q \\\n",
    "    opentelemetry-api==1.37.0 \\\n",
    "    opentelemetry-sdk==1.37.0 \\\n",
    "    opentelemetry-exporter-otlp-proto-grpc==1.37.0 \\\n",
    "    opentelemetry-instrumentation \\\n",
    "    pynvml requests\n",
    "\n",
    "print(\"‚úÖ Observability stack installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Install Visualization Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install visualization tools\n",
    "!pip install -q \\\n",
    "    plotly pandas numpy \\\n",
    "    pygraphistry \\\n",
    "    umap-learn scikit-learn\n",
    "\n",
    "# Install RAPIDS for GPU-accelerated graph analytics (optional)\n",
    "!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\" \"cudf-cu12==25.6.*\"\n",
    "\n",
    "print(\"‚úÖ Visualization stack installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Verify Dual GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dual GPU environment\n",
    "!nvidia-smi --query-gpu=index,name,memory.total,compute_cap --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f\"\\nFound {torch.cuda.device_count()} GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dual GPU environment verified\")\n",
    "print(\"   GPU 0 ‚Üí LLM Inference + Observability\")\n",
    "print(\"   GPU 1 ‚Üí Unified Visualization Dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Multi-Layer Observability Setup (10 min)\n",
    "\n",
    "### Cell 5: Configure Resource Attributes (GPU Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.resources import Resource\n",
    "\n",
    "resource = Resource.create({\n",
    "    \"service.name\": \"llamatelemetry-production\",\n",
    "    \"service.version\": \"0.1.0\",\n",
    "    \"deployment.environment\": \"kaggle\",\n",
    "    \"host.name\": \"kaggle-t4-dual\",\n",
    "    \"gpu.model\": \"Tesla T4\",\n",
    "    \"gpu.count\": 2,\n",
    "    \"gpu.compute_capability\": \"7.5\",\n",
    "    \"llm.framework\": \"llama.cpp\",\n",
    "    \"llm.backend\": \"gguf\",\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Resource attributes configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Setup Complete OpenTelemetry Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace, metrics, _logs\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk._logs import LoggerProvider\n",
    "from opentelemetry.sdk.trace.export import (\n",
    "    BatchSpanProcessor,\n",
    "    ConsoleSpanExporter,\n",
    "    InMemorySpanExporter,\n",
    ")\n",
    "from opentelemetry.sdk.metrics.export import (\n",
    "    PeriodicExportingMetricReader,\n",
    "    ConsoleMetricExporter,\n",
    ")\n",
    "from opentelemetry.sdk._logs.export import (\n",
    "    BatchLogRecordProcessor,\n",
    "    ConsoleLogExporter,\n",
    ")\n",
    "\n",
    "# Tracing\n",
    "memory_span_exporter = InMemorySpanExporter()\n",
    "tracer_provider = TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n",
    "tracer_provider.add_span_processor(BatchSpanProcessor(memory_span_exporter))\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Metrics\n",
    "meter_provider = MeterProvider(\n",
    "    resource=resource,\n",
    "    metric_readers=[\n",
    "        PeriodicExportingMetricReader(\n",
    "            ConsoleMetricExporter(),\n",
    "            export_interval_millis=10000,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "metrics.set_meter_provider(meter_provider)\n",
    "meter = metrics.get_meter(__name__)\n",
    "\n",
    "# Logging\n",
    "logger_provider = LoggerProvider(resource=resource)\n",
    "logger_provider.add_log_record_processor(BatchLogRecordProcessor(ConsoleLogExporter()))\n",
    "_logs.set_logger_provider(logger_provider)\n",
    "logger = _logs.get_logger(__name__)\n",
    "\n",
    "print(\"‚úÖ OpenTelemetry stack initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Create Custom Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counters\n",
    "request_counter = meter.create_counter(\n",
    "    \"llm.requests.total\",\n",
    "    description=\"Total LLM requests\",\n",
    "    unit=\"1\",\n",
    ")\n",
    "\n",
    "error_counter = meter.create_counter(\n",
    "    \"llm.errors.total\",\n",
    "    description=\"Total LLM errors\",\n",
    "    unit=\"1\",\n",
    ")\n",
    "\n",
    "# Histograms\n",
    "latency_histogram = meter.create_histogram(\n",
    "    \"llm.request.duration\",\n",
    "    description=\"Request latency distribution\",\n",
    "    unit=\"ms\",\n",
    ")\n",
    "\n",
    "token_histogram = meter.create_histogram(\n",
    "    \"llm.tokens.count\",\n",
    "    description=\"Token count distribution\",\n",
    "    unit=\"{token}\",\n",
    ")\n",
    "\n",
    "# Observable Gauges\n",
    "def get_gpu_memory_callback(options):\n",
    "    \"\"\"Callback for GPU memory observable gauge\"\"\"\n",
    "    import pynvml\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        yield metrics.Observation(\n",
    "            value=memory_info.used / 1024**2,  # MB\n",
    "            attributes={\"gpu.id\": \"0\"}\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "gpu_memory_gauge = meter.create_observable_gauge(\n",
    "    \"gpu.memory.used\",\n",
    "    callbacks=[get_gpu_memory_callback],\n",
    "    description=\"GPU memory usage\",\n",
    "    unit=\"MB\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom instruments created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Define Unified Metrics Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pynvml\n",
    "\n",
    "class UnifiedMetricsCollector:\n",
    "    \"\"\"Collects metrics from all observability sources\"\"\"\n",
    "\n",
    "    def __init__(self, server_url: str, tracer, memory_exporter):\n",
    "        self.server_url = server_url\n",
    "        self.tracer = tracer\n",
    "        self.memory_exporter = memory_exporter\n",
    "        self.running = False\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        # Storage\n",
    "        self.otel_spans = []\n",
    "        self.llama_metrics = defaultdict(list)\n",
    "        self.gpu_metrics = []\n",
    "        self.model_internals = {}\n",
    "        self.timestamps = []\n",
    "\n",
    "        # Initialize PyNVML\n",
    "        try:\n",
    "            pynvml.nvmlInit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def collect_otel_spans(self):\n",
    "        \"\"\"Get spans from memory exporter\"\"\"\n",
    "        spans = self.memory_exporter.get_finished_spans()\n",
    "        with self.lock:\n",
    "            self.otel_spans.extend(spans)\n",
    "        return len(spans)\n",
    "\n",
    "    def collect_llama_metrics(self):\n",
    "        \"\"\"Poll llama.cpp /metrics endpoint\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/metrics\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                # Parse Prometheus metrics (simplified)\n",
    "                metrics = {}\n",
    "                for line in response.text.split(\"\\n\"):\n",
    "                    if line.startswith(\"llamacpp:\"):\n",
    "                        parts = line.split()\n",
    "                        if len(parts) >= 2:\n",
    "                            name = parts[0]\n",
    "                            value = float(parts[1])\n",
    "                            metrics[name] = value\n",
    "\n",
    "                with self.lock:\n",
    "                    for key, value in metrics.items():\n",
    "                        self.llama_metrics[key].append(value)\n",
    "                return metrics\n",
    "        except:\n",
    "            pass\n",
    "        return {}\n",
    "\n",
    "    def collect_gpu_metrics(self):\n",
    "        \"\"\"Collect GPU metrics via PyNVML\"\"\"\n",
    "        try:\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # W\n",
    "\n",
    "            gpu_data = {\n",
    "                \"timestamp\": time.time(),\n",
    "                \"utilization\": utilization.gpu,\n",
    "                \"memory_used_mb\": memory.used / 1024**2,\n",
    "                \"memory_total_mb\": memory.total / 1024**2,\n",
    "                \"temperature_c\": temp,\n",
    "                \"power_w\": power,\n",
    "            }\n",
    "\n",
    "            with self.lock:\n",
    "                self.gpu_metrics.append(gpu_data)\n",
    "            return gpu_data\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def collect_all(self):\n",
    "        \"\"\"Single collection cycle across all sources\"\"\"\n",
    "        timestamp = time.time()\n",
    "\n",
    "        otel_count = self.collect_otel_spans()\n",
    "        llama_metrics = self.collect_llama_metrics()\n",
    "        gpu_data = self.collect_gpu_metrics()\n",
    "\n",
    "        with self.lock:\n",
    "            self.timestamps.append(timestamp)\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"otel_spans\": otel_count,\n",
    "            \"llama_metrics\": len(llama_metrics),\n",
    "            \"gpu_data\": bool(gpu_data),\n",
    "        }\n",
    "\n",
    "    def start_background_collection(self, interval: float = 1.0):\n",
    "        \"\"\"Start continuous collection in background\"\"\"\n",
    "        self.running = True\n",
    "\n",
    "        def collect_loop():\n",
    "            while self.running:\n",
    "                self.collect_all()\n",
    "                time.sleep(interval)\n",
    "\n",
    "        thread = threading.Thread(target=collect_loop, daemon=True)\n",
    "        thread.start()\n",
    "        print(f\"üìä Started unified metrics collection (interval={interval}s)\")\n",
    "\n",
    "    def stop_background_collection(self):\n",
    "        \"\"\"Stop collection\"\"\"\n",
    "        self.running = False\n",
    "        print(\"‚èπÔ∏è Stopped metrics collection\")\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Get collection summary\"\"\"\n",
    "        with self.lock:\n",
    "            return {\n",
    "                \"total_spans\": len(self.otel_spans),\n",
    "                \"llama_metrics_count\": len(self.llama_metrics),\n",
    "                \"gpu_samples\": len(self.gpu_metrics),\n",
    "                \"collection_duration\": self.timestamps[-1] - self.timestamps[0] if self.timestamps else 0,\n",
    "            }\n",
    "\n",
    "# Initialize unified collector (will be used after server starts)\n",
    "print(\"‚úÖ Unified metrics collector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Start Instrumented Inference Pipeline (5 min)\n",
    "\n",
    "### Cell 9: Download GGUF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"unsloth/Qwen2.5-3B-Instruct-GGUF\",\n",
    "    filename=\"Qwen2.5-3B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=\"/kaggle/working/models\",\n",
    ")\n",
    "print(f\"‚úÖ Model: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Start llama-server with Full Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.server import ServerManager\n",
    "\n",
    "server = ServerManager(server_url=\"http://127.0.0.1:8090\")\n",
    "server.start_server(\n",
    "    model_path=model_path,\n",
    "    gpu_layers=99,\n",
    "    tensor_split=\"1.0,0.0\",  # GPU 0 only\n",
    "    flash_attn=1,\n",
    "    n_parallel=8,  # 8 parallel slots\n",
    "    port=8090,\n",
    "    extra_args=[\"--metrics\", \"--slots\"],  # Enable observability endpoints\n",
    ")\n",
    "print(\"‚úÖ Server started with full instrumentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Start Background Metrics Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector now that server is running\n",
    "collector = UnifiedMetricsCollector(\n",
    "    server_url=\"http://127.0.0.1:8090\",\n",
    "    tracer=tracer,\n",
    "    memory_exporter=memory_span_exporter,\n",
    ")\n",
    "\n",
    "collector.start_background_collection(interval=1.0)\n",
    "time.sleep(3)  # Let it collect initial data\n",
    "print(f\"üìä Collecting metrics... {collector.get_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Create Production Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api import LlamaCppClient\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "import time\n",
    "\n",
    "class ProductionLLMClient:\n",
    "    \"\"\"Production LLM client with full instrumentation\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, tracer, meter):\n",
    "        self.client = LlamaCppClient(base_url)\n",
    "        self.tracer = tracer\n",
    "        self.request_counter = request_counter\n",
    "        self.latency_histogram = latency_histogram\n",
    "        self.token_histogram = token_histogram\n",
    "\n",
    "    def chat_completion(self, messages: list, **kwargs):\n",
    "        model = kwargs.get(\"model\", \"unknown\")\n",
    "        max_tokens = kwargs.get(\"max_tokens\", 100)\n",
    "        temperature = kwargs.get(\"temperature\", 0.7)\n",
    "\n",
    "        with self.tracer.start_as_current_span(\n",
    "            name=f\"llm.chat.{model}\",\n",
    "            kind=trace.SpanKind.CLIENT,\n",
    "        ) as span:\n",
    "            try:\n",
    "                span.set_attribute(\"llm.system\", \"llama.cpp\")\n",
    "                span.set_attribute(\"llm.model\", model)\n",
    "                span.set_attribute(\"llm.request.max_tokens\", max_tokens)\n",
    "                span.set_attribute(\"llm.request.temperature\", temperature)\n",
    "                span.set_attribute(\"llm.request.messages\", len(messages))\n",
    "\n",
    "                start_time = time.time()\n",
    "                response = self.client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    **kwargs\n",
    "                )\n",
    "                latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "                finish_reason = response.choices[0].finish_reason\n",
    "                content = response.choices[0].message.content\n",
    "\n",
    "                span.set_attribute(\"llm.response.finish_reason\", finish_reason)\n",
    "                span.set_attribute(\"llm.response.length\", len(content))\n",
    "\n",
    "                self.request_counter.add(\n",
    "                    1,\n",
    "                    attributes={\n",
    "                        \"model\": model,\n",
    "                        \"finish_reason\": finish_reason,\n",
    "                        \"status\": \"success\",\n",
    "                    }\n",
    "                )\n",
    "                self.latency_histogram.record(\n",
    "                    latency_ms,\n",
    "                    attributes={\"model\": model, \"status\": \"success\"}\n",
    "                )\n",
    "\n",
    "                if hasattr(response, 'usage'):\n",
    "                    input_tokens = getattr(response.usage, 'prompt_tokens', 0)\n",
    "                    output_tokens = getattr(response.usage, 'completion_tokens', 0)\n",
    "\n",
    "                    span.set_attribute(\"llm.usage.input_tokens\", input_tokens)\n",
    "                    span.set_attribute(\"llm.usage.output_tokens\", output_tokens)\n",
    "\n",
    "                    self.token_histogram.record(\n",
    "                        input_tokens,\n",
    "                        attributes={\"model\": model, \"token_type\": \"input\"}\n",
    "                    )\n",
    "                    self.token_histogram.record(\n",
    "                        output_tokens,\n",
    "                        attributes={\"model\": model, \"token_type\": \"output\"}\n",
    "                    )\n",
    "\n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "                return response\n",
    "\n",
    "            except Exception as e:\n",
    "                span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "                span.record_exception(e)\n",
    "                self.request_counter.add(\n",
    "                    1,\n",
    "                    attributes={\n",
    "                        \"model\": model,\n",
    "                        \"status\": \"error\",\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                    }\n",
    "                )\n",
    "                raise\n",
    "\n",
    "client = ProductionLLMClient(\"http://127.0.0.1:8090\", tracer, meter)\n",
    "print(\"‚úÖ Production LLM client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Generate Sample Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"Explain CUDA programming\",\n",
    "    \"What is quantization?\",\n",
    "    \"Describe transformer architecture\",\n",
    "    \"How does FlashAttention work?\",\n",
    "    \"What is GGUF format?\",\n",
    "]\n",
    "\n",
    "print(\"üöÄ Generating sample requests...\")\n",
    "for i, prompt in enumerate(test_prompts * 3):  # 15 total requests\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    print(f\"  Request {i+1}/15 complete\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"‚úÖ Generated load. Metrics: {collector.get_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Unified Visualization Dashboard (GPU 1) (20 min)\n",
    "\n",
    "### Cell 14: Switch to GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(\"üîÑ Switched to GPU 1 for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION 1: Request Trace Graphs (Graphistry 2D)\n",
    "\n",
    "### Cell 15: Setup Graphistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphistry\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secrets = UserSecretsClient()\n",
    "graphistry.register(\n",
    "    api=3,\n",
    "    username=secrets.get_secret(\"Graphistry_Username\"),\n",
    "    personal_key_id=secrets.get_secret(\"Graphistry_Personal_Key_ID\"),\n",
    "    personal_key_secret=secrets.get_secret(\"Graphistry_Personal_Key_Secret\"),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Graphistry configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 16: Transform Spans to Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with collector.lock:\n",
    "    spans = collector.otel_spans\n",
    "\n",
    "span_data = []\n",
    "for span in spans:\n",
    "    span_data.append({\n",
    "        \"span_id\": format(span.context.span_id, \"016x\"),\n",
    "        \"parent_span_id\": format(span.parent.span_id, \"016x\") if span.parent else None,\n",
    "        \"trace_id\": format(span.context.trace_id, \"032x\"),\n",
    "        \"name\": span.name,\n",
    "        \"duration_ms\": (span.end_time - span.start_time) / 1_000_000,\n",
    "        \"status\": span.status.status_code.name,\n",
    "        \"model\": span.attributes.get(\"llm.model\", \"unknown\") if span.attributes else \"unknown\",\n",
    "    })\n",
    "\n",
    "df_spans = pd.DataFrame(span_data)\n",
    "\n",
    "edges = []\n",
    "for _, span in df_spans.iterrows():\n",
    "    if span[\"parent_span_id\"]:\n",
    "        edges.append({\n",
    "            \"source\": span[\"parent_span_id\"],\n",
    "            \"destination\": span[\"span_id\"],\n",
    "        })\n",
    "\n",
    "df_edges = pd.DataFrame(edges) if edges else pd.DataFrame(columns=[\"source\", \"destination\"])\n",
    "\n",
    "print(f\"üìä Spans: {len(df_spans)}, Edges: {len(df_edges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 17: Create Graphistry Trace Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_edges) > 0:\n",
    "    g = graphistry.edges(df_edges, \"source\", \"destination\")\n",
    "    g = g.nodes(df_spans, \"span_id\")\n",
    "    g = g.bind(\n",
    "        point_title=\"name\",\n",
    "        point_size=\"duration_ms\",\n",
    "        point_color=\"status\",\n",
    "    )\n",
    "    g = g.encode_point_color(\"status\", categorical_mapping={\n",
    "        \"OK\": \"#4CAF50\", \"ERROR\": \"#F44336\", \"UNSET\": \"#9E9E9E\"\n",
    "    }, as_categorical=True)\n",
    "\n",
    "    url_traces = g.plot(render=False)\n",
    "    print(f\"üîó Trace Graph: {url_traces}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No trace edges available for visualization\")\n",
    "    url_traces = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION 2: Performance Metrics (Plotly 2D)\n",
    "\n",
    "### Cell 18: Prepare Metrics DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with collector.lock:\n",
    "    df_gpu = pd.DataFrame(collector.gpu_metrics)\n",
    "\n",
    "if len(df_gpu) > 0:\n",
    "    df_gpu[\"timestamp\"] = pd.to_datetime(df_gpu[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "# Create metrics from spans\n",
    "span_metrics = []\n",
    "for span in collector.otel_spans:\n",
    "    attrs = span.attributes or {}\n",
    "    span_metrics.append({\n",
    "        \"timestamp\": pd.to_datetime(span.start_time, unit=\"ns\"),\n",
    "        \"duration_ms\": (span.end_time - span.start_time) / 1_000_000,\n",
    "        \"input_tokens\": attrs.get(\"llm.usage.input_tokens\", 0),\n",
    "        \"output_tokens\": attrs.get(\"llm.usage.output_tokens\", 0),\n",
    "        \"status\": span.status.status_code.name,\n",
    "    })\n",
    "\n",
    "df_span_metrics = pd.DataFrame(span_metrics)\n",
    "\n",
    "print(f\"üìä GPU samples: {len(df_gpu)}, Span metrics: {len(df_span_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 19: Create Comprehensive 2D Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Latency Distribution (ms)\",\n",
    "        \"GPU Utilization Over Time (%)\",\n",
    "        \"Token Usage (Input vs Output)\",\n",
    "        \"GPU Memory Usage (MB)\",\n",
    "        \"Request Success Rate\",\n",
    "        \"GPU Temperature & Power\"\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]\n",
    "    ],\n",
    "    vertical_spacing=0.12,\n",
    ")\n",
    "\n",
    "# 1. Latency histogram\n",
    "if len(df_span_metrics) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df_span_metrics[\"duration_ms\"],\n",
    "            nbinsx=30,\n",
    "            name=\"Latency\",\n",
    "            marker_color=\"blue\",\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. GPU utilization over time\n",
    "if len(df_gpu) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_gpu[\"timestamp\"],\n",
    "            y=df_gpu[\"utilization\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"GPU %\",\n",
    "            line=dict(color=\"green\"),\n",
    "            fill=\"tozeroy\",\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. Token usage scatter\n",
    "if len(df_span_metrics) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_span_metrics[\"input_tokens\"],\n",
    "            y=df_span_metrics[\"output_tokens\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Tokens\",\n",
    "            marker=dict(\n",
    "                size=df_span_metrics[\"duration_ms\"] / 10,\n",
    "                color=df_span_metrics[\"duration_ms\"],\n",
    "                colorscale=\"Viridis\",\n",
    "                showscale=True,\n",
    "            ),\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. GPU memory usage\n",
    "if len(df_gpu) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_gpu[\"timestamp\"],\n",
    "            y=df_gpu[\"memory_used_mb\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Memory MB\",\n",
    "            line=dict(color=\"red\"),\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# 5. Success rate bar\n",
    "if len(df_span_metrics) > 0:\n",
    "    status_counts = df_span_metrics[\"status\"].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=status_counts.index,\n",
    "            y=status_counts.values,\n",
    "            name=\"Requests\",\n",
    "            marker_color=[\"green\" if s == \"OK\" else \"red\" for s in status_counts.index],\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# 6. Temperature and power\n",
    "if len(df_gpu) > 0:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_gpu[\"timestamp\"],\n",
    "            y=df_gpu[\"temperature_c\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Temp ¬∞C\",\n",
    "            line=dict(color=\"orange\"),\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_gpu[\"timestamp\"],\n",
    "            y=df_gpu[\"power_w\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Power W\",\n",
    "            line=dict(color=\"purple\"),\n",
    "            yaxis=\"y2\",\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"üìä Performance Metrics Dashboard (2D)\",\n",
    "    showlegend=True,\n",
    "    height=900,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION 3: Model Internals 3D (Plotly 3D)\n",
    "\n",
    "### Cell 20: Extract Token Embeddings (Synthetic Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Simulate 100 tokens with 768-dim embeddings\n",
    "np.random.seed(42)\n",
    "n_tokens = 100\n",
    "embedding_dim = 768\n",
    "\n",
    "# Create synthetic embeddings (replace with actual GGUF extraction in production)\n",
    "embeddings = np.random.randn(n_tokens, embedding_dim)\n",
    "\n",
    "# Add semantic clustering (simulate word categories)\n",
    "categories = [\"tech\", \"math\", \"science\", \"language\", \"general\"]\n",
    "token_categories = np.random.choice(categories, size=n_tokens)\n",
    "\n",
    "# Project to 3D using PCA\n",
    "pca = PCA(n_components=3)\n",
    "embeddings_3d = pca.fit_transform(embeddings)\n",
    "\n",
    "df_embeddings = pd.DataFrame({\n",
    "    \"x\": embeddings_3d[:, 0],\n",
    "    \"y\": embeddings_3d[:, 1],\n",
    "    \"z\": embeddings_3d[:, 2],\n",
    "    \"token_id\": range(n_tokens),\n",
    "    \"category\": token_categories,\n",
    "})\n",
    "\n",
    "print(f\"üìä Embedded {n_tokens} tokens to 3D space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 21: Create 3D Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    df_embeddings,\n",
    "    x=\"x\", y=\"y\", z=\"z\",\n",
    "    color=\"category\",\n",
    "    hover_data=[\"token_id\"],\n",
    "    title=\"Token Embedding Space (3D PCA Projection)\",\n",
    "    labels={\"x\": \"PC1\", \"y\": \"PC2\", \"z\": \"PC3\"},\n",
    "    opacity=0.7,\n",
    ")\n",
    "\n",
    "fig_3d.update_traces(marker=dict(size=5))\n",
    "fig_3d.update_layout(height=700)\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 22: 3D Attention Heatmap (Surface Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic attention weights (replace with actual extraction)\n",
    "attention_heads = 8\n",
    "seq_length = 64\n",
    "\n",
    "# Simulate attention weights for one head\n",
    "attention_weights = np.random.rand(seq_length, seq_length)\n",
    "attention_weights = (attention_weights + attention_weights.T) / 2  # Symmetric\n",
    "\n",
    "fig_attn = go.Figure(data=[go.Surface(\n",
    "    z=attention_weights,\n",
    "    colorscale=\"RdBu\",\n",
    "    colorbar=dict(title=\"Attention\"),\n",
    ")])\n",
    "\n",
    "fig_attn.update_layout(\n",
    "    title=\"Attention Weight Heatmap (Head 0, 3D Surface)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Query Position\",\n",
    "        yaxis_title=\"Key Position\",\n",
    "        zaxis_title=\"Attention Score\",\n",
    "    ),\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig_attn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION 4: Real-Time Monitoring Panel\n",
    "\n",
    "### Cell 23: Live Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create live monitoring panel (static snapshot)\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Get latest values\n",
    "latest_gpu = df_gpu.iloc[-1] if len(df_gpu) > 0 else {}\n",
    "total_requests = len(df_span_metrics)\n",
    "success_rate = (df_span_metrics[\"status\"] == \"OK\").mean() * 100 if len(df_span_metrics) > 0 else 0\n",
    "avg_latency = df_span_metrics[\"duration_ms\"].mean() if len(df_span_metrics) > 0 else 0\n",
    "\n",
    "# Create indicator panel\n",
    "fig_monitor = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[\n",
    "        [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
    "        [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}]\n",
    "    ],\n",
    "    subplot_titles=(\"GPU Utilization\", \"Success Rate\", \"Avg Latency\", \"Temperature\")\n",
    ")\n",
    "\n",
    "# GPU Utilization Gauge\n",
    "fig_monitor.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=latest_gpu.get(\"utilization\", 0),\n",
    "        title={\"text\": \"GPU %\"},\n",
    "        gauge={\"axis\": {\"range\": [0, 100]}, \"bar\": {\"color\": \"green\"}},\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Success Rate Gauge\n",
    "fig_monitor.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number+delta\",\n",
    "        value=success_rate,\n",
    "        title={\"text\": \"Success %\"},\n",
    "        delta={\"reference\": 100},\n",
    "        gauge={\"axis\": {\"range\": [0, 100]}, \"bar\": {\"color\": \"blue\"}},\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Latency Gauge\n",
    "fig_monitor.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"number+delta\",\n",
    "        value=avg_latency,\n",
    "        title={\"text\": \"Avg Latency (ms)\"},\n",
    "        delta={\"reference\": 500, \"relative\": False},\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Temperature Gauge\n",
    "fig_monitor.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=latest_gpu.get(\"temperature_c\", 0),\n",
    "        title={\"text\": \"Temp ¬∞C\"},\n",
    "        gauge={\n",
    "            \"axis\": {\"range\": [0, 100]},\n",
    "            \"bar\": {\"color\": \"orange\"},\n",
    "            \"threshold\": {\"line\": {\"color\": \"red\", \"width\": 4}, \"thickness\": 0.75, \"value\": 80},\n",
    "        },\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig_monitor.update_layout(\n",
    "    title_text=\"üî¥ LIVE Monitoring Panel\",\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig_monitor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Summary & Analysis (5 min)\n",
    "\n",
    "### Cell 24: Print Observability Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = collector.get_summary()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä PRODUCTION OBSERVABILITY STACK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n‚úÖ OpenTelemetry:\")\n",
    "print(f\"  Total Spans: {summary['total_spans']}\")\n",
    "print(f\"  Trace Duration: {summary['collection_duration']:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚úÖ llama.cpp Metrics:\")\n",
    "print(f\"  Metric Types Collected: {summary['llama_metrics_count']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ GPU Monitoring:\")\n",
    "print(f\"  Samples Collected: {summary['gpu_samples']}\")\n",
    "if len(df_gpu) > 0:\n",
    "    print(f\"  Avg GPU Utilization: {df_gpu['utilization'].mean():.2f}%\")\n",
    "    print(f\"  Peak Memory: {df_gpu['memory_used_mb'].max():.2f} MB\")\n",
    "    print(f\"  Max Temperature: {df_gpu['temperature_c'].max():.2f}¬∞C\")\n",
    "\n",
    "print(f\"\\n‚úÖ Request Statistics:\")\n",
    "print(f\"  Total Requests: {len(df_span_metrics)}\")\n",
    "if len(df_span_metrics) > 0:\n",
    "    print(f\"  Success Rate: {success_rate:.2f}%\")\n",
    "    print(f\"  Avg Latency: {avg_latency:.2f}ms\")\n",
    "    print(f\"  P95 Latency: {df_span_metrics['duration_ms'].quantile(0.95):.2f}ms\")\n",
    "    print(f\"  Total Tokens: {df_span_metrics['input_tokens'].sum() + df_span_metrics['output_tokens'].sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 25: Visualization Links Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüé® UNIFIED DASHBOARD COMPONENTS:\")\n",
    "print(f\"\\n1Ô∏è‚É£ Trace Graphs (Graphistry 2D):\")\n",
    "if url_traces:\n",
    "    print(f\"   {url_traces}\")\n",
    "else:\n",
    "    print(\"   Not available (no trace edges)\")\n",
    "print(f\"\\n2Ô∏è‚É£ Performance Metrics (Plotly 2D): ‚úÖ Rendered above\")\n",
    "print(f\"\\n3Ô∏è‚É£ Model Internals (Plotly 3D): ‚úÖ Rendered above\")\n",
    "print(f\"\\n4Ô∏è‚É£ Real-Time Monitoring: ‚úÖ Rendered above\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ PRODUCTION OBSERVABILITY STACK COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ ALL THREE OBJECTIVES ACHIEVED:\")\n",
    "print(\"   1. CUDA Inference (GPU 0)\")\n",
    "print(\"   2. LLM Observability (GPU 0)\")\n",
    "print(\"   3. Unified Visualizations (GPU 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Cleanup\n",
    "\n",
    "### Cell 26: Stop All Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop metrics collection\n",
    "collector.stop_background_collection()\n",
    "\n",
    "# Stop server\n",
    "server.stop_server()\n",
    "\n",
    "print(\"‚úÖ All services stopped. Observability stack demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
