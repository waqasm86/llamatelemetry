{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + Graph Visualization (Split-GPU)\n",
    "\n",
    "**Duration:** ~30 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook demonstrates the **split-GPU architecture**: LLM inference on GPU 0\n",
    "and RAPIDS/Graphistry graph analytics on GPU 1.\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "GPU 0 (15 GB)          GPU 1 (15 GB)\n",
    "┌────────────────┐     ┌────────────────┐\n",
    "│  llama-server  │     │  cuDF / cuGraph │\n",
    "│  (LLM model)   │     │  Graphistry     │\n",
    "└────────────────┘     └────────────────┘\n",
    "```\n",
    "\n",
    "### What you'll learn\n",
    "1. Configure split-GPU mode\n",
    "2. Extract entities from text with LLM\n",
    "3. Build graph structures on GPU with RAPIDS\n",
    "4. Visualize with Graphistry\n",
    "5. Build traced end-to-end pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "# RAPIDS and graphistry are pre-installed on Kaggle GPU notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Split-GPU Mode\n",
    "\n",
    "Start the LLM server on GPU 0 with `tensor_split=\"1.0,0.0\"` to leave GPU 1\n",
    "entirely free for RAPIDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamatelemetry\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from llamatelemetry.kaggle import rapids_gpu, auto_register_graphistry\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llamatelemetry.init(service_name=\"graph-viz\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# LLM on GPU 0 only\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"1.0,0.0\", ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Register Graphistry (uses Kaggle secrets)\n",
    "auto_register_graphistry()\n",
    "print(\"Split-GPU mode active: LLM on GPU 0, RAPIDS on GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data with LLM\n",
    "\n",
    "Use the LLM to extract entities and relationships from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@llamatelemetry.trace(name=\"extract-entities\")\n",
    "def extract_entities(text):\n",
    "    prompt = f\"\"\"Extract entities and relationships from this text as JSON.\n",
    "Return format: {{\"entities\": [\"name1\", \"name2\"], \"relationships\": [[\"entity1\", \"relation\", \"entity2\"]]}}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256, temperature=0.3,\n",
    "    )\n",
    "    try:\n",
    "        return json.loads(resp.choices[0].message.content)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: try to find JSON in response\n",
    "        content = resp.choices[0].message.content\n",
    "        start = content.find(\"{\")\n",
    "        end = content.rfind(\"}\") + 1\n",
    "        if start >= 0 and end > start:\n",
    "            return json.loads(content[start:end])\n",
    "        return {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "texts = [\n",
    "    \"Albert Einstein developed the theory of relativity at the University of Zurich. He later moved to Princeton.\",\n",
    "    \"Marie Curie discovered radium and polonium. She worked at the University of Paris with Pierre Curie.\",\n",
    "    \"Alan Turing worked at Bletchley Park during WWII. He later joined the University of Manchester.\",\n",
    "]\n",
    "\n",
    "all_entities = []\n",
    "all_relationships = []\n",
    "for text in texts:\n",
    "    result = extract_entities(text)\n",
    "    all_entities.extend(result.get(\"entities\", []))\n",
    "    all_relationships.extend(result.get(\"relationships\", []))\n",
    "    print(f\"Extracted {len(result.get('entities', []))} entities, {len(result.get('relationships', []))} relationships\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_entities)} entities, {len(all_relationships)} relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph on GPU 1\n",
    "\n",
    "Use RAPIDS cuDF and cuGraph on GPU 1 to construct the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rapids_gpu(1):\n",
    "    try:\n",
    "        import cudf\n",
    "        import cugraph\n",
    "\n",
    "        # Build edge DataFrame\n",
    "        edges = []\n",
    "        for rel in all_relationships:\n",
    "            if len(rel) >= 3:\n",
    "                edges.append({\"src\": rel[0], \"relationship\": rel[1], \"dst\": rel[2]})\n",
    "\n",
    "        if edges:\n",
    "            edge_df = cudf.DataFrame(edges)\n",
    "            print(f\"Edge DataFrame ({len(edge_df)} edges):\")\n",
    "            print(edge_df.to_pandas())\n",
    "        else:\n",
    "            print(\"No edges extracted — using sample data\")\n",
    "            edge_df = cudf.DataFrame({\n",
    "                \"src\": [\"Einstein\", \"Einstein\", \"Curie\", \"Curie\", \"Turing\"],\n",
    "                \"relationship\": [\"worked_at\", \"moved_to\", \"discovered\", \"worked_at\", \"worked_at\"],\n",
    "                \"dst\": [\"Zurich\", \"Princeton\", \"Radium\", \"Paris\", \"Bletchley Park\"],\n",
    "            })\n",
    "    except ImportError:\n",
    "        import pandas as pd\n",
    "        print(\"RAPIDS not available — using pandas fallback\")\n",
    "        edge_df = pd.DataFrame({\n",
    "            \"src\": [\"Einstein\", \"Einstein\", \"Curie\", \"Curie\", \"Turing\"],\n",
    "            \"relationship\": [\"worked_at\", \"moved_to\", \"discovered\", \"worked_at\", \"worked_at\"],\n",
    "            \"dst\": [\"Zurich\", \"Princeton\", \"Radium\", \"Paris\", \"Bletchley Park\"],\n",
    "        })\n",
    "        print(edge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with Graphistry\n",
    "\n",
    "Graphistry renders interactive GPU-accelerated graph visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rapids_gpu(1):\n",
    "    try:\n",
    "        import graphistry\n",
    "\n",
    "        g = graphistry.edges(edge_df, \"src\", \"dst\")\n",
    "        g = g.bind(edge_title=\"relationship\")\n",
    "        g.plot()\n",
    "    except Exception as e:\n",
    "        print(f\"Graphistry visualization: {e}\")\n",
    "        print(\"Tip: Ensure GRAPHISTRY_API_KEY is set in Kaggle secrets\")\n",
    "        # Fallback: print the graph structure\n",
    "        print(\"\\nGraph structure:\")\n",
    "        if hasattr(edge_df, 'to_pandas'):\n",
    "            print(edge_df.to_pandas().to_string(index=False))\n",
    "        else:\n",
    "            print(edge_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Pipeline\n",
    "\n",
    "A full traced pipeline: extract → build → visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.workflow(name=\"graph-pipeline\")\n",
    "def graph_pipeline(texts):\n",
    "    \"\"\"End-to-end: text → entities → graph → visualization.\"\"\"\n",
    "\n",
    "    # Step 1: Extract entities from all texts\n",
    "    entities, relationships = [], []\n",
    "    for text in texts:\n",
    "        with llamatelemetry.span(\"extract\", text_length=len(text)):\n",
    "            result = extract_entities(text)\n",
    "            entities.extend(result.get(\"entities\", []))\n",
    "            relationships.extend(result.get(\"relationships\", []))\n",
    "\n",
    "    # Step 2: Build graph\n",
    "    with llamatelemetry.span(\"build-graph\", num_edges=len(relationships)):\n",
    "        import pandas as pd\n",
    "        edges = []\n",
    "        for rel in relationships:\n",
    "            if len(rel) >= 3:\n",
    "                edges.append({\"src\": rel[0], \"rel\": rel[1], \"dst\": rel[2]})\n",
    "        df = pd.DataFrame(edges) if edges else pd.DataFrame(columns=[\"src\", \"rel\", \"dst\"])\n",
    "\n",
    "    print(f\"Pipeline complete: {len(entities)} entities, {len(edges)} edges\")\n",
    "    return df\n",
    "\n",
    "result_df = graph_pipeline([\n",
    "    \"Isaac Newton formulated the laws of motion at Cambridge University.\",\n",
    "    \"Niels Bohr developed the atomic model at the University of Copenhagen.\",\n",
    "])\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The split-GPU architecture enables:\n",
    "- **GPU 0**: Full 15 GB for the LLM model\n",
    "- **GPU 1**: Full 15 GB for RAPIDS analytics + Graphistry rendering\n",
    "- **No VRAM contention** between LLM and graph workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
