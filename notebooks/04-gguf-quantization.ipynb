{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GGUF Quantization Guide\n",
    "\n",
    "**Duration:** ~20 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook covers GGUF quantization types, how to inspect GGUF files,\n",
    "VRAM planning for Kaggle T4 GPUs, and benchmarking different quantizations.\n",
    "\n",
    "### Quantization Overview\n",
    "\n",
    "| Family | Types | Bits | Quality | Speed |\n",
    "|--------|-------|------|---------|-------|\n",
    "| **Legacy** | Q4_0, Q8_0 | 4-8 | Low-High | Fast |\n",
    "| **K-Quants** | Q3_K_S … Q6_K | 3-6 | Good | Balanced |\n",
    "| **I-Quants** | IQ3_XS … IQ4_NL | 3-4 | Best-at-size | Slower |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "\n",
    "import llamatelemetry\n",
    "llamatelemetry.init(service_name=\"gguf-quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Quantization Types\n",
    "\n",
    "The SDK provides `QUANT_TYPE_INFO` with metadata for every GGML quantization type,\n",
    "including bits-per-weight and recommended use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api import GGMLType\n",
    "\n",
    "# Common quantization types and their properties\n",
    "quant_info = [\n",
    "    (\"Q4_0\",    4.0,  \"Legacy 4-bit, fast but lower quality\"),\n",
    "    (\"Q4_K_M\",  4.5,  \"K-Quant 4-bit medium — best balance for most uses\"),\n",
    "    (\"Q5_K_M\",  5.5,  \"K-Quant 5-bit medium — higher quality, +20% VRAM\"),\n",
    "    (\"Q6_K\",    6.5,  \"K-Quant 6-bit — near-FP16 quality\"),\n",
    "    (\"Q8_0\",    8.0,  \"8-bit — highest quality quantized\"),\n",
    "    (\"IQ3_XS\",  3.3,  \"I-Quant 3-bit — smallest, for 70B on dual T4\"),\n",
    "    (\"IQ4_NL\",  4.5,  \"I-Quant 4-bit — better quality than Q4_K_M at same size\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Type':<10} {'Bits/W':<8} {'7B Size (GB)':<14} {'13B Size (GB)':<14} Description\")\n",
    "print(\"-\" * 80)\n",
    "for name, bpw, desc in quant_info:\n",
    "    size_7b = 7.0 * bpw / 8\n",
    "    size_13b = 13.0 * bpw / 8\n",
    "    print(f\"{name:<10} {bpw:<8.1f} {size_7b:<14.1f} {size_13b:<14.1f} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting GGUF Files\n",
    "\n",
    "Use `parse_gguf_header()` to read architecture metadata, layer counts, and\n",
    "quantization details directly from a GGUF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llamatelemetry.llama import parse_gguf_header, get_model_summary\n",
    "from llamatelemetry.models import ModelInfo\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# Parse GGUF header\n",
    "info = parse_gguf_header(model_path)\n",
    "print(f\"File: {info.file}\")\n",
    "print(f\"Size: {info.size_mb:.1f} MB\")\n",
    "print(f\"Architecture: {info.metadata.architecture}\")\n",
    "print(f\"Context length: {info.metadata.context_length}\")\n",
    "print(f\"Embedding dim: {info.metadata.embedding_length}\")\n",
    "print(f\"Layers: {info.metadata.block_count}\")\n",
    "\n",
    "# One-line summary\n",
    "print(f\"\\nSummary: {get_model_summary(model_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle T4 VRAM Recommendations\n",
    "\n",
    "Each T4 has 15 GB of VRAM. With dual T4, you have 30 GB total.\n",
    "The model must fit in VRAM plus overhead for context and KV cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VRAM calculator for Kaggle T4\n",
    "T4_VRAM_GB = 15.0\n",
    "DUAL_T4_VRAM_GB = 30.0\n",
    "KV_OVERHEAD_GB = 1.5  # approximate KV cache overhead\n",
    "\n",
    "models = [\n",
    "    (\"Gemma-3 1B\",   1,  \"Q4_K_M\", 4.5),\n",
    "    (\"Gemma-3 4B\",   4,  \"Q4_K_M\", 4.5),\n",
    "    (\"Llama-3.2 7B\", 7,  \"Q4_K_M\", 4.5),\n",
    "    (\"Llama-3.2 7B\", 7,  \"Q5_K_M\", 5.5),\n",
    "    (\"Llama-3.1 13B\", 13, \"Q4_K_M\", 4.5),\n",
    "    (\"Llama-3.1 70B\", 70, \"IQ3_XS\", 3.3),\n",
    "    (\"Llama-3.1 70B\", 70, \"Q4_K_M\", 4.5),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<20} {'Quant':<10} {'Size (GB)':<12} {'Single T4':<12} {'Dual T4'}\")\n",
    "print(\"-\" * 70)\n",
    "for name, params, quant, bpw in models:\n",
    "    size_gb = params * bpw / 8\n",
    "    fits_single = \"Yes\" if (size_gb + KV_OVERHEAD_GB) <= T4_VRAM_GB else \"No\"\n",
    "    fits_dual = \"Yes\" if (size_gb + KV_OVERHEAD_GB) <= DUAL_T4_VRAM_GB else \"No\"\n",
    "    print(f\"{name:<20} {quant:<10} {size_gb:<12.1f} {fits_single:<12} {fits_dual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Different Quantizations\n",
    "\n",
    "Compare inference speed and output quality across quantization levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "\n",
    "# Download Q5_K_M for comparison\n",
    "model_q5 = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q5_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "test_prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "quant_models = [(\"Q4_K_M\", model_path), (\"Q5_K_M\", model_q5)]\n",
    "\n",
    "for quant_name, mpath in quant_models:\n",
    "    mgr = ServerManager()\n",
    "    mgr.start_server(model_path=mpath, gpu_layers=99, ctx_size=2048)\n",
    "    mgr.wait_until_ready(timeout=60)\n",
    "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "    # Warm up\n",
    "    client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": \"Hi\"}], max_tokens=8)\n",
    "\n",
    "    # Benchmark\n",
    "    t0 = time.perf_counter()\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": test_prompt}],\n",
    "        max_tokens=128, temperature=0.7,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    tokens = resp.usage.completion_tokens\n",
    "\n",
    "    mem = llamatelemetry.gpu.snapshot()\n",
    "    vram_mb = sum(s.mem_used_mb for s in mem)\n",
    "\n",
    "    print(f\"\\n--- {quant_name} ---\")\n",
    "    print(f\"  VRAM: {vram_mb} MB total across GPUs\")\n",
    "    print(f\"  Latency: {elapsed*1000:.0f} ms\")\n",
    "    print(f\"  Throughput: {tokens/elapsed:.1f} tok/s\")\n",
    "    print(f\"  Output: {resp.choices[0].message.content[:120]}...\")\n",
    "\n",
    "    mgr.stop_server()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-Quants for Large Models\n",
    "\n",
    "I-Quants (IQ3_XS, IQ3_S, IQ4_NL) use importance-weighted quantization for\n",
    "better quality at extreme compression. This makes 70B models feasible on dual T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I-Quant VRAM estimations for large models\n",
    "large_models = [\n",
    "    (\"70B IQ3_XS\",  70, 3.3, \"Best fit for dual T4 — 28.9 GB\"),\n",
    "    (\"70B IQ3_S\",   70, 3.5, \"Slightly better quality — requires tight VRAM\"),\n",
    "    (\"70B Q4_K_M\",  70, 4.5, \"Does NOT fit on dual T4 (39.4 GB)\"),\n",
    "    (\"34B Q4_K_M\",  34, 4.5, \"Comfortable fit on dual T4\"),\n",
    "    (\"34B Q5_K_M\",  34, 5.5, \"Fits with reduced context window\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<15} {'VRAM (GB)':<12} {'Fits Dual T4?':<15} Notes\")\n",
    "print(\"-\" * 70)\n",
    "for name, params, bpw, notes in large_models:\n",
    "    vram = params * bpw / 8\n",
    "    fits = \"Yes\" if (vram + KV_OVERHEAD_GB) <= DUAL_T4_VRAM_GB else \"No\"\n",
    "    print(f\"{name:<15} {vram:<12.1f} {fits:<15} {notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "**Kaggle T4 Recommendations:**\n",
    "\n",
    "| Model Size | Recommended Quant | GPUs | Context |\n",
    "|-----------|-------------------|------|--------|\n",
    "| 1B-4B | Q5_K_M or Q6_K | Single T4 | 4096+ |\n",
    "| 7B-8B | Q4_K_M | Single or Dual T4 | 2048-4096 |\n",
    "| 13B | Q4_K_M | Dual T4 (50/50) | 2048 |\n",
    "| 34B | Q4_K_M | Dual T4 (50/50) | 1024-2048 |\n",
    "| 70B | IQ3_XS | Dual T4 (50/50) | 512-1024 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
