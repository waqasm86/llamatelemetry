{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8cf5402",
   "metadata": {},
   "source": [
    "## Step 1: Verify Dual GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5253aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç DUAL GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get GPU info\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpus = result.stdout.strip().split('\\n')\n",
    "print(f\"\\nüìä Detected {len(gpus)} GPU(s):\")\n",
    "for gpu in gpus:\n",
    "    print(f\"   {gpu}\")\n",
    "\n",
    "if len(gpus) >= 2:\n",
    "    print(\"\\n‚úÖ Dual T4 environment confirmed!\")\n",
    "    print(\"   Total VRAM: 30GB (15GB √ó 2)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Only 1 GPU detected!\")\n",
    "    print(\"   Enable 'GPU T4 x2' in Kaggle settings.\")\n",
    "\n",
    "# CUDA version\n",
    "print(\"\\nüìä CUDA Version:\")\n",
    "!nvcc --version | grep release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5ca7c",
   "metadata": {},
   "source": [
    "## Step 2: Install llamatelemetry v0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n",
    "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n",
    "!pip install -q huggingface_hub sseclient-py\n",
    "\n",
    "import llamatelemetry\n",
    "print(f\"‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2febcb",
   "metadata": {},
   "source": [
    "## Step 3: Understanding Multi-GPU Options\n",
    "\n",
    "llama.cpp provides several flags for multi-GPU configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.multigpu import MultiGPUConfig, SplitMode, GPUInfo\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã MULTI-GPU CONFIGURATION OPTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üîπ --tensor-split, -ts\n",
    "   Distributes VRAM usage across GPUs.\n",
    "   Example: --tensor-split 0.5,0.5 (50% each GPU)\n",
    "   Example: --tensor-split 0.7,0.3 (70% GPU0, 30% GPU1)\n",
    "\n",
    "üîπ --split-mode, -sm  \n",
    "   How to split the model across GPUs:\n",
    "   ‚Ä¢ 'layer' - Split by transformer layers (default, recommended)\n",
    "   ‚Ä¢ 'row'   - Split by matrix rows (can be slower)\n",
    "   ‚Ä¢ 'none'  - Disable multi-GPU (single GPU only)\n",
    "\n",
    "üîπ --main-gpu, -mg\n",
    "   Primary GPU for small tensors and scratch buffers.\n",
    "   Default: 0 (first GPU)\n",
    "\n",
    "üîπ --n-gpu-layers, -ngl\n",
    "   Number of layers to offload to GPU(s).\n",
    "   Use 99 to offload all layers.\n",
    "\"\"\")\n",
    "\n",
    "# Show split mode enum\n",
    "print(\"\\nüìã Split Modes:\")\n",
    "for mode in SplitMode:\n",
    "    print(f\"   {mode.name}: {mode.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfed953",
   "metadata": {},
   "source": [
    "## Step 4: GPU Info Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058365ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.multigpu import detect_gpus, get_free_vram\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä GPU INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get detailed GPU info using detect_gpus (actual API function)\n",
    "gpus = detect_gpus()\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(f\"\\nüîπ GPU {gpu.id}: {gpu.name}\")\n",
    "    print(f\"   Total VRAM: {gpu.memory_total_gb:.1f} GB\")\n",
    "    print(f\"   Free VRAM: {gpu.memory_free_gb:.1f} GB\")\n",
    "    if gpu.compute_capability:\n",
    "        print(f\"   Compute Capability: {gpu.compute_capability}\")\n",
    "\n",
    "# Calculate total available VRAM\n",
    "total_vram = sum(gpu.memory_free_gb for gpu in gpus)\n",
    "print(f\"\\nüìä Total Available VRAM: {total_vram:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140cd97",
   "metadata": {},
   "source": [
    "## Step 5: Download a Larger Model for Multi-GPU Testing\n",
    "\n",
    "We'll use Gemma-3-4B which benefits from dual-GPU distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# For multi-GPU testing, use a 4B model\n",
    "MODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\n",
    "MODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n",
    "\n",
    "print(f\"üì• Downloading {MODEL_FILE}...\")\n",
    "print(f\"   This ~2.5GB model will be split across both GPUs.\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=MODEL_REPO,\n",
    "    filename=MODEL_FILE,\n",
    "    local_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "\n",
    "size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "print(f\"\\n‚úÖ Model downloaded: {model_path}\")\n",
    "print(f\"   Size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35c0e9",
   "metadata": {},
   "source": [
    "## Step 6: Tensor-Split Configurations\n",
    "\n",
    "Different ways to distribute the model across GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a16bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.api.multigpu import MultiGPUConfig, SplitMode\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã TENSOR-SPLIT CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        \"name\": \"Equal Split (50/50)\",\n",
    "        \"tensor_split\": [0.5, 0.5],\n",
    "        \"description\": \"Equal distribution across both GPUs\",\n",
    "        \"use_case\": \"Default, balanced workload\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GPU 0 Heavy (70/30)\",\n",
    "        \"tensor_split\": [0.7, 0.3],\n",
    "        \"description\": \"More VRAM on GPU 0\",\n",
    "        \"use_case\": \"When GPU 1 needed for other tasks\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GPU 0 Only (100/0)\",\n",
    "        \"tensor_split\": [1.0, 0.0],\n",
    "        \"description\": \"Single GPU mode\",\n",
    "        \"use_case\": \"When GPU 1 reserved for RAPIDS/Graphistry\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, config in enumerate(configs, 1):\n",
    "    print(f\"\\nüîπ Config {i}: {config['name']}\")\n",
    "    print(f\"   Tensor Split: {config['tensor_split']}\")\n",
    "    print(f\"   Description: {config['description']}\")\n",
    "    print(f\"   Use Case: {config['use_case']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e3e2c",
   "metadata": {},
   "source": [
    "## Step 7: Start Server with Dual-GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.server import ServerManager\n",
    "from llamatelemetry.api.multigpu import SplitMode\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING DUAL-GPU SERVER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dual-GPU configuration parameters\n",
    "dual_config = {\n",
    "    \"model_path\": model_path,\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 8080,\n",
    "    \n",
    "    # Multi-GPU settings\n",
    "    \"gpu_layers\": 99,              # Offload all layers\n",
    "    \"tensor_split\": \"0.5,0.5\",     # Equal split (as comma-separated string)\n",
    "    \n",
    "    # Performance\n",
    "    \"ctx_size\": 8192,\n",
    "    \"batch_size\": 1024,\n",
    "    \n",
    "    # Parallelism\n",
    "    \"n_parallel\": 4,\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Dual-GPU Configuration:\")\n",
    "print(f\"   Model: {model_path.split('/')[-1]}\")\n",
    "print(f\"   Tensor Split: GPU0=50%, GPU1=50%\")\n",
    "print(f\"   Context Size: {dual_config['ctx_size']}\")\n",
    "\n",
    "# Start server\n",
    "server = ServerManager(server_url=f\"http://{dual_config['host']}:{dual_config['port']}\")\n",
    "print(\"\\nüöÄ Starting server...\")\n",
    "\n",
    "try:\n",
    "    server.start_server(\n",
    "        model_path=dual_config['model_path'],\n",
    "        host=dual_config['host'],\n",
    "        port=dual_config['port'],\n",
    "        gpu_layers=dual_config['gpu_layers'],\n",
    "        ctx_size=dual_config['ctx_size'],\n",
    "        batch_size=dual_config['batch_size'],\n",
    "        n_parallel=dual_config['n_parallel'],\n",
    "        timeout=120,\n",
    "        verbose=True,\n",
    "        # Multi-GPU tensor split\n",
    "        tensor_split=dual_config['tensor_split']\n",
    "    )\n",
    "    print(\"\\n‚úÖ Dual-GPU server started successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Server failed to start: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c50fb0",
   "metadata": {},
   "source": [
    "## Step 8: Verify Multi-GPU Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä GPU MEMORY DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check memory usage on both GPUs\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.used,memory.total,utilization.gpu\", \n",
    "     \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "print(\"\\nüìä GPU Memory After Model Load:\")\n",
    "for line in result.stdout.strip().split('\\n'):\n",
    "    parts = line.split(', ')\n",
    "    if len(parts) >= 4:\n",
    "        idx, name, used, total = parts[0], parts[1], parts[2], parts[3]\n",
    "        print(f\"   GPU {idx}: {used} / {total}\")\n",
    "\n",
    "print(\"\\nüí° Both GPUs should show VRAM usage if tensor-split is working.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb983b",
   "metadata": {},
   "source": [
    "## Step 9: Benchmark Multi-GPU Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3690e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llamatelemetry.api.client import LlamaCppClient\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä MULTI-GPU PERFORMANCE BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Longer prompts to test multi-GPU throughput\n",
    "prompts = [\n",
    "    \"Write a detailed explanation of how GPU parallelism works in deep learning.\",\n",
    "    \"Explain the architecture of a transformer model step by step.\",\n",
    "    \"Describe the CUDA programming model and its key concepts.\",\n",
    "    \"What are the advantages of using multiple GPUs for inference?\",\n",
    "    \"Explain tensor parallelism vs pipeline parallelism.\",\n",
    "]\n",
    "\n",
    "print(f\"\\nüèÉ Running benchmark with {len(prompts)} prompts...\\n\")\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time = 0\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.chat.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    \n",
    "    total_input_tokens += input_tokens\n",
    "    total_output_tokens += output_tokens\n",
    "    total_time += elapsed\n",
    "    \n",
    "    tok_per_sec = output_tokens / elapsed\n",
    "    print(f\"   Prompt {i}: {output_tokens} tokens in {elapsed:.2f}s ({tok_per_sec:.1f} tok/s)\")\n",
    "\n",
    "print(f\"\\nüìä Benchmark Results:\")\n",
    "print(f\"   Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"   Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"   Total Time: {total_time:.2f}s\")\n",
    "print(f\"   Average Generation Speed: {total_output_tokens/total_time:.1f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b558e2",
   "metadata": {},
   "source": [
    "## Step 10: Test Different Split Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7db403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop current server\n",
    "print(\"üõë Stopping server for reconfiguration...\")\n",
    "server.stop_server()\n",
    "\n",
    "import time\n",
    "time.sleep(2)\n",
    "print(\"‚úÖ Server stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîß TESTING 70/30 SPLIT CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 70/30 split - more on GPU 0\n",
    "config_70_30 = {\n",
    "    \"model_path\": model_path,\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 8080,\n",
    "    \"gpu_layers\": 99,\n",
    "    \"tensor_split\": \"0.7,0.3\",  # 70% GPU0, 30% GPU1\n",
    "    \"ctx_size\": 8192,\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Tensor Split: GPU0=70%, GPU1=30%\")\n",
    "print(f\"   Use Case: When GPU1 needs memory for other tasks\")\n",
    "\n",
    "try:\n",
    "    server.start_server(\n",
    "        model_path=config_70_30['model_path'],\n",
    "        host=config_70_30['host'],\n",
    "        port=config_70_30['port'],\n",
    "        gpu_layers=config_70_30['gpu_layers'],\n",
    "        ctx_size=config_70_30['ctx_size'],\n",
    "        timeout=60,\n",
    "        verbose=True,\n",
    "        tensor_split=config_70_30['tensor_split']\n",
    "    )\n",
    "    print(\"\\n‚úÖ 70/30 split server started!\")\n",
    "    \n",
    "    # Check memory distribution\n",
    "    print(\"\\nüìä Memory Distribution:\")\n",
    "    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to start: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da3269",
   "metadata": {},
   "source": [
    "## Step 11: Split-GPU Mode for LLM + RAPIDS\n",
    "\n",
    "Configure for running LLM on GPU 0 while reserving GPU 1 for RAPIDS/Graphistry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop current server\n",
    "server.stop_server()\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ SPLIT-GPU MODE: LLM + RAPIDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "This configuration runs the LLM entirely on GPU 0,\n",
    "leaving GPU 1 free for RAPIDS/cuGraph/Graphistry.\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       GPU 0 (15GB)      ‚îÇ        GPU 1 (15GB)           ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ  llama-server   ‚îÇ    ‚îÇ   ‚îÇ  RAPIDS / Graphistry    ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ  (Full Model)   ‚îÇ    ‚îÇ   ‚îÇ  (Graph Visualization)  ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# Single GPU configuration (GPU 0 only)\n",
    "split_gpu_config = {\n",
    "    \"model_path\": model_path,\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 8080,\n",
    "    \"gpu_layers\": 99,\n",
    "    \"tensor_split\": \"1.0,0.0\",  # 100% on GPU 0\n",
    "    \"ctx_size\": 4096,  # Smaller context to fit in single GPU\n",
    "}\n",
    "\n",
    "try:\n",
    "    server.start_server(\n",
    "        model_path=split_gpu_config['model_path'],\n",
    "        host=split_gpu_config['host'],\n",
    "        port=split_gpu_config['port'],\n",
    "        gpu_layers=split_gpu_config['gpu_layers'],\n",
    "        ctx_size=split_gpu_config['ctx_size'],\n",
    "        timeout=60,\n",
    "        verbose=True,\n",
    "        tensor_split=split_gpu_config['tensor_split']\n",
    "    )\n",
    "    print(\"\\n‚úÖ Split-GPU mode server started!\")\n",
    "    print(\"   GPU 0: llama-server\")\n",
    "    print(\"   GPU 1: Available for RAPIDS/Graphistry\")\n",
    "    \n",
    "    print(\"\\nüìä Memory Distribution:\")\n",
    "    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to start: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e1778",
   "metadata": {},
   "source": [
    "## Step 12: Verify GPU 1 is Free for RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe283bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä GPU 1 AVAILABILITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,memory.used,memory.free\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "lines = result.stdout.strip().split('\\n')\n",
    "if len(lines) >= 2:\n",
    "    gpu1_info = lines[1].split(', ')\n",
    "    used = gpu1_info[1].strip()\n",
    "    free = gpu1_info[2].strip()\n",
    "    \n",
    "    print(f\"\\nüìä GPU 1 Status:\")\n",
    "    print(f\"   Memory Used: {used}\")\n",
    "    print(f\"   Memory Free: {free}\")\n",
    "    \n",
    "    # Parse free memory\n",
    "    free_mb = int(free.replace(' MiB', ''))\n",
    "    if free_mb > 14000:  # > 14GB free\n",
    "        print(f\"\\n‚úÖ GPU 1 has {free_mb/1024:.1f} GB free - Ready for RAPIDS!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è GPU 1 has limited free memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd73e7f",
   "metadata": {},
   "source": [
    "## Step 13: Quick RAPIDS Verification on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a931a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Force RAPIDS to use GPU 1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî• RAPIDS ON GPU 1 VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    \n",
    "    # Create a small cuDF DataFrame on GPU 1\n",
    "    df = cudf.DataFrame({\n",
    "        'x': range(1000),\n",
    "        'y': range(1000, 2000)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úÖ cuDF working on GPU 1\")\n",
    "    print(f\"   DataFrame shape: {df.shape}\")\n",
    "    print(f\"   Memory used: {df.memory_usage().sum() / 1024:.2f} KB\")\n",
    "    \n",
    "    # Verify GPU\n",
    "    print(f\"\\nüìä cuPy GPU Info:\")\n",
    "    device = cp.cuda.Device(0)  # Device 0 in filtered view = actual GPU 1\n",
    "    print(f\"   Name: {device.attributes['Name'].decode()}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è RAPIDS not available: {e}\")\n",
    "    print(\"   Install with: pip install cudf-cu12 cuml-cu12\")\n",
    "\n",
    "# Reset CUDA_VISIBLE_DEVICES\n",
    "del os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c23da2",
   "metadata": {},
   "source": [
    "## Step 14: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõë Stopping server...\")\n",
    "server.stop_server()\n",
    "\n",
    "print(\"\\n‚úÖ Server stopped\")\n",
    "print(\"\\nüìä Final GPU Status:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9a1f2",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "You've learned:\n",
    "1. ‚úÖ Multi-GPU configuration options (tensor-split, split-mode)\n",
    "2. ‚úÖ Equal split (50/50) for maximum model size\n",
    "3. ‚úÖ Asymmetric split (70/30) for mixed workloads\n",
    "4. ‚úÖ Single-GPU mode (100/0) for LLM + RAPIDS\n",
    "5. ‚úÖ Performance benchmarking across GPUs\n",
    "\n",
    "## Configuration Quick Reference\n",
    "\n",
    "| Use Case | Tensor Split | GPU 0 | GPU 1 |\n",
    "|----------|--------------|-------|-------|\n",
    "| Max Model Size | 0.5, 0.5 | LLM (50%) | LLM (50%) |\n",
    "| LLM + Light Task | 0.7, 0.3 | LLM (70%) | LLM (30%) |\n",
    "| LLM + RAPIDS | 1.0, 0.0 | LLM (100%) | RAPIDS (100%) |\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [04-gguf-quantization](04-gguf-quantization-llamatelemetry-v0.1.0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
