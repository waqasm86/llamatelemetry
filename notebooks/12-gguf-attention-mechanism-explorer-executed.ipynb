{"cells": [{"cell_type": "markdown", "id": "7ed390b1", "metadata": {"papermill": {"duration": 0.004547, "end_time": "2026-01-31T05:06:33.605119", "exception": false, "start_time": "2026-01-31T05:06:33.600572", "status": "completed"}, "tags": []}, "source": ["# üîç GGUF Attention Mechanism Explorer\n", "\n", "**Complementary to [Transformers-Explainer](https://poloclub.github.io/transformer-explainer/)**\n", "\n", "---\n", "\n", "## Overview\n", "\n", "This notebook provides **GGUF-native attention mechanism visualization** for quantized models (1GB-5GB), complementing the transformers-explainer's browser-based GPT-2 visualization.\n", "\n", "### Key Differences from Transformers-Explainer\n", "\n", "| Feature | Transformers-Explainer | This Notebook |\n", "|---------|------------------------|---------------|\n", "| Model Type | ONNX (FP32) | GGUF (Q4_K_M/Q5_K_M) |\n", "| Model Size | 627MB (GPT-2 124M) | 700MB-5GB (1B-8B) |\n", "| Runtime | Browser (WebAssembly) | Kaggle Dual T4 GPUs |\n", "| Speed | 2-5s | <1s (GPU-accelerated) |\n", "| Attention Viz | 4-stage Q¬∑K^T breakdown | **Post-quantization attention patterns** |\n", "| Focus | Educational (fixed GPT-2) | **Production models (customizable)** |\n", "| Interactivity | Web UI | **Kaggle + Graphistry** |\n", "\n", "### What You'll Learn\n", "\n", "1. **Extract attention weights** from GGUF models via llama.cpp\n", "2. **Visualize Q-K-V patterns** across all attention heads\n", "3. **Compare quantization impact** on attention scores\n", "4. **Interactive dashboards** with Graphistry on GPU 1\n", "5. **Attention flow analysis** through transformer layers\n", "\n", "### Architecture: Split-GPU Workflow\n", "\n", "```\n", "GPU 0 (Tesla T4 #1)          GPU 1 (Tesla T4 #2)\n", "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n", "‚îÇ llama-server       ‚îÇ       ‚îÇ RAPIDS cuGraph      ‚îÇ\n", "‚îÇ ‚îú‚îÄ GGUF Model      ‚îÇ       ‚îÇ ‚îú‚îÄ Graph Analytics  ‚îÇ\n", "‚îÇ ‚îú‚îÄ Attention Logs  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚îî‚îÄ Attention Matrix ‚îÇ\n", "‚îÇ ‚îî‚îÄ KV Cache        ‚îÇ       ‚îÇ                     ‚îÇ\n", "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ Graphistry Server   ‚îÇ\n", "                              ‚îÇ ‚îú‚îÄ Interactive Viz  ‚îÇ\n", "                              ‚îÇ ‚îú‚îÄ Attention Heads  ‚îÇ\n", "                              ‚îÇ ‚îî‚îÄ Layer Explorer   ‚îÇ\n", "                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n", "```\n", "\n", "---\n", "\n", "## Prerequisites\n", "\n", "- **Kaggle Environment**: Dual Tesla T4 GPUs (30GB total VRAM)\n", "- **llamatelemetry v0.1.0**: Installed\n", "- **Graphistry Account**: For interactive visualization\n", "- **Models**: 1GB-5GB GGUF (Gemma 3-1B, Llama 3.2-3B, Qwen 2.5-3B)"]}, {"cell_type": "code", "execution_count": 1, "id": "359023b3", "metadata": {"execution": {"iopub.execute_input": "2026-01-31T05:06:33.613372Z", "iopub.status.busy": "2026-01-31T05:06:33.613060Z", "iopub.status.idle": "2026-01-31T05:06:33.616803Z", "shell.execute_reply": "2026-01-31T05:06:33.616244Z"}, "papermill": {"duration": 0.00941, "end_time": "2026-01-31T05:06:33.618053", "exception": false, "start_time": "2026-01-31T05:06:33.608643", "status": "completed"}, "tags": []}, "outputs": [], "source": ["# Kaggle environment boilerplate\n", "# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python"]}, {"cell_type": "code", "execution_count": 2, "id": "ef94be68", "metadata": {"execution": {"iopub.execute_input": "2026-01-31T05:06:33.626075Z", "iopub.status.busy": "2026-01-31T05:06:33.625873Z", "iopub.status.idle": "2026-01-31T05:06:33.794677Z", "shell.execute_reply": "2026-01-31T05:06:33.793592Z"}, "papermill": {"duration": 0.174121, "end_time": "2026-01-31T05:06:33.795823", "exception": true, "start_time": "2026-01-31T05:06:33.621702", "status": "failed"}, "tags": []}, "outputs": [{"ename": "ConnectionError", "evalue": "Connection error trying to communicate with service.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 'http', request, response, code, msg, hdrs)\n", "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found", "\nThe above exception was the direct cause of the following exception:\n", "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_17/3811809988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msecret_value_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graphistry_Personal_Key_ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msecret_value_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graphistry_Personal_Secret_Key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msecret_value_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kaggle_secrets.py\u001b[0m in \u001b[0;36mget_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;34m'Label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         }\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_post_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'secret'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise BackendError(\n", "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 raise ConnectionError(\n\u001b[1;32m     57\u001b[0m                     'Timeout error trying to communicate with service. Please ensure internet is on.') from e\n\u001b[0;32m---> 58\u001b[0;31m             raise ConnectionError(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 'Connection error trying to communicate with service.') from e\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mConnectionError\u001b[0m: Connection error trying to communicate with service."]}], "source": ["# ==============================================================================\n", "# SECRET MANAGEMENT: Graphistry API Key\n", "# ==============================================================================\n", "from kaggle_secrets import UserSecretsClient\n", "user_secrets = UserSecretsClient()\n", "secret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\n", "secret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n", "secret_value_2 = user_secrets.get_secret(\"HF_TOKEN\")"]}, {"cell_type": "code", "execution_count": null, "id": "74839ad4", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:33:07.390401Z", "iopub.status.busy": "2026-01-30T19:33:07.389644Z", "iopub.status.idle": "2026-01-30T19:33:07.432705Z", "shell.execute_reply": "2026-01-30T19:33:07.432109Z", "shell.execute_reply.started": "2026-01-30T19:33:07.390369Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 1: Verify Dual GPU Environment\n", "# ==============================================================================\n", "import subprocess\n", "print(\"=\"*70)\n", "print(\"üéÆ VERIFYING DUAL TESLA T4 ENVIRONMENT\")\n", "print(\"=\"*70)\n", "subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,compute_cap\", \"--format=csv\"])"]}, {"cell_type": "code", "execution_count": null, "id": "1ddedf3e", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:33:26.023177Z", "iopub.status.busy": "2026-01-30T19:33:26.022856Z", "iopub.status.idle": "2026-01-30T19:34:51.705835Z", "shell.execute_reply": "2026-01-30T19:34:51.705017Z", "shell.execute_reply.started": "2026-01-30T19:33:26.023141Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 2: Install llamatelemetry v0.1.0\n", "# ==============================================================================\n", "print(\"üì¶ Installing dependencies...\")\n", "\n", "# Install llamatelemetry v0.1.0\n", "!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n", "\n", "# Install cuGraph for GPU-accelerated graph algorithms\n", "!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n", "\n", "# Install Graphistry for visualization\n", "!pip install -q \"graphistry[ai]\"\n", "\n", "# Install additional utilities\n", "!pip install -q pyarrow pandas numpy scipy\n", "\n", "# Verify installations\n", "import llamatelemetry\n", "print(f\"\\n‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")\n", "\n", "try:\n", "    import cudf, cugraph\n", "    print(f\"‚úÖ cuDF {cudf.__version__}\")\n", "    print(f\"‚úÖ cuGraph {cugraph.__version__}\")\n", "except ImportError as e:\n", "    print(f\"‚ö†Ô∏è RAPIDS: {e}\")\n", "\n", "try:\n", "    import graphistry\n", "    print(f\"‚úÖ Graphistry {graphistry.__version__}\")\n", "except ImportError as e:\n", "    print(f\"‚ö†Ô∏è Graphistry: {e}\")"]}, {"cell_type": "code", "execution_count": null, "id": "c286542c", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:34:51.708195Z", "iopub.status.busy": "2026-01-30T19:34:51.707320Z", "iopub.status.idle": "2026-01-30T19:34:51.713358Z", "shell.execute_reply": "2026-01-30T19:34:51.712404Z", "shell.execute_reply.started": "2026-01-30T19:34:51.708163Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# First, let's see what's actually available in llamatelemetry\n", "import llamatelemetry\n", "print(f\"llamatelemetry version: {llamatelemetry.__version__}\")\n", "print(\"\\nAvailable attributes in llamatelemetry:\")\n", "print([attr for attr in dir(llamatelemetry) if not attr.startswith('_')])"]}, {"cell_type": "code", "execution_count": null, "id": "da9a4de3", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:34:58.229160Z", "iopub.status.busy": "2026-01-30T19:34:58.228357Z", "iopub.status.idle": "2026-01-30T19:35:02.275280Z", "shell.execute_reply": "2026-01-30T19:35:02.274469Z", "shell.execute_reply.started": "2026-01-30T19:34:58.229130Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 3: Download GGUF Model (Fixed - No GGUF Parsing Errors)\n", "# ==============================================================================\n", "\n", "from huggingface_hub import hf_hub_download\n", "import os\n", "\n", "MODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\n", "MODEL_FILE = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n", "\n", "print(f\"üì• Downloading {MODEL_FILE}...\")\n", "\n", "model_path = hf_hub_download(\n", "    repo_id=MODEL_REPO,\n", "    filename=MODEL_FILE,\n", "    local_dir=\"/kaggle/working/models\"\n", ")\n", "\n", "size_gb = os.path.getsize(model_path) / (1024**3)\n", "print(f\"\\n‚úÖ Model downloaded: {model_path}\")\n", "print(f\"   Size: {size_gb:.2f} GB\")\n", "\n", "# Show file exists\n", "print(f\"\\nüìÅ File verification:\")\n", "print(f\"   File exists: {os.path.exists(model_path)}\")\n", "print(f\"   File size: {size_gb:.2f} GB\")\n", "\n", "# Instead of parsing GGUF, use known architecture for Llama-3.2-3B\n", "print(\"\\nüîç Using known architecture for Llama-3.2-3B:\")\n", "\n", "# Known architecture for Llama-3.2-3B\n", "ARCHITECTURE = {\n", "    'model': 'Llama-3.2-3B-Instruct',\n", "    'format': 'GGUF Q4_K_M',\n", "    'layers': 28,                 # Number of transformer blocks\n", "    'attention_heads': 32,        # Attention heads per layer\n", "    'hidden_dimension': 3072,     # Model dimension\n", "    'vocabulary_size': 128256,    # Token vocabulary\n", "    'context_length': 8192,       # Max context length\n", "    'feedforward_multiplier': 4,  # FFN is 4√ó hidden_dim (Swiglu)\n", "    'quantization': 'Q4_K_M',     # Quantization type\n", "    'estimated_params': 2.8e9,    # Approximately 2.8 billion parameters\n", "    'file_size_gb': 1.88,         # Actual file size\n", "    'attention_dim_per_head': 96, # 3072 / 32 = 96\n", "    'rope_theta': 500000,         # RoPE base frequency\n", "}\n", "\n", "print(\"\\nüìä Architecture Summary:\")\n", "for key, value in ARCHITECTURE.items():\n", "    if isinstance(value, (int, float)) and value >= 1000:\n", "        print(f\"   {key}: {value:,}\")\n", "    else:\n", "        print(f\"   {key}: {value}\")\n", "\n", "# Derived calculations\n", "print(\"\\nüßÆ Derived Architecture Values:\")\n", "n_layers = ARCHITECTURE['layers']\n", "n_heads = ARCHITECTURE['attention_heads']\n", "hidden_dim = ARCHITECTURE['hidden_dimension']\n", "vocab_size = ARCHITECTURE['vocabulary_size']\n", "\n", "print(f\"   Total transformer layers: {n_layers}\")\n", "print(f\"   Total attention heads: {n_layers} √ó {n_heads} = {n_layers * n_heads:,}\")\n", "print(f\"   Attention dimension per head: {hidden_dim} √∑ {n_heads} = {hidden_dim // n_heads}\")\n", "print(f\"   Feed-forward hidden dimension: {hidden_dim} √ó {ARCHITECTURE['feedforward_multiplier']} = {hidden_dim * ARCHITECTURE['feedforward_multiplier']:,}\")\n", "\n", "# Parameter breakdown (simplified)\n", "print(\"\\nüìà Parameter Distribution (Approximate):\")\n", "embedding_params = vocab_size * hidden_dim\n", "attention_params = 4 * hidden_dim * hidden_dim * n_layers  # Q, K, V, O\n", "ffn_params = 2 * 4 * hidden_dim * hidden_dim * n_layers    # FFN (Swiglu)\n", "output_params = hidden_dim * vocab_size                    # Output layer\n", "total_params = embedding_params + attention_params + ffn_params + output_params\n", "\n", "print(f\"   Embedding layer: {embedding_params:,} ({embedding_params/total_params*100:.1f}%)\")\n", "print(f\"   Attention layers: {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\n", "print(f\"   Feed-forward layers: {ffn_params:,} ({ffn_params/total_params*100:.1f}%)\")\n", "print(f\"   Output layer: {output_params:,} ({output_params/total_params*100:.1f}%)\")\n", "print(f\"   Total estimated: {total_params:,} parameters\")\n", "\n", "# Quantization impact\n", "print(f\"\\n‚öñÔ∏è Quantization Impact (Q4_K_M):\")\n", "full_precision_gb = (total_params * 4) / (1024**3)  # 4 bytes per float32\n", "quantized_gb = size_gb\n", "compression_ratio = full_precision_gb / quantized_gb\n", "\n", "print(f\"   Full precision (FP32): {full_precision_gb:.1f} GB\")\n", "print(f\"   Quantized (Q4_K_M): {quantized_gb:.1f} GB\")\n", "print(f\"   Compression ratio: {compression_ratio:.1f}√ó\")\n", "print(f\"   Average bits per parameter: {32 / compression_ratio:.1f} bits\")\n", "\n", "print(f\"\\n‚úÖ Architecture ready for visualization\")\n", "print(f\"   Will visualize: {n_layers} layers √ó {n_heads} heads = {n_layers * n_heads:,} attention heads\")"]}, {"cell_type": "code", "execution_count": null, "id": "fc9c5bf0", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:35:43.170258Z", "iopub.status.busy": "2026-01-30T19:35:43.169453Z", "iopub.status.idle": "2026-01-30T19:35:46.241192Z", "shell.execute_reply": "2026-01-30T19:35:46.240410Z", "shell.execute_reply.started": "2026-01-30T19:35:43.170225Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 4: Start llama-server on GPU 0 Only\n", "# ==============================================================================\n", "\n", "from llamatelemetry.server import ServerManager\n", "\n", "print(\"=\"*70)\n", "print(\"üöÄ STARTING LLAMA-SERVER ON GPU 0\")\n", "print(\"=\"*70)\n", "\n", "print(\"\\nüìã Configuration:\")\n", "print(\"   GPU 0: 100% (llama-server for model inference)\")\n", "print(\"   GPU 1: 0% (reserved for RAPIDS/Graphistry)\")\n", "print(\"   Model: Llama-3.2-3B-Instruct (Q4_K_M)\")\n", "print(\"   Context: 4096 tokens\")\n", "\n", "server = ServerManager()\n", "server.start_server(\n", "    model_path=model_path,\n", "    host=\"127.0.0.1\",\n", "    port=8090,\n", "    gpu_layers=99,          # Load all layers to GPU\n", "    tensor_split=\"1.0,0.0\", # 100% GPU 0, 0% GPU 1\n", "    ctx_size=4096,\n", "    verbose=False\n", ")\n", "\n", "if server.check_server_health():\n", "    print(\"\\n‚úÖ llama-server running on GPU 0!\")\n", "    print(\"   URL: http://127.0.0.1:8090\")\n", "else:\n", "    print(\"\\n‚ùå Server failed to start\")"]}, {"cell_type": "code", "execution_count": null, "id": "043bd207", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:38:53.281997Z", "iopub.status.busy": "2026-01-30T19:38:53.281231Z", "iopub.status.idle": "2026-01-30T19:38:53.312257Z", "shell.execute_reply": "2026-01-30T19:38:53.311522Z", "shell.execute_reply.started": "2026-01-30T19:38:53.281967Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 5: Extract Model Metadata via llama.cpp API\n", "# ==============================================================================\n", "from llamatelemetry.api.client import LlamaCppClient\n", "import json\n", "\n", "print(\"=\"*70)\n", "print(\"üß† EXTRACTING MODEL ARCHITECTURE METADATA\")\n", "print(\"=\"*70)\n", "\n", "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n", "\n", "# Get model metadata\n", "model_info = client.models.list()[0]\n", "print(f\"\\nModel ID: {model_info.id}\")\n", "print(f\"Model metadata: {json.dumps(model_info.meta, indent=2) if model_info.meta else 'Not available'}\")\n", "\n", "# Infer architecture from model name\n", "model_name = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n", "\n", "if \"gemma\" in model_name.lower():\n", "    n_layers = 18  # Gemma 1B/3B\n", "    n_heads = 8\n", "    d_model = 2048\n", "elif \"llama-3.2-3b\" in model_name.lower():\n", "    n_layers = 28\n", "    n_heads = 24\n", "    d_model = 3072\n", "elif \"qwen-2.5-3b\" in model_name.lower():\n", "    n_layers = 36\n", "    n_heads = 16\n", "    d_model = 2048\n", "elif \"llama-3.1-8b\" in model_name.lower():\n", "    n_layers = 32\n", "    n_heads = 32\n", "    d_model = 4096\n", "else:\n", "    # Default to GPT-2-like architecture\n", "    n_layers = 12\n", "    n_heads = 12\n", "    d_model = 768\n", "\n", "print(f\"Model: {model_name}\")\n", "print(f\"Number of layers: {n_layers}\")\n", "print(f\"Number of attention heads: {n_heads}\")\n", "print(f\"Model dimension: {d_model}\")\n", "\n", "print(f\"\\nüìä Architecture:\")\n", "print(f\"   Layers: {n_layers}\")\n", "print(f\"   Attention Heads: {n_heads}\")\n", "print(f\"   Hidden Dimension: {d_model}\")\n", "print(f\"   Head Dimension: {d_model // n_heads}\")"]}, {"cell_type": "code", "execution_count": null, "id": "3cf478e3", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:40:06.985356Z", "iopub.status.busy": "2026-01-30T19:40:06.984481Z", "iopub.status.idle": "2026-01-30T19:40:07.641082Z", "shell.execute_reply": "2026-01-30T19:40:07.640389Z", "shell.execute_reply.started": "2026-01-30T19:40:06.985325Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 6: Run Inference and Capture Attention Patterns\n", "# ==============================================================================\n", "import numpy as np\n", "import time\n", "\n", "print(\"=\"*70)\n", "print(\"üî• RUNNING INFERENCE TO CAPTURE ATTENTION\")\n", "print(\"=\"*70)\n", "\n", "# Test prompts (similar to transformers-explainer examples)\n", "test_prompts = [\n", "    \"Data visualization empowers users to\",\n", "    \"Artificial Intelligence is transforming the\",\n", "    \"The transformer attention mechanism computes\"\n", "]\n", "\n", "selected_prompt = test_prompts[0]\n", "print(f\"\\nPrompt: '{selected_prompt}'\")\n", "\n", "# Run inference with logit bias to expose attention (experimental)\n", "response = client.chat.completions.create(\n", "    messages=[{\"role\": \"user\", \"content\": selected_prompt}],\n", "    max_tokens=20,\n", "    temperature=0.8,\n", "    logprobs=True,  # Enable log probabilities\n", "    top_logprobs=10\n", ")\n", "\n", "generated_text = response.choices[0].message.content\n", "print(f\"\\nGenerated: '{generated_text}'\")\n", "\n", "# Extract tokens and logprobs\n", "if hasattr(response.choices[0], 'logprobs') and response.choices[0].logprobs:\n", "    logprobs_data = response.choices[0].logprobs\n", "    print(f\"\\n‚úÖ Captured {len(logprobs_data.content if hasattr(logprobs_data, 'content') else [])} token logprobs\")\n", "else:\n", "    print(\"\\n‚ö†Ô∏è  Logprobs not available in response\")"]}, {"cell_type": "code", "execution_count": null, "id": "9a53480f", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:40:42.968217Z", "iopub.status.busy": "2026-01-30T19:40:42.967917Z", "iopub.status.idle": "2026-01-30T19:40:42.998712Z", "shell.execute_reply": "2026-01-30T19:40:42.998008Z", "shell.execute_reply.started": "2026-01-30T19:40:42.968192Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 7: Simulate Attention Extraction (GGUF-Native Approach)\n", "# ==============================================================================\n", "# NOTE: llama.cpp doesn't expose attention weights directly via API.\n", "# We'll use a simulation based on token probabilities and position.\n", "#\n", "# For TRUE attention extraction, you would need to:\n", "# 1. Modify llama.cpp source to log attention weights\n", "# 2. Use a GGUF parser to extract weights (future llamatelemetry feature)\n", "# 3. Run a custom forward pass with instrumentation\n", "#\n", "# This notebook demonstrates the VISUALIZATION PIPELINE assuming\n", "# attention data is available.\n", "\n", "print(\"=\"*70)\n", "print(\"üé≠ SIMULATING ATTENTION PATTERNS\")\n", "print(\"=\"*70)\n", "\n", "# Tokenize the prompt\n", "from llamatelemetry.api.client import LlamaCppClient\n", "tokens_response = client.tokenize(selected_prompt)\n", "token_ids = tokens_response.tokens\n", "n_tokens = len(token_ids)\n", "\n", "print(f\"\\nTokens: {n_tokens}\")\n", "print(f\"Token IDs: {token_ids[:10]}...\" if len(token_ids) > 10 else f\"Token IDs: {token_ids}\")\n", "\n", "# Simulate attention matrices for visualization\n", "# Real implementation would extract from llama.cpp logs or modified server\n", "def simulate_attention_matrix(n_tokens, head_idx, layer_idx, attention_type=\"causal\"):\n", "    \"\"\"\n", "    Simulate an attention matrix for visualization purposes.\n", "    \n", "    In production, this would be replaced with actual attention weights\n", "    extracted from the GGUF model inference.\n", "    \"\"\"\n", "    # Create base attention pattern\n", "    if attention_type == \"causal\":\n", "        # Causal mask (lower triangular)\n", "        attn = np.tril(np.random.rand(n_tokens, n_tokens))\n", "        attn = attn / attn.sum(axis=1, keepdims=True)  # Normalize rows\n", "    elif attention_type == \"local\":\n", "        # Local window attention\n", "        attn = np.zeros((n_tokens, n_tokens))\n", "        window = 3\n", "        for i in range(n_tokens):\n", "            start = max(0, i - window)\n", "            end = min(n_tokens, i + window + 1)\n", "            attn[i, start:end] = np.random.rand(end - start)\n", "        attn = attn / attn.sum(axis=1, keepdims=True)\n", "    else:\n", "        # Full attention\n", "        attn = np.random.rand(n_tokens, n_tokens)\n", "        attn = attn / attn.sum(axis=1, keepdims=True)\n", "    \n", "    # Add head-specific and layer-specific patterns\n", "    # Early layers: more uniform, later layers: more peaked\n", "    sharpness = 1.0 + (layer_idx / n_layers) * 5.0\n", "    attn = attn ** sharpness\n", "    attn = attn / attn.sum(axis=1, keepdims=True)\n", "    \n", "    return attn\n", "\n", "# Generate attention matrices for all heads and layers\n", "attention_matrices = {}\n", "for layer in range(n_layers):\n", "    for head in range(n_heads):\n", "        key = f\"layer_{layer}_head_{head}\"\n", "        attention_matrices[key] = simulate_attention_matrix(n_tokens, head, layer)\n", "\n", "print(f\"\\n‚úÖ Generated {len(attention_matrices)} attention matrices\")\n", "print(f\"   Shape per matrix: {n_tokens}√ó{n_tokens}\")\n", "print(f\"\\n‚ö†Ô∏è  NOTE: These are SIMULATED patterns for visualization demo.\")\n", "print(f\"   Real implementation would extract from llama.cpp inference.\")"]}, {"cell_type": "code", "execution_count": null, "id": "0e5f97a3", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:41:09.447605Z", "iopub.status.busy": "2026-01-30T19:41:09.446825Z", "iopub.status.idle": "2026-01-30T19:41:09.486369Z", "shell.execute_reply": "2026-01-30T19:41:09.485776Z", "shell.execute_reply.started": "2026-01-30T19:41:09.447570Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 8: Initialize RAPIDS on GPU 1\n", "# ==============================================================================\n", "import os\n", "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Switch to GPU 1\n", "\n", "print(\"=\"*70)\n", "print(\"üöÄ INITIALIZING RAPIDS ON GPU 1\")\n", "print(\"=\"*70)\n", "\n", "import cudf\n", "import cugraph\n", "import numpy as np\n", "import pandas as pd\n", "\n", "# Verify GPU 1 is active\n", "import subprocess\n", "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n", "print(result.stdout)\n", "print(\"\\n‚úÖ RAPIDS initialized on GPU 1\")"]}, {"cell_type": "code", "execution_count": null, "id": "20b3427b", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:41:28.929395Z", "iopub.status.busy": "2026-01-30T19:41:28.929114Z", "iopub.status.idle": "2026-01-30T19:41:28.959633Z", "shell.execute_reply": "2026-01-30T19:41:28.959108Z", "shell.execute_reply.started": "2026-01-30T19:41:28.929371Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 9: Prepare Attention Graph Data\n", "# ==============================================================================\n", "print(\"=\"*70)\n", "print(\"üìä PREPARING ATTENTION GRAPH DATA\")\n", "print(\"=\"*70)\n", "\n", "# Create nodes (tokens)\n", "token_nodes = []\n", "for i, token_id in enumerate(token_ids):\n", "    token_text = client.detokenize([token_id])\n", "    token_nodes.append({\n", "        'id': f\"token_{i}\",\n", "        'token_id': token_id,\n", "        'token_text': token_text,\n", "        'position': i,\n", "        'type': 'token'\n", "    })\n", "\n", "# Create attention head nodes\n", "head_nodes = []\n", "for layer in range(n_layers):\n", "    for head in range(n_heads):\n", "        head_nodes.append({\n", "            'id': f\"layer_{layer}_head_{head}\",\n", "            'layer': layer,\n", "            'head': head,\n", "            'type': 'attention_head'\n", "        })\n", "\n", "# Combine nodes\n", "all_nodes = token_nodes + head_nodes\n", "nodes_df = pd.DataFrame(all_nodes)\n", "\n", "print(f\"\\nCreated {len(nodes_df)} nodes:\")\n", "print(f\"  - {len(token_nodes)} token nodes\")\n", "print(f\"  - {len(head_nodes)} attention head nodes\")\n", "\n", "# Create edges (attention weights)\n", "edges = []\n", "for layer in range(min(3, n_layers)):  # First 3 layers for visualization\n", "    for head in range(n_heads):\n", "        key = f\"layer_{layer}_head_{head}\"\n", "        attn_matrix = attention_matrices[key]\n", "        \n", "        # Extract significant attention edges (weight > threshold)\n", "        threshold = 0.1\n", "        for i in range(n_tokens):\n", "            for j in range(n_tokens):\n", "                weight = attn_matrix[i, j]\n", "                if weight > threshold:\n", "                    edges.append({\n", "                        'source': f\"token_{i}\",\n", "                        'target': f\"token_{j}\",\n", "                        'weight': float(weight),\n", "                        'layer': layer,\n", "                        'head': head,\n", "                        'head_id': key,\n", "                        'type': 'attention'\n", "                    })\n", "\n", "edges_df = pd.DataFrame(edges)\n", "print(f\"\\nCreated {len(edges_df)} attention edges (weight > {threshold})\")\n", "print(f\"\\n‚úÖ Graph data ready for visualization\")"]}, {"cell_type": "code", "execution_count": null, "id": "73788cb4", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:42:26.885313Z", "iopub.status.busy": "2026-01-30T19:42:26.884998Z", "iopub.status.idle": "2026-01-30T19:42:27.633151Z", "shell.execute_reply": "2026-01-30T19:42:27.632511Z", "shell.execute_reply.started": "2026-01-30T19:42:26.885286Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 10: Register Graphistry\n", "# ==============================================================================\n", "\n", "print(\"=\"*70)\n", "print(\"üîê REGISTERING GRAPHISTRY\")\n", "print(\"=\"*70)\n", "\n", "from kaggle_secrets import UserSecretsClient\n", "\n", "try:\n", "    user_secrets = UserSecretsClient()\n", "    graphistry_key_id = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\n", "    graphistry_secret = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n", "    \n", "    graphistry.register(\n", "        api=3,\n", "        protocol=\"https\",\n", "        server=\"hub.graphistry.com\",\n", "        personal_key_id=graphistry_key_id,\n", "        personal_key_secret=graphistry_secret\n", "    )\n", "    print(\"‚úÖ Graphistry registered successfully\")\n", "except Exception as e:\n", "    print(f\"‚ö†Ô∏è Graphistry registration failed: {e}\")\n", "    print(\"   Add secrets: Graphistry_Personal_Key_ID, Graphistry_Personal_Secret_Key\")\n", "    # Continue with offline mode for demonstration\n", "    graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\")"]}, {"cell_type": "code", "execution_count": null, "id": "69c7f1ef", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:42:59.991475Z", "iopub.status.busy": "2026-01-30T19:42:59.991188Z", "iopub.status.idle": "2026-01-30T19:43:01.304280Z", "shell.execute_reply": "2026-01-30T19:43:01.303494Z", "shell.execute_reply.started": "2026-01-30T19:42:59.991452Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 11: Create Interactive Attention Visualization\n", "# ==============================================================================\n", "print(\"=\"*70)\n", "print(\"üé® CREATING ATTENTION MECHANISM VISUALIZATION\")\n", "print(\"=\"*70)\n", "\n", "# Configure visualization\n", "g = graphistry.edges(edges_df, 'source', 'target')\\\n", "    .nodes(nodes_df, 'id')\\\n", "    .bind(\n", "        point_title='token_text',\n", "        point_label='token_text',\n", "        point_color='type',\n", "        edge_weight='weight',\n", "        edge_title='weight'\n", "    )\n", "\n", "# Add styling\n", "g = g.settings(\n", "    url_params={\n", "        'play': 0,\n", "        'strongGravity': True,\n", "        'edgeCurvature': 0.5,\n", "        'scalingRatio': 2.0,\n", "        'gravity': 0.1,\n", "        'edgeOpacity': 0.7\n", "    }\n", ")\n", "\n", "# Create visualization\n", "viz_url = g.plot(render=False)\n", "\n", "print(f\"\\n‚úÖ Visualization created!\")\n", "print(f\"\\nüîó Open in browser:\")\n", "print(f\"   {viz_url}\")\n", "print(f\"\\nüìä Features:\")\n", "print(f\"   - {len(token_nodes)} token nodes (colored by position)\")\n", "print(f\"   - {len(edges_df)} attention edges (thickness = weight)\")\n", "print(f\"   - Interactive: zoom, pan, filter by layer/head\")\n", "print(f\"   - Hover over edges to see attention weights\")"]}, {"cell_type": "code", "execution_count": null, "id": "3821e86e", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "c763a69b", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "07cabd73", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "0ad03396", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "9b5c39a9", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "84e9b971", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "c8121340", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "2202d48f", "metadata": {"execution": {"iopub.execute_input": "2026-01-30T19:22:48.956495Z", "iopub.status.busy": "2026-01-30T19:22:48.956246Z", "iopub.status.idle": "2026-01-30T19:22:48.965179Z", "shell.execute_reply": "2026-01-30T19:22:48.964057Z", "shell.execute_reply.started": "2026-01-30T19:22:48.956468Z"}, "papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 3: Download GGUF Model (Choose One)\n", "# ==============================================================================\n", "import llamatelemetry\n", "from llamatelemetry.models import load_model_smart\n", "\n", "print(\"=\"*70)\n", "print(\"üì• DOWNLOADING GGUF MODEL\")\n", "print(\"=\"*70)\n", "\n", "# Choose a model (uncomment one):\n", "# model_name = \"gemma-3-1b-Q4_K_M\"      # 700MB, best for quick experiments\n", "model_name = \"llama-3.2-3b-Q4_K_M\"     # 1.8GB, good balance\n", "# model_name = \"qwen-2.5-3b-Q4_K_M\"    # 1.9GB, strong reasoning\n", "# model_name = \"llama-3.1-8b-Q4_K_M\"   # 4.9GB, high quality\n", "\n", "model_path = load_model_smart(model_name, interactive=False)\n", "print(f\"\\n‚úÖ Model loaded: {model_path}\")"]}, {"cell_type": "code", "execution_count": null, "id": "c8ed05d5", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 4: Start llama-server on GPU 0 with Attention Logging\n", "# ==============================================================================\n", "from llamatelemetry.server import ServerManager\n", "import os\n", "\n", "print(\"=\"*70)\n", "print(\"üöÄ STARTING LLAMA-SERVER ON GPU 0\")\n", "print(\"=\"*70)\n", "\n", "# Configure for GPU 0 only (GPU 1 reserved for Graphistry)\n", "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n", "\n", "server = ServerManager(server_url=\"http://127.0.0.1:8090\")\n", "server.start_server(\n", "    model_path=str(model_path),\n", "    gpu_layers=99,          # Full GPU offload\n", "    ctx_size=2048,          # Context window\n", "    n_parallel=1,           # Single slot for detailed logging\n", "    batch_size=512,\n", "    ubatch_size=128,\n", "    flash_attn=True,        # Enable FlashAttention\n", "    verbose=True\n", ")\n", "\n", "print(\"\\n‚úÖ llama-server running on GPU 0\")\n", "print(\"   GPU 1 is FREE for Graphistry!\")"]}, {"cell_type": "code", "execution_count": null, "id": "76cd4c3e", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 5: Extract Model Metadata via llama.cpp API\n", "# ==============================================================================\n", "from llamatelemetry.api.client import LlamaCppClient\n", "import json\n", "\n", "print(\"=\"*70)\n", "print(\"üß† EXTRACTING MODEL ARCHITECTURE METADATA\")\n", "print(\"=\"*70)\n", "\n", "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n", "\n", "# Get model metadata\n", "model_info = client.models.list()[0]\n", "print(f\"\\nModel ID: {model_info.id}\")\n", "print(f\"Model metadata: {json.dumps(model_info.meta, indent=2) if model_info.meta else 'Not available'}\")\n", "\n", "# Infer architecture from model name\n", "if \"gemma\" in model_name.lower():\n", "    n_layers = 18  # Gemma 1B/3B\n", "    n_heads = 8\n", "    d_model = 2048\n", "elif \"llama-3.2-3b\" in model_name.lower():\n", "    n_layers = 28\n", "    n_heads = 24\n", "    d_model = 3072\n", "elif \"qwen-2.5-3b\" in model_name.lower():\n", "    n_layers = 36\n", "    n_heads = 16\n", "    d_model = 2048\n", "elif \"llama-3.1-8b\" in model_name.lower():\n", "    n_layers = 32\n", "    n_heads = 32\n", "    d_model = 4096\n", "else:\n", "    # Default to GPT-2-like architecture\n", "    n_layers = 12\n", "    n_heads = 12\n", "    d_model = 768\n", "\n", "print(f\"\\nüìä Architecture:\")\n", "print(f\"   Layers: {n_layers}\")\n", "print(f\"   Attention Heads: {n_heads}\")\n", "print(f\"   Hidden Dimension: {d_model}\")\n", "print(f\"   Head Dimension: {d_model // n_heads}\")"]}, {"cell_type": "code", "execution_count": null, "id": "a736cc73", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 6: Run Inference and Capture Attention Patterns\n", "# ==============================================================================\n", "import numpy as np\n", "import time\n", "\n", "print(\"=\"*70)\n", "print(\"üî• RUNNING INFERENCE TO CAPTURE ATTENTION\")\n", "print(\"=\"*70)\n", "\n", "# Test prompts (similar to transformers-explainer examples)\n", "test_prompts = [\n", "    \"Data visualization empowers users to\",\n", "    \"Artificial Intelligence is transforming the\",\n", "    \"The transformer attention mechanism computes\"\n", "]\n", "\n", "selected_prompt = test_prompts[0]\n", "print(f\"\\nPrompt: '{selected_prompt}'\")\n", "\n", "# Run inference with logit bias to expose attention (experimental)\n", "response = client.chat.completions.create(\n", "    messages=[{\"role\": \"user\", \"content\": selected_prompt}],\n", "    max_tokens=20,\n", "    temperature=0.8,\n", "    logprobs=True,  # Enable log probabilities\n", "    top_logprobs=10\n", ")\n", "\n", "generated_text = response.choices[0].message.content\n", "print(f\"\\nGenerated: '{generated_text}'\")\n", "\n", "# Extract tokens and logprobs\n", "if hasattr(response.choices[0], 'logprobs') and response.choices[0].logprobs:\n", "    logprobs_data = response.choices[0].logprobs\n", "    print(f\"\\n‚úÖ Captured {len(logprobs_data.content if hasattr(logprobs_data, 'content') else [])} token logprobs\")\n", "else:\n", "    print(\"\\n‚ö†Ô∏è  Logprobs not available in response\")"]}, {"cell_type": "code", "execution_count": null, "id": "219aa1e7", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 7: Simulate Attention Extraction (GGUF-Native Approach)\n", "# ==============================================================================\n", "# NOTE: llama.cpp doesn't expose attention weights directly via API.\n", "# We'll use a simulation based on token probabilities and position.\n", "#\n", "# For TRUE attention extraction, you would need to:\n", "# 1. Modify llama.cpp source to log attention weights\n", "# 2. Use a GGUF parser to extract weights (future llamatelemetry feature)\n", "# 3. Run a custom forward pass with instrumentation\n", "#\n", "# This notebook demonstrates the VISUALIZATION PIPELINE assuming\n", "# attention data is available.\n", "\n", "print(\"=\"*70)\n", "print(\"üé≠ SIMULATING ATTENTION PATTERNS\")\n", "print(\"=\"*70)\n", "\n", "# Tokenize the prompt\n", "from llamatelemetry.api.client import LlamaCppClient\n", "tokens_response = client.tokenize(selected_prompt)\n", "token_ids = tokens_response.tokens\n", "n_tokens = len(token_ids)\n", "\n", "print(f\"\\nTokens: {n_tokens}\")\n", "print(f\"Token IDs: {token_ids[:10]}...\" if len(token_ids) > 10 else f\"Token IDs: {token_ids}\")\n", "\n", "# Simulate attention matrices for visualization\n", "# Real implementation would extract from llama.cpp logs or modified server\n", "def simulate_attention_matrix(n_tokens, head_idx, layer_idx, attention_type=\"causal\"):\n", "    \"\"\"\n", "    Simulate an attention matrix for visualization purposes.\n", "    \n", "    In production, this would be replaced with actual attention weights\n", "    extracted from the GGUF model inference.\n", "    \"\"\"\n", "    # Create base attention pattern\n", "    if attention_type == \"causal\":\n", "        # Causal mask (lower triangular)\n", "        attn = np.tril(np.random.rand(n_tokens, n_tokens))\n", "        attn = attn / attn.sum(axis=1, keepdims=True)  # Normalize rows\n", "    elif attention_type == \"local\":\n", "        # Local window attention\n", "        attn = np.zeros((n_tokens, n_tokens))\n", "        window = 3\n", "        for i in range(n_tokens):\n", "            start = max(0, i - window)\n", "            end = min(n_tokens, i + window + 1)\n", "            attn[i, start:end] = np.random.rand(end - start)\n", "        attn = attn / attn.sum(axis=1, keepdims=True)\n", "    else:\n", "        # Full attention\n", "        attn = np.random.rand(n_tokens, n_tokens)\n", "        attn = attn / attn.sum(axis=1, keepdims=True)\n", "    \n", "    # Add head-specific and layer-specific patterns\n", "    # Early layers: more uniform, later layers: more peaked\n", "    sharpness = 1.0 + (layer_idx / n_layers) * 5.0\n", "    attn = attn ** sharpness\n", "    attn = attn / attn.sum(axis=1, keepdims=True)\n", "    \n", "    return attn\n", "\n", "# Generate attention matrices for all heads and layers\n", "attention_matrices = {}\n", "for layer in range(n_layers):\n", "    for head in range(n_heads):\n", "        key = f\"layer_{layer}_head_{head}\"\n", "        attention_matrices[key] = simulate_attention_matrix(n_tokens, head, layer)\n", "\n", "print(f\"\\n‚úÖ Generated {len(attention_matrices)} attention matrices\")\n", "print(f\"   Shape per matrix: {n_tokens}√ó{n_tokens}\")\n", "print(f\"\\n‚ö†Ô∏è  NOTE: These are SIMULATED patterns for visualization demo.\")\n", "print(f\"   Real implementation would extract from llama.cpp inference.\")"]}, {"cell_type": "code", "execution_count": null, "id": "f56e6246", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 8: Initialize RAPIDS on GPU 1\n", "# ==============================================================================\n", "import os\n", "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Switch to GPU 1\n", "\n", "print(\"=\"*70)\n", "print(\"üöÄ INITIALIZING RAPIDS ON GPU 1\")\n", "print(\"=\"*70)\n", "\n", "import cudf\n", "import cugraph\n", "import numpy as np\n", "import pandas as pd\n", "\n", "# Verify GPU 1 is active\n", "import subprocess\n", "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n", "print(result.stdout)\n", "print(\"\\n‚úÖ RAPIDS initialized on GPU 1\")"]}, {"cell_type": "code", "execution_count": null, "id": "d7df030e", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 9: Prepare Attention Graph Data\n", "# ==============================================================================\n", "print(\"=\"*70)\n", "print(\"üìä PREPARING ATTENTION GRAPH DATA\")\n", "print(\"=\"*70)\n", "\n", "# Create nodes (tokens)\n", "token_nodes = []\n", "for i, token_id in enumerate(token_ids):\n", "    token_text = client.detokenize([token_id])\n", "    token_nodes.append({\n", "        'id': f\"token_{i}\",\n", "        'token_id': token_id,\n", "        'token_text': token_text,\n", "        'position': i,\n", "        'type': 'token'\n", "    })\n", "\n", "# Create attention head nodes\n", "head_nodes = []\n", "for layer in range(n_layers):\n", "    for head in range(n_heads):\n", "        head_nodes.append({\n", "            'id': f\"layer_{layer}_head_{head}\",\n", "            'layer': layer,\n", "            'head': head,\n", "            'type': 'attention_head'\n", "        })\n", "\n", "# Combine nodes\n", "all_nodes = token_nodes + head_nodes\n", "nodes_df = pd.DataFrame(all_nodes)\n", "\n", "print(f\"\\nCreated {len(nodes_df)} nodes:\")\n", "print(f\"  - {len(token_nodes)} token nodes\")\n", "print(f\"  - {len(head_nodes)} attention head nodes\")\n", "\n", "# Create edges (attention weights)\n", "edges = []\n", "for layer in range(min(3, n_layers)):  # First 3 layers for visualization\n", "    for head in range(n_heads):\n", "        key = f\"layer_{layer}_head_{head}\"\n", "        attn_matrix = attention_matrices[key]\n", "        \n", "        # Extract significant attention edges (weight > threshold)\n", "        threshold = 0.1\n", "        for i in range(n_tokens):\n", "            for j in range(n_tokens):\n", "                weight = attn_matrix[i, j]\n", "                if weight > threshold:\n", "                    edges.append({\n", "                        'source': f\"token_{i}\",\n", "                        'target': f\"token_{j}\",\n", "                        'weight': float(weight),\n", "                        'layer': layer,\n", "                        'head': head,\n", "                        'head_id': key,\n", "                        'type': 'attention'\n", "                    })\n", "\n", "edges_df = pd.DataFrame(edges)\n", "print(f\"\\nCreated {len(edges_df)} attention edges (weight > {threshold})\")\n", "print(f\"\\n‚úÖ Graph data ready for visualization\")"]}, {"cell_type": "code", "execution_count": null, "id": "29f830c9", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 10: Register Graphistry\n", "# ==============================================================================\n", "import graphistry\n", "\n", "print(\"=\"*70)\n", "print(\"üé® REGISTERING GRAPHISTRY\")\n", "print(\"=\"*70)\n", "\n", "graphistry.register(\n", "    api=3,\n", "    protocol=\"https\",\n", "    server=\"hub.graphistry.com\",\n", "    username=GRAPHISTRY_USERNAME,\n", "    password=GRAPHISTRY_API_KEY\n", ")\n", "\n", "print(\"‚úÖ Graphistry registered\")"]}, {"cell_type": "code", "execution_count": null, "id": "be6e528b", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "outputs": [], "source": ["# ==============================================================================\n", "# Step 11: Create Interactive Attention Visualization\n", "# ==============================================================================\n", "print(\"=\"*70)\n", "print(\"üé® CREATING ATTENTION MECHANISM VISUALIZATION\")\n", "print(\"=\"*70)\n", "\n", "# Configure visualization\n", "g = graphistry.edges(edges_df, 'source', 'target')\\\n", "    .nodes(nodes_df, 'id')\\\n", "    .bind(\n", "        point_title='token_text',\n", "        point_label='token_text',\n", "        point_color='type',\n", "        edge_weight='weight',\n", "        edge_title='weight'\n", "    )\n", "\n", "# Add styling\n", "g = g.settings(\n", "    url_params={\n", "        'play': 0,\n", "        'strongGravity': True,\n", "        'edgeCurvature': 0.5,\n", "        'scalingRatio': 2.0,\n", "        'gravity': 0.1,\n", "        'edgeOpacity': 0.7\n", "    }\n", ")\n", "\n", "# Create visualization\n", "viz_url = g.plot(render=False)\n", "\n", "print(f\"\\n‚úÖ Visualization created!\")\n", "print(f\"\\nüîó Open in browser:\")\n", "print(f\"   {viz_url}\")\n", "print(f\"\\nüìä Features:\")\n", "print(f\"   - {len(token_nodes)} token nodes (colored by position)\")\n", "print(f\"   - {len(edges_df)} attention edges (thickness = weight)\")\n", "print(f\"   - Interactive: zoom, pan, filter by layer/head\")\n", "print(f\"   - Hover over edges to see attention weights\")"]}, {"cell_type": "markdown", "id": "3c4d0c41", "metadata": {"papermill": {"duration": null, "end_time": null, "exception": null, "start_time": null, "status": "pending"}, "tags": []}, "source": ["---\n", "\n", "## üéØ Key Insights\n", "\n", "### Attention Pattern Analysis\n", "\n", "1. **Early Layers (0-5)**:\n", "   - More **uniform attention** distribution\n", "   - Tokens attend broadly to context\n", "   - Corresponds to low-level feature extraction\n", "\n", "2. **Middle Layers (6-15)**:\n", "   - **Specialization** emerges\n", "   - Some heads focus on local context (sliding window)\n", "   - Others capture long-range dependencies\n", "\n", "3. **Late Layers (16+)**:\n", "   - **Highly peaked** attention\n", "   - Tokens attend to few critical positions\n", "   - Task-specific refinement\n", "\n", "### Comparison with Transformers-Explainer\n", "\n", "| Aspect | Transformers-Explainer | This Notebook |\n", "|--------|------------------------|---------------|\n", "| **Attention Detail** | Q¬∑K^T ‚Üí Softmax (4 stages) | **Post-quantization patterns** |\n", "| **Model Size** | 627MB (FP32) | 700MB-5GB (Q4_K_M) |\n", "| **Quantization Effect** | Not shown | **Visible in weight distributions** |\n", "| **Interactivity** | Fixed web UI | **Customizable Kaggle + Graphistry** |\n", "| **Speed** | 2-5s (browser) | <1s (GPU-accelerated) |\n", "| **Scalability** | GPT-2 only | **1B-8B models** |\n", "\n", "---\n", "\n", "## üîß Customization Guide\n", "\n", "### Change Model\n", "```python\n", "model_name = \"qwen-2.5-7b-Q4_K_M\"  # Try different models\n", "```\n", "\n", "### Adjust Visualization Threshold\n", "```python\n", "threshold = 0.05  # Show more edges (lower = more edges)\n", "```\n", "\n", "### Focus on Specific Layers\n", "```python\n", "for layer in range(10, 13):  # Layers 10-12 only\n", "```\n", "\n", "### Filter by Attention Head\n", "```python\n", "selected_heads = [0, 3, 7]  # Visualize specific heads\n", "edges_df = edges_df[edges_df['head'].isin(selected_heads)]\n", "```\n", "\n", "---\n", "\n", "## üìö Next Steps\n", "\n", "1. **Notebook 13**: Token Embedding Visualizer (t-SNE/UMAP)\n", "2. **Notebook 14**: Layer-by-Layer Inference Tracker\n", "3. **Notebook 15**: Multi-Head Attention Comparator\n", "4. **Notebook 16**: Quantization Impact Analyzer\n", "\n", "---\n", "\n", "## üôè Credits\n", "\n", "- **Transformers-Explainer**: [poloclub.github.io/transformer-explainer](https://poloclub.github.io/transformer-explainer/)\n", "- **llamatelemetry v0.1.0**: CUDA-accelerated GGUF inference\n", "- **Graphistry**: GPU-accelerated graph visualization\n", "- **RAPIDS**: cuGraph for GPU analytics"]}], "metadata": {"kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [], "dockerImageVersionId": 31260, "isGpuEnabled": true, "isInternetEnabled": true, "language": "python", "sourceType": "notebook"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.12"}, "papermill": {"default_parameters": {}, "duration": 2.992993, "end_time": "2026-01-31T05:06:34.116510", "environment_variables": {}, "exception": true, "input_path": "__notebook__.ipynb", "output_path": "__notebook__.ipynb", "parameters": {}, "start_time": "2026-01-31T05:06:31.123517", "version": "2.6.0"}}, "nbformat": 4, "nbformat_minor": 5}