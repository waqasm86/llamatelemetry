{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7e98e189","cell_type":"markdown","source":"## Step 1: Model Size Reference","metadata":{}},{"id":"a45b2792-671f-4e87-a20f-1accaf6012aa","cell_type":"markdown","source":"Verifies dual T4 GPU environment (30GB total VRAM) required for deploying large language models (13B-70B parameters) on Kaggle.","metadata":{}},{"id":"5570c27d","cell_type":"code","source":"# Step 1: Model Size Reference (Updated for Kaggle Limits)\nprint(\"=\"*70)\nprint(\"ğŸ“Š MODEL SIZE REFERENCE FOR DUAL T4 (30GB VRAM)\")\nprint(\"=\"*70)\n\nmodels_table = \"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘ Model Size    â•‘ Quantization  â•‘ VRAM Needed  â•‘ Kaggle Fit?  â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘ 1-3B          â•‘ Q4_K_M        â•‘ ~2-3 GB      â•‘ âœ… Single GPU â•‘\nâ•‘ 7-8B          â•‘ Q4_K_M        â•‘ ~5-6 GB      â•‘ âœ… Single GPU â•‘\nâ•‘ 13B           â•‘ Q4_K_M        â•‘ ~8-9 GB      â•‘ âœ… Single GPU â•‘\nâ•‘ 13B           â•‘ Q8_0          â•‘ ~14-15 GB    â•‘ âœ… Single GPU â•‘\nâ•‘ 32-34B        â•‘ Q4_K_M        â•‘ ~20-22 GB    â•‘ âš ï¸ Dual GPU   â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ“Œ Recommended Models for Kaggle:\nâ€¢ Best Speed: 7-8B Q4_K_M (5-6GB VRAM)\nâ€¢ Best Balance: 13B Q4_K_M (8-9GB VRAM)\nâ€¢ Maximum Quality: 13B Q8_0 (14-15GB VRAM)\n\"\"\"\nprint(models_table)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:53:08.867385Z","iopub.execute_input":"2026-02-05T03:53:08.868089Z","iopub.status.idle":"2026-02-05T03:53:08.876535Z","shell.execute_reply.started":"2026-02-05T03:53:08.868061Z","shell.execute_reply":"2026-02-05T03:53:08.875690Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š MODEL SIZE REFERENCE FOR DUAL T4 (30GB VRAM)\n======================================================================\n\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘ Model Size    â•‘ Quantization  â•‘ VRAM Needed  â•‘ Kaggle Fit?  â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘ 1-3B          â•‘ Q4_K_M        â•‘ ~2-3 GB      â•‘ âœ… Single GPU â•‘\nâ•‘ 7-8B          â•‘ Q4_K_M        â•‘ ~5-6 GB      â•‘ âœ… Single GPU â•‘\nâ•‘ 13B           â•‘ Q4_K_M        â•‘ ~8-9 GB      â•‘ âœ… Single GPU â•‘\nâ•‘ 13B           â•‘ Q8_0          â•‘ ~14-15 GB    â•‘ âœ… Single GPU â•‘\nâ•‘ 32-34B        â•‘ Q4_K_M        â•‘ ~20-22 GB    â•‘ âš ï¸ Dual GPU   â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ“Œ Recommended Models for Kaggle:\nâ€¢ Best Speed: 7-8B Q4_K_M (5-6GB VRAM)\nâ€¢ Best Balance: 13B Q4_K_M (8-9GB VRAM)\nâ€¢ Maximum Quality: 13B Q8_0 (14-15GB VRAM)\n\n","output_type":"stream"}],"execution_count":1},{"id":"d3b229da-1dd5-4ea3-bd5f-a893283659fa","cell_type":"markdown","source":"Installs llamatelemetry v0.1.0 with multi-GPU tensor-split support for distributing large models across dual T4 GPUs.","metadata":{}},{"id":"d336e787-d802-4a95-95ab-62eaec63a5e3","cell_type":"code","source":"!nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:53:12.249968Z","iopub.execute_input":"2026-02-05T03:53:12.250271Z","iopub.status.idle":"2026-02-05T03:53:12.404025Z","shell.execute_reply.started":"2026-02-05T03:53:12.250245Z","shell.execute_reply":"2026-02-05T03:53:12.403214Z"}},"outputs":[{"name":"stdout","text":"index, name, memory.total [MiB], memory.free [MiB]\n0, Tesla T4, 15360 MiB, 14913 MiB\n1, Tesla T4, 15360 MiB, 14913 MiB\n","output_type":"stream"}],"execution_count":2},{"id":"2c67cb53","cell_type":"markdown","source":"## Step 3: Setup Environment","metadata":{}},{"id":"793d81c8-0901-472b-a180-57ee1c60cdec","cell_type":"markdown","source":"Installs llamatelemetry v0.1.0 with multi-GPU support and dependencies for deploying large quantized models on dual T4 GPUs.","metadata":{}},{"id":"e7992615-825e-46e2-b8e1-9a0f15e10ac0","cell_type":"markdown","source":"Displays quantization guide for large models showing which parameter sizes fit on dual T4 with different quantization types (IQ3_XS, Q4_K_M).","metadata":{}},{"id":"9c27a5fa-cbeb-443d-83e6-305f67b32f02","cell_type":"code","source":"%%time\nprint(\"ğŸ“¦ Installing dependencies...\")\n\n!pip install -q huggingface-hub sseclient-py\n\n# Install llamatelemetry v0.1.0\n!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph (matching Kaggle RAPIDS 25.6.0)\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry\n!pip install -q \"graphistry[ai]\" huggingface_hub\n\n# Verify\nimport llamatelemetry\nprint(f\"\\nâœ… llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"âœ… cuDF {cudf.__version__}\")\n    print(f\"âœ… cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"âœ… Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:53:18.401280Z","iopub.execute_input":"2026-02-05T03:53:18.401658Z","iopub.status.idle":"2026-02-05T03:55:11.522161Z","shell.execute_reply.started":"2026-02-05T03:53:18.401625Z","shell.execute_reply":"2026-02-05T03:55:11.521257Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing dependencies...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(â€¦):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea750cf2d0f9436a84118e0b163c1c19"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llamatelemetry 0.1.0 installed\nâœ… cuDF 25.06.00\nâœ… cuGraph 25.06.00\nâœ… Graphistry 0.50.6\nCPU times: user 59.8 s, sys: 14.3 s, total: 1min 14s\nWall time: 1min 53s\n","output_type":"stream"}],"execution_count":3},{"id":"e7574d99-f69d-4b6d-a324-5f3e67d5f915","cell_type":"markdown","source":"Calculates VRAM requirements for various large models demonstrating feasibility of 70B models with IQ3_XS quantization on dual T4.","metadata":{}},{"id":"db35d786-6111-472f-a928-8c5df1ecaaf0","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2b1db834-2bfd-4e7c-a6ba-a64308aaea3f","cell_type":"markdown","source":"Downloads large language model (13B+ parameters) in aggressive quantization format optimized for dual T4 VRAM constraints.","metadata":{}},{"id":"dcfe8f01-1ac7-4702-bf55-4194a7138f3d","cell_type":"code","source":"# Step 3: Setup Secrets\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nimport os\n\n# Setup HuggingFace token (optional for private models)\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN_2\")\n    if hf_token:\n        login(hf_token)\n        print(\"âœ… HuggingFace token loaded\")\nexcept:\n    print(\"â„¹ï¸ No HuggingFace token - using public models only\")\n\n# Setup Graphistry\ntry:\n    graphistry.register(\n        api=3,\n        protocol=\"https\",\n        server=\"hub.graphistry.com\",\n        personal_key_id=user_secrets.get_secret(\"Graphistry_Personal_Key_ID\"),\n        personal_key_secret=user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n    )\n    print(\"âœ… Graphistry registered successfully\")\nexcept:\n    print(\"â„¹ï¸ Graphistry not registered - visualization will be limited\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:55:23.698709Z","iopub.execute_input":"2026-02-05T03:55:23.699570Z","iopub.status.idle":"2026-02-05T03:55:24.850671Z","shell.execute_reply.started":"2026-02-05T03:55:23.699533Z","shell.execute_reply":"2026-02-05T03:55:24.849749Z"}},"outputs":[{"name":"stdout","text":"âœ… HuggingFace token loaded\nâœ… Graphistry registered successfully\n","output_type":"stream"}],"execution_count":4},{"id":"36467754","cell_type":"markdown","source":"## Step 4: Download 70B IQ3_XS Model","metadata":{}},{"id":"aeee2440-43c7-4a41-97bf-06c80baebe50","cell_type":"markdown","source":"Downloads a large model (13B-70B parameters) in aggressive quantization (IQ3_XS/Q4_K_M) optimized for dual T4 VRAM constraints.","metadata":{}},{"id":"7242b4ba-1355-4a4d-ba88-a15ed03ac078","cell_type":"markdown","source":"Configures multi-GPU settings with tensor-split distributing model layers equally (0.5,0.5) across both T4 GPUs for maximum capacity.","metadata":{}},{"id":"72ea665f","cell_type":"code","source":"# Step 4: Download 3 LLM Models (1-4GB each)\nfrom huggingface_hub import hf_hub_download\nimport os\n\nprint(\"=\"*70)\nprint(\"ğŸ“¥ DOWNLOADING 3 LLM MODELS (1-4GB EACH)\")\nprint(\"=\"*70)\n\n# Create models directory\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\n\n# Model configurations - all under 4GB each\nmodel_configs = [\n    {\n        \"name\": \"Llama-3.2-3B-Instruct\",\n        \"repo\": \"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n        \"file\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",  # ~1.88GB\n        \"size_gb\": 1.88\n    },\n    {\n        \"name\": \"Mistral-7B-Instruct\",\n        \"repo\": \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        \"file\": \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # ~4.08GB\n        \"size_gb\": 4.08\n    },\n    {\n        \"name\": \"Phi-3-mini-4k\",\n        \"repo\": \"microsoft/Phi-3-mini-4k-instruct-gguf\",\n        \"file\": \"Phi-3-mini-4k-instruct-q4.gguf\",  # ~2.1GB\n        \"size_gb\": 2.1\n    }\n]\n\nmodel_paths = {}\n\nfor model in model_configs:\n    print(f\"\\nğŸ“¥ Downloading {model['name']}...\")\n    print(f\"   Quantization: Q4_K_M\")\n    print(f\"   Estimated size: {model['size_gb']:.2f} GB\")\n    \n    try:\n        # Download with timeout and progress\n        model_path = hf_hub_download(\n            repo_id=model[\"repo\"],\n            filename=model[\"file\"],\n            local_dir=\"/kaggle/working/models\",\n            local_dir_use_symlinks=False\n        )\n        \n        actual_size = os.path.getsize(model_path) / (1024**3)\n        model_paths[model[\"name\"]] = model_path\n        print(f\"âœ… Downloaded: {os.path.basename(model_path)}\")\n        print(f\"   Actual size: {actual_size:.2f} GB\")\n        print(f\"   Path: {model_path}\")\n        \n    except Exception as e:\n        print(f\"âš ï¸ Failed to download {model['name']}: {e}\")\n        # Try alternative\n        print(\"   Trying alternative model...\")\n\nprint(f\"\\nğŸ“Š Summary: Downloaded {len(model_paths)} models\")\nfor name, path in model_paths.items():\n    print(f\"   â€¢ {name}: {os.path.basename(path)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:55:28.651341Z","iopub.execute_input":"2026-02-05T03:55:28.651906Z","iopub.status.idle":"2026-02-05T03:55:49.593273Z","shell.execute_reply.started":"2026-02-05T03:55:28.651879Z","shell.execute_reply":"2026-02-05T03:55:49.592206Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“¥ DOWNLOADING 3 LLM MODELS (1-4GB EACH)\n======================================================================\n\nğŸ“¥ Downloading Llama-3.2-3B-Instruct...\n   Quantization: Q4_K_M\n   Estimated size: 1.88 GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6315717352914edcb9f894484991e3d9"}},"metadata":{}},{"name":"stdout","text":"âœ… Downloaded: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   Actual size: 1.88 GB\n   Path: /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n\nğŸ“¥ Downloading Mistral-7B-Instruct...\n   Quantization: Q4_K_M\n   Estimated size: 4.08 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"mistral-7b-instruct-v0.2.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5f8247b42c4f4c9c4907ff8300ec6e"}},"metadata":{}},{"name":"stdout","text":"âœ… Downloaded: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n   Actual size: 4.07 GB\n   Path: /kaggle/working/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n\nğŸ“¥ Downloading Phi-3-mini-4k...\n   Quantization: Q4_K_M\n   Estimated size: 2.10 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Phi-3-mini-4k-instruct-q4.gguf:   0%|          | 0.00/2.39G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b60845a15549f5a524b726b38a2609"}},"metadata":{}},{"name":"stdout","text":"âœ… Downloaded: Phi-3-mini-4k-instruct-q4.gguf\n   Actual size: 2.23 GB\n   Path: /kaggle/working/models/Phi-3-mini-4k-instruct-q4.gguf\n\nğŸ“Š Summary: Downloaded 3 models\n   â€¢ Llama-3.2-3B-Instruct: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   â€¢ Mistral-7B-Instruct: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n   â€¢ Phi-3-mini-4k: Phi-3-mini-4k-instruct-q4.gguf\n","output_type":"stream"}],"execution_count":5},{"id":"784b32d7","cell_type":"markdown","source":"## Step 5: Calculate Optimal Tensor Split","metadata":{}},{"id":"3552cad8-5984-400a-9353-3e2738ea449e","cell_type":"markdown","source":"Starts llama-server with dual-GPU configuration enabling models too large for single T4 to run by splitting across both GPUs.","metadata":{}},{"id":"7454d73d","cell_type":"code","source":"# Step 5: Check GPU Availability\nimport subprocess\n\nprint(\"=\"*70)\nprint(\"ğŸ” GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nğŸ“Š Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\nâœ… Dual T4 ready for multi-model testing!\")\n    print(\"   Can run different models on different GPUs\")\nelse:\n    print(\"\\nâš ï¸ Only 1 GPU detected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:56:46.262553Z","iopub.execute_input":"2026-02-05T03:56:46.262901Z","iopub.status.idle":"2026-02-05T03:56:46.315253Z","shell.execute_reply.started":"2026-02-05T03:56:46.262873Z","shell.execute_reply":"2026-02-05T03:56:46.314493Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ” GPU ENVIRONMENT CHECK\n======================================================================\n\nğŸ“Š Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 14910 MiB\n   1, Tesla T4, 15360 MiB, 14910 MiB\n\nâœ… Dual T4 ready for multi-model testing!\n   Can run different models on different GPUs\n","output_type":"stream"}],"execution_count":6},{"id":"7a23a811","cell_type":"markdown","source":"## Step 6: Start Server with llm Models","metadata":{}},{"id":"700226e2-17dc-42ea-8f9b-4ad3b5a81778","cell_type":"markdown","source":"Starts llama-server with dual-GPU tensor splitting, enabling large models that wouldn't fit on single T4 to run across both GPUs.","metadata":{}},{"id":"d018a8ad-1acf-4de1-9ace-ee823810212d","cell_type":"markdown","source":"Verifies model distribution by checking VRAM usage on both GPUs confirming weights are properly split using tensor parallelism.","metadata":{}},{"id":"4ae5d009","cell_type":"code","source":"# Step 6: Start Single llama-server on GPU 0 (FIXED VERSION)\nfrom llamatelemetry.server import ServerManager\nimport time  # Import at the top\n\nprint(\"=\"*70)\nprint(\"ğŸš€ STARTING SINGLE LLAMA-SERVER ON GPU 0\")\nprint(\"=\"*70)\n\n# Choose one model to run\nselected_model = None\nmodel_name = \"\"\n\nif \"Llama-3.2-3B-Instruct\" in model_paths:\n    selected_model = model_paths[\"Llama-3.2-3B-Instruct\"]\n    model_name = \"Llama-3.2-3B-Instruct\"\nelif \"Mistral-7B-Instruct\" in model_paths:\n    selected_model = model_paths[\"Mistral-7B-Instruct\"]\n    model_name = \"Mistral-7B-Instruct\"\nelif \"Phi-3-mini-4k\" in model_paths:\n    selected_model = model_paths[\"Phi-3-mini-4k\"]\n    model_name = \"Phi-3-mini-4k\"\n\nif not selected_model:\n    print(\"âŒ No models available to run!\")\n    # Use first available model\n    for name, path in model_paths.items():\n        selected_model = path\n        model_name = name\n        break\n\nprint(f\"ğŸ“‹ Configuration:\")\nprint(f\"   Model: {model_name}\")\nprint(f\"   Path: {os.path.basename(selected_model)}\")\nprint(f\"   GPU 0: 100% (llama-server)\")\nprint(f\"   GPU 1: 0% (reserved for Graphistry)\")\n\nserver = ServerManager()\n\n# First try: Simple configuration based on Notebook 07\ntry:\n    print(\"\\nğŸ”„ Attempt 1: Simple configuration (no flash attention)...\")\n    server.start_server(\n        model_path=selected_model,\n        host=\"127.0.0.1\",\n        port=8090,\n        gpu_layers=99,\n        tensor_split=\"1.0,0.0\",  # All on GPU 0\n        ctx_size=4096,\n        # Skip problematic parameters\n    )\n    \n    print(\"â³ Waiting for server to start...\")\n    time.sleep(5)\n    \n    if server.check_server_health(timeout=60):\n        print(\"âœ… llama-server running on GPU 0!\")\n        print(f\"   Server URL: http://127.0.0.1:8090\")\n    else:\n        print(\"âŒ Server health check failed\")\n        \nexcept Exception as e:\n    print(f\"âŒ Attempt 1 failed: {e}\")\n    \n    # Second try: Even simpler configuration\n    print(\"\\nğŸ”„ Attempt 2: Minimal configuration...\")\n    try:\n        # Stop any existing server first\n        try:\n            server.stop_server()\n        except:\n            pass\n        \n        time.sleep(2)\n        \n        # Minimal parameters only\n        server.start_server(\n            model_path=selected_model,\n            host=\"127.0.0.1\",\n            port=8090,\n            gpu_layers=32,  # Reduced layers\n            tensor_split=\"1.0,0.0\",\n            ctx_size=2048,  # Reduced context\n        )\n        \n        time.sleep(5)\n        if server.check_server_health(timeout=60):\n            print(\"âœ… Server started with minimal config!\")\n        else:\n            print(\"âŒ Server health check failed\")\n            \n    except Exception as e2:\n        print(f\"âŒ Attempt 2 failed: {e2}\")\n        \n        # Third try: Use parameters from working Notebook 07\n        print(\"\\nğŸ”„ Attempt 3: Copy configuration from Notebook 07...\")\n        try:\n            try:\n                server.stop_server()\n            except:\n                pass\n            \n            time.sleep(2)\n            \n            # Exact parameters from Notebook 07 (which works)\n            server.start_server(\n                model_path=selected_model,\n                host=\"127.0.0.1\",\n                port=8090,\n                gpu_layers=99,\n                tensor_split=\"1.0,0.0\",\n                ctx_size=4096,\n                # No additional parameters\n            )\n            \n            time.sleep(5)\n            if server.check_server_health(timeout=60):\n                print(\"âœ… Server started with Notebook 07 config!\")\n            else:\n                print(\"âŒ Still failing\")\n                print(\"\\nğŸ”§ Debug: Checking server process...\")\n                import subprocess\n                result = subprocess.run([\"ps\", \"aux\"], capture_output=True, text=True)\n                if \"llama-server\" in result.stdout:\n                    print(\"âš ï¸ llama-server process found but not responding\")\n                else:\n                    print(\"âŒ No llama-server process found\")\n                    \n        except Exception as e3:\n            print(f\"âŒ All attempts failed: {e3}\")\n            print(\"\\nğŸ”§ Troubleshooting steps:\")\n            print(\"1. Check if model file exists and is valid\")\n            print(f\"   Path: {selected_model}\")\n            print(f\"   Exists: {os.path.exists(selected_model)}\")\n            print(f\"   Size: {os.path.getsize(selected_model) / 1024**3:.2f} GB\")\n            \n            print(\"\\n2. Try manual start:\")\n            print(f\"   !/usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server \\\\\")\n            print(f\"     --model {selected_model} \\\\\")\n            print(f\"     --host 127.0.0.1 \\\\\")\n            print(f\"     --port 8090 \\\\\")\n            print(f\"     --n-gpu-layers 99 \\\\\")\n            print(f\"     --ctx-size 4096 \\\\\")\n            print(f\"     --tensor-split 1.0,0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:05:17.229416Z","iopub.execute_input":"2026-02-05T03:05:17.229734Z","iopub.status.idle":"2026-02-05T03:05:26.424237Z","shell.execute_reply.started":"2026-02-05T03:05:17.229707Z","shell.execute_reply":"2026-02-05T03:05:26.423559Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸš€ STARTING SINGLE LLAMA-SERVER ON GPU 0\n======================================================================\nğŸ“‹ Configuration:\n   Model: Llama-3.2-3B-Instruct\n   Path: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   GPU 0: 100% (llama-server)\n   GPU 1: 0% (reserved for Graphistry)\n\nğŸ”„ Attempt 1: Simple configuration (no flash attention)...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready....... âœ“ Ready in 4.1s\nâ³ Waiting for server to start...\nâœ… llama-server running on GPU 0!\n   Server URL: http://127.0.0.1:8090\n","output_type":"stream"}],"execution_count":15},{"id":"f82a4573-e960-4850-aeff-db8b607c0866","cell_type":"code","source":"# Step 6: Start Multiple llama-servers for All Three Models\nfrom llamatelemetry.server import ServerManager\nimport time\nimport threading\nimport subprocess\n\nprint(\"=\"*70)\nprint(\"ğŸš€ STARTING MULTIPLE LLAMA-SERVERS FOR ALL MODELS\")\nprint(\"=\"*70)\n\n# Define server configurations for each model\nservers_config = []\nif model_paths:\n    # Use different ports for each model\n    ports = [8090, 8091, 8092]\n    gpu_assignments = [\"1.0,0.0\", \"0.5,0.5\", \"0.0,1.0\"]  # Different GPU allocations\n    \n    for i, (model_name, model_path) in enumerate(model_paths.items()):\n        if i < len(ports):\n            servers_config.append({\n                \"name\": model_name,\n                \"path\": model_path,\n                \"port\": ports[i],\n                \"gpu_assignment\": gpu_assignments[i] if len(gpu_assignments) > i else \"1.0,0.0\"\n            })\n\nprint(\"ğŸ“‹ Configuration:\")\nfor config in servers_config:\n    print(f\"   Model: {config['name']}\")\n    print(f\"   Port: {config['port']}\")\n    print(f\"   GPU Split: {config['gpu_assignment']}\")\n    print(f\"   Path: {os.path.basename(config['path'])}\")\n    print()\n\n# Function to start a single server\ndef start_single_server(config):\n    server = ServerManager()\n    \n    print(f\"\\nğŸ”„ Starting server for {config['name']} on port {config['port']}...\")\n    \n    try:\n        server.start_server(\n            model_path=config[\"path\"],\n            host=\"127.0.0.1\",\n            port=config[\"port\"],\n            gpu_layers=99,\n            tensor_split=config[\"gpu_assignment\"],\n            ctx_size=4096,\n        )\n        \n        time.sleep(3)\n        \n        if server.check_server_health(timeout=30):\n            print(f\"âœ… {config['name']} server running on port {config['port']}!\")\n            return server\n        else:\n            print(f\"âŒ {config['name']} server health check failed\")\n            return None\n            \n    except Exception as e:\n        print(f\"âŒ Failed to start {config['name']} server: {e}\")\n        \n        # Try with simpler config\n        try:\n            server.stop_server()\n            time.sleep(2)\n            \n            server.start_server(\n                model_path=config[\"path\"],\n                host=\"127.0.0.1\",\n                port=config[\"port\"],\n                gpu_layers=32,\n                tensor_split=config[\"gpu_assignment\"],\n                ctx_size=2048,\n            )\n            \n            time.sleep(3)\n            if server.check_server_health(timeout=30):\n                print(f\"âœ… {config['name']} server started with minimal config!\")\n                return server\n        except Exception as e2:\n            print(f\"âŒ {config['name']} server failed completely: {e2}\")\n            return None\n\n# Start all servers\nactive_servers = {}\nfor config in servers_config:\n    server = start_single_server(config)\n    if server:\n        active_servers[config[\"name\"]] = {\n            \"server\": server,\n            \"port\": config[\"port\"],\n            \"gpu_assignment\": config[\"gpu_assignment\"]\n        }\n\nprint(f\"\\nğŸ“Š Summary: {len(active_servers)} servers running\")\nfor name, info in active_servers.items():\n    print(f\"   â€¢ {name}: http://127.0.0.1:{info['port']} (GPU: {info['gpu_assignment']})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:57:06.243102Z","iopub.execute_input":"2026-02-05T03:57:06.243478Z","iopub.status.idle":"2026-02-05T03:57:18.528278Z","shell.execute_reply.started":"2026-02-05T03:57:06.243432Z","shell.execute_reply":"2026-02-05T03:57:18.527444Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸš€ STARTING MULTIPLE LLAMA-SERVERS FOR ALL MODELS\n======================================================================\nğŸ“‹ Configuration:\n   Model: Llama-3.2-3B-Instruct\n   Port: 8090\n   GPU Split: 1.0,0.0\n   Path: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n\n   Model: Mistral-7B-Instruct\n   Port: 8091\n   GPU Split: 0.5,0.5\n   Path: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n\n   Model: Phi-3-mini-4k\n   Port: 8092\n   GPU Split: 0.0,1.0\n   Path: Phi-3-mini-4k-instruct-q4.gguf\n\n\nğŸ”„ Starting server for Llama-3.2-3B-Instruct on port 8090...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready...... âœ“ Ready in 3.1s\nâœ… Llama-3.2-3B-Instruct server running on port 8090!\n\nğŸ”„ Starting server for Mistral-7B-Instruct on port 8091...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nâœ“ llama-server already running at http://127.0.0.1:8090\nâœ… Mistral-7B-Instruct server running on port 8091!\n\nğŸ”„ Starting server for Phi-3-mini-4k on port 8092...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nâœ“ llama-server already running at http://127.0.0.1:8090\nâœ… Phi-3-mini-4k server running on port 8092!\n\nğŸ“Š Summary: 3 servers running\n   â€¢ Llama-3.2-3B-Instruct: http://127.0.0.1:8090 (GPU: 1.0,0.0)\n   â€¢ Mistral-7B-Instruct: http://127.0.0.1:8091 (GPU: 0.5,0.5)\n   â€¢ Phi-3-mini-4k: http://127.0.0.1:8092 (GPU: 0.0,1.0)\n","output_type":"stream"}],"execution_count":7},{"id":"bacdc736","cell_type":"markdown","source":"## Step 7: Verify GPU Memory Usage","metadata":{}},{"id":"faf095cd-c516-4e32-b70d-086c26e41890","cell_type":"markdown","source":"Benchmarks large model inference performance measuring tokens/second, latency, and throughput on dual-GPU setup.","metadata":{}},{"id":"47fe52aa","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ’¾ GPU MEMORY USAGE\")\nprint(\"=\"*70)\n\n!nvidia-smi\n\nprint(\"\\nğŸ“Š Expected memory distribution:\")\nprint(\"   GPU 0: ~12-13 GB (model layers 0-39)\")\nprint(\"   GPU 1: ~12-13 GB (model layers 40-79)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:06:08.144097Z","iopub.execute_input":"2026-02-05T03:06:08.144871Z","iopub.status.idle":"2026-02-05T03:06:08.554954Z","shell.execute_reply.started":"2026-02-05T03:06:08.144841Z","shell.execute_reply":"2026-02-05T03:06:08.554033Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¾ GPU MEMORY USAGE\n======================================================================\nThu Feb  5 03:06:08 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   50C    P0             27W /   70W |    2565MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   51C    P0             27W /   70W |     105MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             371      C   .../binaries/cuda12/llama-server       2562MiB |\n|    1   N/A  N/A             371      C   .../binaries/cuda12/llama-server        102MiB |\n+-----------------------------------------------------------------------------------------+\n\nğŸ“Š Expected memory distribution:\n   GPU 0: ~12-13 GB (model layers 0-39)\n   GPU 1: ~12-13 GB (model layers 40-79)\n","output_type":"stream"}],"execution_count":16},{"id":"999effae-23c5-4c84-97d0-a017ce9e0996","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ’¾ GPU MEMORY USAGE - ALL MODELS\")\nprint(\"=\"*70)\n\n# Check memory usage\n!nvidia-smi\n\nprint(\"\\nğŸ“Š Expected memory distribution:\")\nprint(\"   GPU 0: Llama-3.2-3B-Instruct (100%)\")\nprint(\"   GPU 0: Mistral-7B-Instruct (50%) + GPU 1: Mistral-7B-Instruct (50%)\")\nprint(\"   GPU 1: Phi-3-mini-4k (100%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:00:23.966852Z","iopub.execute_input":"2026-02-05T04:00:23.967877Z","iopub.status.idle":"2026-02-05T04:00:24.380036Z","shell.execute_reply.started":"2026-02-05T04:00:23.967845Z","shell.execute_reply":"2026-02-05T04:00:24.379181Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¾ GPU MEMORY USAGE - ALL MODELS\n======================================================================\nThu Feb  5 04:00:24 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   62C    P0             31W /   70W |    2565MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   50C    P0             29W /   70W |     105MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             255      C   .../binaries/cuda12/llama-server       2562MiB |\n|    1   N/A  N/A             255      C   .../binaries/cuda12/llama-server        102MiB |\n+-----------------------------------------------------------------------------------------+\n\nğŸ“Š Expected memory distribution:\n   GPU 0: Llama-3.2-3B-Instruct (100%)\n   GPU 0: Mistral-7B-Instruct (50%) + GPU 1: Mistral-7B-Instruct (50%)\n   GPU 1: Phi-3-mini-4k (100%)\n","output_type":"stream"}],"execution_count":8},{"id":"b2374443-c8b1-46dd-8d3b-90926d771272","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0e7b4a02","cell_type":"markdown","source":"## Step 8: Test Inference","metadata":{}},{"id":"1bd03ada-63c8-4a54-ab4c-d64ae3a71811","cell_type":"markdown","source":"Tests different context window sizes (2K, 4K, 8K) to find optimal balance between context length and available VRAM.","metadata":{}},{"id":"7a80ca00","cell_type":"code","source":"from llamatelemetry.api.client import LlamaCppClient\nimport time\n\nprint(\"=\"*70)\nprint(\"=\"*70)\n\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n\n# Test prompt\nprompt = \"Explain quantum computing in simple terms.\"\n\nprint(f\"\\nğŸ’¬ Prompt: {prompt}\")\nprint(\"\\nğŸ“ Response:\\n\")\n\nstart = time.time()\n\nresponse = client.chat.create(\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    max_tokens=200,\n    temperature=0.7,\n)\n\nelapsed = time.time() - start\ntokens = response.usage.completion_tokens\n\nprint(response.choices[0].message.content)\n\nprint(f\"\\nğŸ“Š Performance:\")\nprint(f\"   Time: {elapsed:.2f}s\")\nprint(f\"   Tokens: {tokens}\")\nprint(f\"   Speed: {tokens/elapsed:.1f} tok/s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:06:12.456522Z","iopub.execute_input":"2026-02-05T03:06:12.457010Z","iopub.status.idle":"2026-02-05T03:06:15.373466Z","shell.execute_reply.started":"2026-02-05T03:06:12.456977Z","shell.execute_reply":"2026-02-05T03:06:15.372820Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n======================================================================\n\nğŸ’¬ Prompt: Explain quantum computing in simple terms.\n\nğŸ“ Response:\n\nQuantum computing is a new way of processing information that's different from the traditional computers we use every day.\n\n**Traditional Computers**\n\nIn a traditional computer, information is represented as 0s and 1s. These 0s and 1s are stored on tiny switches called transistors inside the computer's brain (called the CPU). The computer processes information by flipping these switches between 0 and 1.\n\n**Quantum Computers**\n\nBut in a quantum computer, information is represented as special particles called qubits (quantum bits). These qubits can exist as both 0 AND 1 at the same time, which means they can process multiple possibilities simultaneously. This is different from traditional computers, where information is either 0 or 1.\n\nThink of it like a coin flip. In a traditional computer, you'd flip a coin and get heads (0) or tails (1). But in a quantum computer, the coin could be both heads AND tails at the same time\n\nğŸ“Š Performance:\n   Time: 2.83s\n   Tokens: 200\n   Speed: 70.6 tok/s\n","output_type":"stream"}],"execution_count":17},{"id":"0f5bcc31-3cdc-4b5c-8fc2-5231f1f79bba","cell_type":"code","source":"from llamatelemetry.api.client import LlamaCppClient\nimport time\nimport pandas as pd\n\nprint(\"=\"*70)\nprint(\"ğŸ¤– TEST INFERENCE - ALL MODELS\")\nprint(\"=\"*70)\n\n# Test prompts\ntest_prompts = [\n    \"Explain quantum computing in simple terms.\",\n    \"What is the capital of France?\",\n    \"Write a Python function to calculate factorial.\"\n]\n\n# Create clients for each server\nclients = {}\nfor name, info in active_servers.items():\n    clients[name] = LlamaCppClient(base_url=f\"http://127.0.0.1:{info['port']}\")\n\n# Test each model with each prompt\nresults = []\n\nfor model_name, client in clients.items():\n    print(f\"\\nğŸ§ª Testing {model_name}:\")\n    print(\"-\" * 40)\n    \n    for prompt in test_prompts[:1]:  # Test with first prompt for brevity\n        print(f\"\\nğŸ’¬ Prompt: {prompt}\")\n        \n        start = time.time()\n        try:\n            response = client.chat.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=100,\n                temperature=0.7,\n            )\n            \n            elapsed = time.time() - start\n            tokens = response.usage.completion_tokens\n            \n            # Display first 200 chars of response\n            response_text = response.choices[0].message.content\n            print(f\"ğŸ“ Response (first 200 chars): {response_text[:200]}...\")\n            \n            print(f\"ğŸ“Š Performance: {elapsed:.2f}s | {tokens} tokens | {tokens/elapsed:.1f} tok/s\")\n            \n            results.append({\n                \"Model\": model_name,\n                \"Prompt\": prompt[:30] + \"...\",\n                \"Time (s)\": f\"{elapsed:.2f}\",\n                \"Tokens\": tokens,\n                \"Speed (tok/s)\": f\"{tokens/elapsed:.1f}\"\n            })\n            \n        except Exception as e:\n            print(f\"âŒ Error: {e}\")\n            results.append({\n                \"Model\": model_name,\n                \"Prompt\": prompt[:30] + \"...\",\n                \"Time (s)\": \"Error\",\n                \"Tokens\": 0,\n                \"Speed (tok/s)\": \"N/A\"\n            })\n\n# Display results as a table\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“‹ INFERENCE RESULTS SUMMARY\")\nprint(\"=\"*70)\n\nif results:\n    df = pd.DataFrame(results)\n    print(df.to_string(index=False))\nelse:\n    print(\"No results collected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:01:58.195718Z","iopub.execute_input":"2026-02-05T04:01:58.196777Z","iopub.status.idle":"2026-02-05T04:01:59.893475Z","shell.execute_reply.started":"2026-02-05T04:01:58.196730Z","shell.execute_reply":"2026-02-05T04:01:59.892626Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ¤– TEST INFERENCE - ALL MODELS\n======================================================================\n\nğŸ§ª Testing Llama-3.2-3B-Instruct:\n----------------------------------------\n\nğŸ’¬ Prompt: Explain quantum computing in simple terms.\nğŸ“ Response (first 200 chars): Imagine you have a huge library with an infinite number of books, and each book represents a possible solution to a problem. A regular computer would look through the books one by one, checking each s...\nğŸ“Š Performance: 1.62s | 100 tokens | 61.8 tok/s\n\nğŸ§ª Testing Mistral-7B-Instruct:\n----------------------------------------\n\nğŸ’¬ Prompt: Explain quantum computing in simple terms.\nâŒ Error: HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\n\nğŸ§ª Testing Phi-3-mini-4k:\n----------------------------------------\n\nğŸ’¬ Prompt: Explain quantum computing in simple terms.\nâŒ Error: HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\n\n======================================================================\nğŸ“‹ INFERENCE RESULTS SUMMARY\n======================================================================\n                Model                            Prompt Time (s)  Tokens Speed (tok/s)\nLlama-3.2-3B-Instruct Explain quantum computing in s...     1.62     100          61.8\n  Mistral-7B-Instruct Explain quantum computing in s...    Error       0           N/A\n        Phi-3-mini-4k Explain quantum computing in s...    Error       0           N/A\n","output_type":"stream"}],"execution_count":9},{"id":"2c7744f2","cell_type":"markdown","source":"## Step 9: Performance Benchmarks","metadata":{}},{"id":"5910051c-81c7-45f2-aacc-e9ec3087c7d5","cell_type":"markdown","source":"Benchmarks large model inference performance measuring tokens/second throughput on the dual-GPU setup.","metadata":{}},{"id":"3820eddd-ede2-46ca-9deb-81a68c810e7c","cell_type":"markdown","source":"Compares quantization quality by testing same model in IQ3_XS vs Q4_K_M showing quality/size trade-offs for large models.","metadata":{}},{"id":"086512e1","cell_type":"code","source":"import time\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š PERFORMANCE BENCHMARKS\")\nprint(\"=\"*70)\n\nprompts = [\n    \"Hello\",  # Very short\n    \"What is the capital of France?\",  # Short\n    \"Write a paragraph about machine learning.\",  # Medium\n    \"Explain the differences between Python and JavaScript with examples.\",  # Long\n]\n\nprint(\"\\nğŸƒ Running benchmarks...\\n\")\n\nfor prompt in prompts:\n    start = time.time()\n    \n    response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=100,\n        temperature=0.7,\n    )\n    \n    elapsed = time.time() - start\n    tokens = response.usage.completion_tokens\n    \n    print(f\"   Prompt: \\\"{prompt[:40]}...\\\"\")\n    print(f\"   Time: {elapsed:.2f}s | Tokens: {tokens} | Speed: {tokens/elapsed:.1f} tok/s\")\n    print()\n\nprint(\"\"\"\nğŸ“ Expected Performance (70B on Dual T4):\n   â€¢ IQ3_XS: ~5-10 tok/s\n   â€¢ IQ2_XXS: ~8-15 tok/s (lower quality)\n   \n   Compared to:\n   â€¢ 7B Q4_K_M on single T4: ~40-60 tok/s\n   â€¢ 13B Q4_K_M on single T4: ~25-35 tok/s\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:06:22.534183Z","iopub.execute_input":"2026-02-05T03:06:22.534947Z","iopub.status.idle":"2026-02-05T03:06:25.672183Z","shell.execute_reply.started":"2026-02-05T03:06:22.534918Z","shell.execute_reply":"2026-02-05T03:06:25.671345Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š PERFORMANCE BENCHMARKS\n======================================================================\n\nğŸƒ Running benchmarks...\n\n   Prompt: \"Hello...\"\n   Time: 0.21s | Tokens: 8 | Speed: 37.3 tok/s\n\n   Prompt: \"What is the capital of France?...\"\n   Time: 0.12s | Tokens: 8 | Speed: 66.2 tok/s\n\n   Prompt: \"Write a paragraph about machine learning...\"\n   Time: 1.39s | Tokens: 100 | Speed: 72.0 tok/s\n\n   Prompt: \"Explain the differences between Python a...\"\n   Time: 1.41s | Tokens: 100 | Speed: 71.1 tok/s\n\n\nğŸ“ Expected Performance (70B on Dual T4):\n   â€¢ IQ3_XS: ~5-10 tok/s\n   â€¢ IQ2_XXS: ~8-15 tok/s (lower quality)\n   \n   Compared to:\n   â€¢ 7B Q4_K_M on single T4: ~40-60 tok/s\n   â€¢ 13B Q4_K_M on single T4: ~25-35 tok/s\n\n","output_type":"stream"}],"execution_count":18},{"id":"3c000c00-7e2b-4c4e-8bec-cf7a6ce8fda2","cell_type":"code","source":"import time\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š PERFORMANCE BENCHMARKS - ALL MODELS\")\nprint(\"=\"*70)\n\n# Define benchmark prompts\nbenchmark_prompts = [\n    \"Hello, how are you?\",\n    \"Explain the concept of machine learning.\",\n    \"Write a short story about a robot learning to paint.\",\n    \"What are the main differences between Python and JavaScript?\"\n]\n\nprint(\"\\nğŸƒ Running benchmarks for all models...\\n\")\n\nbenchmark_results = []\n\nfor model_name, client in clients.items():\n    print(f\"\\nğŸ”¬ Benchmarking {model_name}:\")\n    print(\"-\" * 40)\n    \n    model_times = []\n    model_speeds = []\n    \n    for i, prompt in enumerate(benchmark_prompts):\n        start = time.time()\n        \n        try:\n            response = client.chat.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=50,  # Fixed tokens for fair comparison\n                temperature=0.7,\n            )\n            \n            elapsed = time.time() - start\n            tokens = response.usage.completion_tokens\n            \n            speed = tokens / elapsed if elapsed > 0 else 0\n            \n            print(f\"   Prompt {i+1}: {elapsed:.2f}s | {speed:.1f} tok/s\")\n            \n            model_times.append(elapsed)\n            model_speeds.append(speed)\n            \n        except Exception as e:\n            print(f\"   Prompt {i+1}: Error - {e}\")\n            model_times.append(0)\n            model_speeds.append(0)\n    \n    # Calculate averages\n    avg_time = sum(model_times) / len(model_times) if model_times else 0\n    avg_speed = sum(model_speeds) / len(model_speeds) if model_speeds else 0\n    \n    benchmark_results.append({\n        \"Model\": model_name,\n        \"Avg Time (s)\": f\"{avg_time:.2f}\",\n        \"Avg Speed (tok/s)\": f\"{avg_speed:.1f}\",\n        \"GPU Assignment\": active_servers[model_name][\"gpu_assignment\"]\n    })\n    \n    print(f\"   ğŸ“Š Average: {avg_time:.2f}s | {avg_speed:.1f} tok/s\")\n\n# Display benchmark summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ† BENCHMARK SUMMARY\")\nprint(\"=\"*70)\n\nif benchmark_results:\n    df_benchmark = pd.DataFrame(benchmark_results)\n    print(df_benchmark.to_string(index=False))\n    \n    # Find fastest model\n    fastest = max(benchmark_results, key=lambda x: float(x[\"Avg Speed (tok/s)\"]))\n    print(f\"\\nğŸ¯ Fastest Model: {fastest['Model']} ({fastest['Avg Speed (tok/s)']} tok/s)\")\n    \n    # Compare model sizes\n    print(\"\\nğŸ“ˆ Model Size Comparison:\")\n    model_sizes = {\n        \"Llama-3.2-3B-Instruct\": \"3B\",\n        \"Mistral-7B-Instruct\": \"7B\", \n        \"Phi-3-mini-4k\": \"3.8B\"\n    }\n    \n    for model_name in model_paths.keys():\n        if model_name in model_sizes:\n            size = model_sizes[model_name]\n            speed = next((r[\"Avg Speed (tok/s)\"] for r in benchmark_results if r[\"Model\"] == model_name), \"N/A\")\n            print(f\"   â€¢ {model_name} ({size}): {speed} tok/s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:02:39.576593Z","iopub.execute_input":"2026-02-05T04:02:39.576921Z","iopub.status.idle":"2026-02-05T04:02:42.819945Z","shell.execute_reply.started":"2026-02-05T04:02:39.576896Z","shell.execute_reply":"2026-02-05T04:02:42.819089Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š PERFORMANCE BENCHMARKS - ALL MODELS\n======================================================================\n\nğŸƒ Running benchmarks for all models...\n\n\nğŸ”¬ Benchmarking Llama-3.2-3B-Instruct:\n----------------------------------------\n   Prompt 1: 0.80s | 58.4 tok/s\n   Prompt 2: 0.81s | 61.7 tok/s\n   Prompt 3: 0.80s | 62.8 tok/s\n   Prompt 4: 0.81s | 61.7 tok/s\n   ğŸ“Š Average: 0.81s | 61.1 tok/s\n\nğŸ”¬ Benchmarking Mistral-7B-Instruct:\n----------------------------------------\n   Prompt 1: Error - HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   Prompt 2: Error - HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   Prompt 3: Error - HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   Prompt 4: Error - HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   ğŸ“Š Average: 0.00s | 0.0 tok/s\n\nğŸ”¬ Benchmarking Phi-3-mini-4k:\n----------------------------------------\n   Prompt 1: Error - HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   Prompt 2: Error - HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   Prompt 3: Error - HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   Prompt 4: Error - HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\n   ğŸ“Š Average: 0.00s | 0.0 tok/s\n\n======================================================================\nğŸ† BENCHMARK SUMMARY\n======================================================================\n                Model Avg Time (s) Avg Speed (tok/s) GPU Assignment\nLlama-3.2-3B-Instruct         0.81              61.1        1.0,0.0\n  Mistral-7B-Instruct         0.00               0.0        0.5,0.5\n        Phi-3-mini-4k         0.00               0.0        0.0,1.0\n\nğŸ¯ Fastest Model: Llama-3.2-3B-Instruct (61.1 tok/s)\n\nğŸ“ˆ Model Size Comparison:\n   â€¢ Llama-3.2-3B-Instruct (3B): 61.1 tok/s\n   â€¢ Mistral-7B-Instruct (7B): 0.0 tok/s\n   â€¢ Phi-3-mini-4k (3.8B): 0.0 tok/s\n","output_type":"stream"}],"execution_count":10},{"id":"graphistry_header_09","cell_type":"markdown","source":"## Step 9B: Model Performance Landscape Visualization","metadata":{}},{"id":"1db08cfe-3cbf-449a-967d-faaa910eedec","cell_type":"markdown","source":"Demonstrates batch processing optimizations for large models including optimal batch sizes and parallel request handling.","metadata":{}},{"id":"graphistry_code_09","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ¨ MODEL PERFORMANCE LANDSCAPE - GRAPHISTRY VISUALIZATION\")\nprint(\"=\"*70)\n\nimport graphistry\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport numpy as np\n\n# --- 1. Register Graphistry ---\nprint(\"\\nğŸ” Registering with Graphistry...\")\ntry:\n    user_secrets = UserSecretsClient()\n    graphistry_key_id = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\n    graphistry_secret = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n\n    graphistry.register(\n        api=3,\n        protocol=\"https\",\n        server=\"hub.graphistry.com\",\n        personal_key_id=graphistry_key_id,\n        personal_key_secret=graphistry_secret\n    )\n    print(\"âœ… Graphistry registered successfully\")\nexcept Exception as e:\n    print(f\"âš ï¸ Graphistry registration failed: {e}\")\n    print(\"   Add secrets: Graphistry_Personal_Key_ID, Graphistry_Personal_Secret_Key\")\n\n# --- 2. Build Model Performance Network ---\nprint(\"\\nğŸ“Š Building model performance landscape...\")\n\n# Model configurations with performance metrics\nmodels_data = [\n    # Model Size, Quantization, VRAM, Speed (tok/s), Quality (stars), Fits T4, GPU Mode\n    {\"model\": \"1B-3B\", \"quant\": \"Q4_K_M\", \"vram_gb\": 2.5, \"speed\": 80, \"quality\": 3, \"fits\": \"Single\", \"category\": \"Small\"},\n    {\"model\": \"7B\", \"quant\": \"Q4_K_M\", \"vram_gb\": 5.5, \"speed\": 55, \"quality\": 4, \"fits\": \"Single\", \"category\": \"Medium\"},\n    {\"model\": \"7B\", \"quant\": \"F16\", \"vram_gb\": 15, \"speed\": 45, \"quality\": 5, \"fits\": \"Single\", \"category\": \"Medium\"},\n    {\"model\": \"13B\", \"quant\": \"Q4_K_M\", \"vram_gb\": 8.5, \"speed\": 32, \"quality\": 4, \"fits\": \"Single\", \"category\": \"Medium\"},\n    {\"model\": \"13B\", \"quant\": \"Q8_0\", \"vram_gb\": 14.5, \"speed\": 28, \"quality\": 5, \"fits\": \"Single\", \"category\": \"Medium\"},\n    {\"model\": \"32B\", \"quant\": \"Q4_K_M\", \"vram_gb\": 21, \"speed\": 20, \"quality\": 5, \"fits\": \"Dual\", \"category\": \"Large\"},\n    {\"model\": \"70B\", \"quant\": \"IQ4_XS\", \"vram_gb\": 35, \"speed\": 6, \"quality\": 4, \"fits\": \"None\", \"category\": \"XLarge\"},\n    {\"model\": \"70B\", \"quant\": \"IQ3_M\", \"vram_gb\": 28, \"speed\": 7, \"quality\": 4, \"fits\": \"Dual\", \"category\": \"XLarge\"},\n    {\"model\": \"70B\", \"quant\": \"IQ3_XS\", \"vram_gb\": 25, \"speed\": 8, \"quality\": 3, \"fits\": \"Dual\", \"category\": \"XLarge\"},\n    {\"model\": \"70B\", \"quant\": \"IQ2_XXS\", \"vram_gb\": 18, \"speed\": 12, \"quality\": 2, \"fits\": \"Dual\", \"category\": \"XLarge\"},\n    {\"model\": \"70B\", \"quant\": \"IQ2_XS\", \"vram_gb\": 20, \"speed\": 11, \"quality\": 2, \"fits\": \"Dual\", \"category\": \"XLarge\"},\n    {\"model\": \"70B\", \"quant\": \"IQ1_M\", \"vram_gb\": 16, \"speed\": 15, \"quality\": 1, \"fits\": \"Dual\", \"category\": \"XLarge\"},\n]\n\n# Create nodes DataFrame\nnodes_df = pd.DataFrame(models_data)\nnodes_df['node_id'] = range(len(nodes_df))\nnodes_df['label'] = nodes_df['model'] + \"\\\\n\" + nodes_df['quant']\n\n# --- 3. Create Compatibility Edges ---\n# Connect models that can work together or represent upgrade paths\nedges_list = []\n\nfor i, model1 in nodes_df.iterrows():\n    for j, model2 in nodes_df.iterrows():\n        if i >= j:\n            continue\n        \n        # Same model size, different quantization = quantization tradeoff edge\n        if model1['model'] == model2['model']:\n            edges_list.append({\n                'source': i,\n                'target': j,\n                'relationship': 'quant_tradeoff',\n                'weight': abs(model1['quality'] - model2['quality']) + 1,\n                'edge_label': 'Quant Options'\n            })\n        \n        # Compatible VRAM range = viable upgrade path\n        elif (abs(model1['vram_gb'] - model2['vram_gb']) < 8 and \n              model1['category'] != model2['category']):\n            edges_list.append({\n                'source': i,\n                'target': j,\n                'relationship': 'upgrade_path',\n                'weight': abs(model1['speed'] - model2['speed']) / 10 + 1,\n                'edge_label': 'Upgrade Path'\n            })\n\nedges_df = pd.DataFrame(edges_list)\n\nprint(f\"   Models: {len(nodes_df)}\")\nprint(f\"   Relationships: {len(edges_df)}\")\n\n# --- 4. Enhanced Node Attributes ---\n\n# Compute derived metrics\nnodes_df['efficiency'] = nodes_df['speed'] / nodes_df['vram_gb']  # tok/s per GB\nnodes_df['value_score'] = (nodes_df['speed'] * nodes_df['quality']) / (nodes_df['vram_gb'] / 10)\n\n# Size encoding (based on value score)\nvmin, vmax = nodes_df['value_score'].min(), nodes_df['value_score'].max()\nnodes_df['node_size'] = 30 + (nodes_df['value_score'] - vmin) / (vmax - vmin) * 70\n\n# Role classification\ndef classify_role(row):\n    if row['fits'] == 'None':\n        return 'Too Large'\n    elif row['speed'] > 40:\n        return 'Speed King'\n    elif row['quality'] >= 4:\n        return 'Quality Leader'\n    elif row['vram_gb'] > 20:\n        return 'Large Model'\n    else:\n        return 'Balanced'\n\nnodes_df['role'] = nodes_df.apply(classify_role, axis=1)\n\n# Rich tooltips\nnodes_df['point_title'] = nodes_df.apply(\n    lambda row: f\"{row['label'].replace(chr(92)*2+'n', ' ')}\\\\n\" +\n                f\"VRAM: {row['vram_gb']:.1f} GB | Speed: {row['speed']} tok/s\\\\n\" +\n                f\"Quality: {'â­' * int(row['quality'])}\\\\n\" +\n                f\"Fits: {row['fits']} GPU | Role: {row['role']}\\\\n\" +\n                f\"Efficiency: {row['efficiency']:.1f} tok/s/GB\\\\n\" +\n                f\"Value Score: {row['value_score']:.1f}\",\n    axis=1\n)\n\nedges_df['edge_title'] = edges_df.apply(\n    lambda row: f\"{row['relationship'].replace('_', ' ').title()}\\\\n\" +\n                f\"Strength: {row['weight']:.1f}\",\n    axis=1\n)\n\n# --- 5. Create Graphistry Visualization ---\nprint(\"\\nğŸ¨ Creating model performance landscape...\")\n\n# Color palettes\ncategory_colors = {\n    'Small': '#4ECDC4',      # Teal - Small models\n    'Medium': '#45B7D1',     # Blue - Medium models  \n    'Large': '#FFA07A',      # Orange - Large models\n    'XLarge': '#FF6B6B',     # Red - Extra large models\n}\n\nfits_colors = {\n    'Single': '#2ECC71',     # Green - Fits single GPU\n    'Dual': '#F39C12',       # Orange - Needs dual GPU\n    'None': '#E74C3C'        # Red - Doesn't fit\n}\n\nrole_icons = {\n    'Speed King': 'rocket',\n    'Quality Leader': 'trophy',\n    'Large Model': 'database',\n    'Balanced': 'balance-scale',\n    'Too Large': 'exclamation-triangle'\n}\n\nrelationship_colors = {\n    'quant_tradeoff': '#9B59B6',  # Purple - Quantization options\n    'upgrade_path': '#3498DB'     # Blue - Upgrade paths\n}\n\n# Bind graph\ng = graphistry.bind(\n    source='source',\n    destination='target',\n    node='node_id',\n    point_title='point_title',\n    edge_title='edge_title'\n)\n\n# Build visualization with advanced encodings\nplotter = (\n    g.edges(edges_df)\n    .nodes(nodes_df)\n    \n    # Primary encoding: Color by model category\n    .encode_point_color('category', categorical_mapping=category_colors, default_mapping='#95A5A6')\n    \n    # Size by value score (quality * speed / VRAM)\n    .encode_point_size('node_size')\n    \n    # Icon by role\n    .encode_point_icon('role', categorical_mapping=role_icons, default_mapping='circle')\n    \n    # Badge: GPU fit indicator\n    .encode_point_badge(\n        'fits',\n        'TopRight',\n        shape='circle',\n        categorical_mapping={\n            'Single': 'microchip',\n            'Dual': 'th',\n            'None': 'ban'\n        },\n        color={'mapping': {'categorical': {'fixed': fits_colors, 'other': '#999'}}}\n    )\n    \n    # Edge color by relationship type\n    .encode_edge_color('relationship', categorical_mapping=relationship_colors)\n    \n    # Settings for performance visualization\n    .settings(url_params={\n        'play': 2000,\n        'pointSize': 2.0,\n        'edgeOpacity': 0.5,\n        'bg': '%23FFFFFF',\n        'showArrows': 'true',\n        'showLabels': 'true',\n        'strongGravity': 'true',\n        'edgeInfluence': 1.5,\n        'pointsOfInterestMax': 5\n    })\n)\n\n# --- 6. Launch Visualization ---\ntry:\n    url = plotter.plot(\n        render=False,\n        name=\"Model Performance Landscape - llamatelemetry v0.1.0\",\n        description=f\"Interactive comparison of {len(nodes_df)} model configurations for Kaggle Dual T4\"\n    )\n\n    print(f\"\\nğŸš€ Visualization Created Successfully!\")\n    print(f\"\\nğŸ”— Graphistry URL:\")\n    print(f\"   {url}\")\n    print(f\"\\nğŸ“Œ Visualization Features:\")\n    print(f\"   âœ“ Color: Model category (Small/Medium/Large/XLarge)\")\n    print(f\"   âœ“ Size: Value score (speedÃ—quality/VRAM)\")\n    print(f\"   âœ“ Icon: Performance role (Speed King, Quality Leader, etc.)\")\n    print(f\"   âœ“ Badge: GPU fit indicator (Single/Dual/None)\")\n    print(f\"   âœ“ Edges: Quantization tradeoffs & upgrade paths\")\n    print(f\"   âœ“ Tooltips: Full specs, efficiency, value scores\")\n    \n    print(f\"\\nğŸ” Key Insights:\")\n    print(f\"   â€¢ Purple edges: Different quantizations of same model\")\n    print(f\"   â€¢ Blue edges: Viable upgrade paths (similar VRAM)\")\n    print(f\"   â€¢ Larger nodes: Better value (high speedÃ—quality, low VRAM)\")\n    print(f\"   â€¢ Green badge: Fits single T4 (15GB)\")\n    print(f\"   â€¢ Orange badge: Requires dual T4 split\")\n    print(f\"   â€¢ Red badge: Too large for dual T4\")\n\n    from IPython.display import display, HTML\n    display(HTML(\n        f'<div style=\"margin:20px; padding:20px; background:linear-gradient(135deg, #667eea 0%, #764ba2 100%); '\n        f'border-radius:12px; color:white; box-shadow:0 4px 6px rgba(0,0,0,0.1);\">'\n        f'<h3 style=\"margin:0 0 10px 0;\">ğŸ¯ Model Selection Dashboard</h3>'\n        f'<p style=\"margin:5px 0;\">Explore {len(nodes_df)} configurations across 4 model sizes</p>'\n        f'<p style=\"margin:5px 0; font-size:14px;\">Compare quantization tradeoffs, performance, and GPU compatibility</p>'\n        f'<a href=\"{url}\" target=\"_blank\" style=\"display:inline-block; margin-top:15px; padding:12px 24px; '\n        f'background:white; color:#667eea; text-decoration:none; border-radius:6px; font-weight:bold; '\n        f'box-shadow:0 2px 4px rgba(0,0,0,0.1);\">ğŸš€ Open Interactive Dashboard</a>'\n        f'</div>'\n    ))\n\nexcept Exception as e:\n    print(f\"\\nâŒ Visualization error: {e}\")\n    print(f\"   Data prepared successfully - {len(nodes_df)} nodes, {len(edges_df)} edges\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… Model performance landscape visualization complete\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:06:31.020549Z","iopub.execute_input":"2026-02-05T03:06:31.020884Z","iopub.status.idle":"2026-02-05T03:06:33.351272Z","shell.execute_reply.started":"2026-02-05T03:06:31.020858Z","shell.execute_reply":"2026-02-05T03:06:33.350665Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ¨ MODEL PERFORMANCE LANDSCAPE - GRAPHISTRY VISUALIZATION\n======================================================================\n\nğŸ” Registering with Graphistry...\nâœ… Graphistry registered successfully\n\nğŸ“Š Building model performance landscape...\n   Models: 12\n   Relationships: 33\n\nğŸ¨ Creating model performance landscape...\n\nğŸš€ Visualization Created Successfully!\n\nğŸ”— Graphistry URL:\n   https://hub.graphistry.com/graph/graph.html?dataset=97a8a1adee474301af5649921b1fce65&type=arrow&viztoken=8df625a6-2901-46dc-8419-8d0b7b980748&usertag=abf3123b-pygraphistry-0.50.6&splashAfter=1770260808&info=true&play=2000&pointSize=2.0&edgeOpacity=0.5&bg=%23FFFFFF&showArrows=true&showLabels=true&strongGravity=true&edgeInfluence=1.5&pointsOfInterestMax=5\n\nğŸ“Œ Visualization Features:\n   âœ“ Color: Model category (Small/Medium/Large/XLarge)\n   âœ“ Size: Value score (speedÃ—quality/VRAM)\n   âœ“ Icon: Performance role (Speed King, Quality Leader, etc.)\n   âœ“ Badge: GPU fit indicator (Single/Dual/None)\n   âœ“ Edges: Quantization tradeoffs & upgrade paths\n   âœ“ Tooltips: Full specs, efficiency, value scores\n\nğŸ” Key Insights:\n   â€¢ Purple edges: Different quantizations of same model\n   â€¢ Blue edges: Viable upgrade paths (similar VRAM)\n   â€¢ Larger nodes: Better value (high speedÃ—quality, low VRAM)\n   â€¢ Green badge: Fits single T4 (15GB)\n   â€¢ Orange badge: Requires dual T4 split\n   â€¢ Red badge: Too large for dual T4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div style=\"margin:20px; padding:20px; background:linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius:12px; color:white; box-shadow:0 4px 6px rgba(0,0,0,0.1);\"><h3 style=\"margin:0 0 10px 0;\">ğŸ¯ Model Selection Dashboard</h3><p style=\"margin:5px 0;\">Explore 12 configurations across 4 model sizes</p><p style=\"margin:5px 0; font-size:14px;\">Compare quantization tradeoffs, performance, and GPU compatibility</p><a href=\"https://hub.graphistry.com/graph/graph.html?dataset=97a8a1adee474301af5649921b1fce65&type=arrow&viztoken=8df625a6-2901-46dc-8419-8d0b7b980748&usertag=abf3123b-pygraphistry-0.50.6&splashAfter=1770260808&info=true&play=2000&pointSize=2.0&edgeOpacity=0.5&bg=%23FFFFFF&showArrows=true&showLabels=true&strongGravity=true&edgeInfluence=1.5&pointsOfInterestMax=5\" target=\"_blank\" style=\"display:inline-block; margin-top:15px; padding:12px 24px; background:white; color:#667eea; text-decoration:none; border-radius:6px; font-weight:bold; box-shadow:0 2px 4px rgba(0,0,0,0.1);\">ğŸš€ Open Interactive Dashboard</a></div>"},"metadata":{}},{"name":"stdout","text":"\n======================================================================\nâœ… Model performance landscape visualization complete\n======================================================================\n","output_type":"stream"}],"execution_count":19},{"id":"d9809c46-0fca-413a-bf53-abcfc41b61e7","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ea2d6f3b","cell_type":"markdown","source":"## Step 11: Streaming for Better UX","metadata":{}},{"id":"9fa244a9-a2fb-422e-9006-b4936692c53f","cell_type":"markdown","source":"Analyzes memory usage patterns during inference showing how VRAM is consumed by model weights, KV cache, and activations.","metadata":{}},{"id":"bdaae6ba","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸŒŠ STREAMING WITH LARGE MODELS\")\nprint(\"=\"*70)\n\nprint(\"\\nğŸ’¬ Streaming response (feels faster!):\\n\")\n\nstart = time.time()\ntoken_count = 0\n\ntry:\n    # Enable streaming with stream=True\n    stream_response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": \"What are the benefits of streaming?\"}],\n        max_tokens=100,\n        temperature=0.7,\n        stream=True  # This enables streaming\n    )\n    \n    print(\"Response: \", end=\"\", flush=True)\n    \n    # Iterate through the stream\n    for chunk in stream_response:\n        if hasattr(chunk, 'choices') and chunk.choices:\n            choice = chunk.choices[0]\n            if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content:\n                content = choice.delta.content\n                print(content, end=\"\", flush=True)\n                token_count += 1\n            elif hasattr(choice, 'message') and hasattr(choice.message, 'content') and choice.message.content:\n                # Some APIs might use message instead of delta\n                content = choice.message.content\n                print(content, end=\"\", flush=True)\n                token_count += 1\n    \n    elapsed = time.time() - start\n    print(f\"\\n\\nğŸ“Š Streamed {token_count} tokens in {elapsed:.2f}s\")\n    if elapsed > 0:\n        print(f\"ğŸ“ˆ Speed: {token_count/elapsed:.1f} tokens/sec\")\n    print(\"\\nâœ… Streaming provides immediate feedback even with slower models!\")\n\nexcept Exception as e:\n    print(f\"\\nâŒ Streaming error: {e}\")\n    \n    # Fallback to non-streaming\n    print(\"\\nğŸ”„ Trying non-streaming fallback...\")\n    try:\n        start = time.time()\n        response = client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": \"What are the benefits of streaming?\"}],\n            max_tokens=100,\n            temperature=0.7,\n            stream=False\n        )\n        \n        if response.choices and response.choices[0].message.content:\n            content = response.choices[0].message.content\n            print(f\"Response: {content}\")\n            \n            # Estimate tokens (rough approximation)\n            token_count = len(content.split())\n            elapsed = time.time() - start\n            print(f\"\\nğŸ“Š Generated {token_count} tokens in {elapsed:.2f}s\")\n            if elapsed > 0:\n                print(f\"ğŸ“ˆ Speed: {token_count/elapsed:.1f} tokens/sec\")\n        else:\n            print(\"âš ï¸ No response content received\")\n            \n    except Exception as e2:\n        print(f\"âŒ Fallback also failed: {e2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:07:31.369206Z","iopub.execute_input":"2026-02-05T03:07:31.369565Z","iopub.status.idle":"2026-02-05T03:07:32.880660Z","shell.execute_reply.started":"2026-02-05T03:07:31.369535Z","shell.execute_reply":"2026-02-05T03:07:32.880072Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸŒŠ STREAMING WITH LARGE MODELS\n======================================================================\n\nğŸ’¬ Streaming response (feels faster!):\n\nResponse: \n\nğŸ“Š Streamed 0 tokens in 1.50s\nğŸ“ˆ Speed: 0.0 tokens/sec\n\nâœ… Streaming provides immediate feedback even with slower models!\n","output_type":"stream"}],"execution_count":20},{"id":"000fb381-bbc1-408d-848f-fd90c558c540","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸŒŠ STREAMING TEST - ALL MODELS\")\nprint(\"=\"*70)\n\nstream_prompt = \"Explain the benefits of streaming in large language models.\"\n\nfor model_name, client in clients.items():\n    print(f\"\\nğŸ’­ Testing streaming for {model_name}:\")\n    print(\"-\" * 40)\n    \n    print(f\"Prompt: {stream_prompt}\")\n    print(\"Response stream: \", end=\"\", flush=True)\n    \n    start = time.time()\n    token_count = 0\n    \n    try:\n        stream_response = client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": stream_prompt}],\n            max_tokens=80,\n            temperature=0.7,\n            stream=True\n        )\n        \n        for chunk in stream_response:\n            if hasattr(chunk, 'choices') and chunk.choices:\n                choice = chunk.choices[0]\n                if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content:\n                    content = choice.delta.content\n                    print(content, end=\"\", flush=True)\n                    token_count += 1\n        \n        elapsed = time.time() - start\n        print(f\"\\n\\nğŸ“Š Streamed {token_count} tokens in {elapsed:.2f}s ({token_count/elapsed:.1f} tok/s)\")\n        \n    except Exception as e:\n        print(f\"\\nâŒ Streaming error: {e}\")\n        \n        # Fallback to non-streaming\n        try:\n            start = time.time()\n            response = client.chat.create(\n                messages=[{\"role\": \"user\", \"content\": stream_prompt}],\n                max_tokens=80,\n                temperature=0.7,\n                stream=False\n            )\n            \n            if response.choices and response.choices[0].message.content:\n                elapsed = time.time() - start\n                print(f\"ğŸ“ Non-streaming response generated in {elapsed:.2f}s\")\n        except Exception as e2:\n            print(f\"âŒ Fallback also failed: {e2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:03:13.080525Z","iopub.execute_input":"2026-02-05T04:03:13.080915Z","iopub.status.idle":"2026-02-05T04:03:14.436401Z","shell.execute_reply.started":"2026-02-05T04:03:13.080889Z","shell.execute_reply":"2026-02-05T04:03:14.435759Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸŒŠ STREAMING TEST - ALL MODELS\n======================================================================\n\nğŸ’­ Testing streaming for Llama-3.2-3B-Instruct:\n----------------------------------------\nPrompt: Explain the benefits of streaming in large language models.\nResponse stream: \n\nğŸ“Š Streamed 0 tokens in 1.34s (0.0 tok/s)\n\nğŸ’­ Testing streaming for Mistral-7B-Instruct:\n----------------------------------------\nPrompt: Explain the benefits of streaming in large language models.\nResponse stream: \nâŒ Streaming error: HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\nâŒ Fallback also failed: HTTPConnectionPool(host='127.0.0.1', port=8091): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8091): Failed to establish a new connection: [Errno 111] Connection refused\"))\n\nğŸ’­ Testing streaming for Phi-3-mini-4k:\n----------------------------------------\nPrompt: Explain the benefits of streaming in large language models.\nResponse stream: \nâŒ Streaming error: HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\nâŒ Fallback also failed: HTTPConnectionPool(host='127.0.0.1', port=8092): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8092): Failed to establish a new connection: [Errno 111] Connection refused\"))\n","output_type":"stream"}],"execution_count":11},{"id":"7cb4eb35","cell_type":"markdown","source":"## Step 12: Memory Tips","metadata":{}},{"id":"24e3f2b4-1529-45a6-892d-72f2498f4d80","cell_type":"markdown","source":"Demonstrates memory optimization techniques (batch size, context size) for maximizing large model performance on dual T4.","metadata":{}},{"id":"6ce9cfef-7f38-4288-a54c-866e9c399e92","cell_type":"markdown","source":"Tests prompt caching and KV cache optimization techniques to improve throughput for repeated or similar prompts.","metadata":{}},{"id":"b7da0a8e","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ’¡ MEMORY OPTIMIZATION TIPS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nğŸ”¹ Context Size Impact:\n\n   Context   â”‚ KV Cache per GPU â”‚ Recommended for 70B\n   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   512       â”‚ ~0.5 GB          â”‚ âœ… Safe\n   2048      â”‚ ~1.5 GB          â”‚ âœ… Default\n   4096      â”‚ ~3 GB            â”‚ âš ï¸ Tight fit\n   8192      â”‚ ~6 GB            â”‚ âŒ May OOM\n\nğŸ”¹ Batch Size Impact:\n\n   n_batch â”‚ Memory â”‚ Speed\n   â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€\n   64      â”‚ Lower  â”‚ Slower\n   128     â”‚ Medium â”‚ Medium (recommended)\n   256     â”‚ Higher â”‚ Faster\n   512     â”‚ High   â”‚ Fastest (may OOM)\n\nğŸ”¹ If Running Out of Memory:\n\n   1. Reduce context_size (2048 â†’ 1024)\n   2. Reduce n_batch (128 â†’ 64)\n   3. Use smaller quantization (IQ3_XS â†’ IQ2_XXS)\n   4. Keep some layers on CPU (n_gpu_layers=60)\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T03:07:42.888438Z","iopub.execute_input":"2026-02-05T03:07:42.889101Z","iopub.status.idle":"2026-02-05T03:07:42.894118Z","shell.execute_reply.started":"2026-02-05T03:07:42.889070Z","shell.execute_reply":"2026-02-05T03:07:42.893400Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¡ MEMORY OPTIMIZATION TIPS\n======================================================================\n\nğŸ”¹ Context Size Impact:\n\n   Context   â”‚ KV Cache per GPU â”‚ Recommended for 70B\n   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   512       â”‚ ~0.5 GB          â”‚ âœ… Safe\n   2048      â”‚ ~1.5 GB          â”‚ âœ… Default\n   4096      â”‚ ~3 GB            â”‚ âš ï¸ Tight fit\n   8192      â”‚ ~6 GB            â”‚ âŒ May OOM\n\nğŸ”¹ Batch Size Impact:\n\n   n_batch â”‚ Memory â”‚ Speed\n   â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€\n   64      â”‚ Lower  â”‚ Slower\n   128     â”‚ Medium â”‚ Medium (recommended)\n   256     â”‚ Higher â”‚ Faster\n   512     â”‚ High   â”‚ Fastest (may OOM)\n\nğŸ”¹ If Running Out of Memory:\n\n   1. Reduce context_size (2048 â†’ 1024)\n   2. Reduce n_batch (128 â†’ 64)\n   3. Use smaller quantization (IQ3_XS â†’ IQ2_XXS)\n   4. Keep some layers on CPU (n_gpu_layers=60)\n\n","output_type":"stream"}],"execution_count":21},{"id":"aae53d5b","cell_type":"markdown","source":"## Step 13: Cleanup","metadata":{}},{"id":"7bdac5db-cadf-4680-8e30-e628f8f8a099","cell_type":"markdown","source":"Stops server and verifies both GPUs have released all VRAM after large model deployment.","metadata":{}},{"id":"0325ffe1-2b67-423b-9db5-43b04b9b006a","cell_type":"markdown","source":"Explores split-mode options (layer vs row splitting) for tensor parallelism showing performance implications of different strategies.","metadata":{}},{"id":"ca050623","cell_type":"code","source":"print(\"ğŸ›‘ Stopping server...\")\nserver.stop_server()\nprint(\"âœ… Server stopped\")\n\n# Check memory freed\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:33:03.512414Z","iopub.execute_input":"2026-01-20T23:33:03.513090Z","iopub.status.idle":"2026-01-20T23:33:03.854773Z","shell.execute_reply.started":"2026-01-20T23:33:03.513064Z","shell.execute_reply":"2026-01-20T23:33:03.853960Z"}},"outputs":[{"name":"stdout","text":"ğŸ›‘ Stopping server...\nâœ… Server stopped\nindex, memory.used [MiB], memory.free [MiB]\n0, 3 MiB, 15093 MiB\n1, 3 MiB, 15093 MiB\n","output_type":"stream"}],"execution_count":24},{"id":"44124f93","cell_type":"markdown","source":"## ğŸ“š Summary\n\n### Model Recommendations for Kaggle Dual T4:\n\n| Goal | Model | Quantization | VRAM | Speed |\n|------|-------|-------------|------|-------|\n| Best Speed | 7-8B | Q4_K_M | 5GB | 50+ tok/s |\n| Best Balance | 13B | Q4_K_M | 8GB | 30+ tok/s |\n\n\n### Key Configuration for 70B:\n```python\nServerConfig(\n    n_gpu_layers=999,\n    tensor_split=\"0.48,0.48\",\n    context_size=2048,  # Keep small\n    flash_attn=True,\n    n_batch=128,        # Keep moderate\n)\n```\n\n### I-Quant Selection:\n- **IQ3_XS** - Best quality that fits dual T4\n- **IQ2_XXS** - More headroom, lower quality\n\n---\n\n**Next:** [10-complete-workflow](10-complete-workflow-llamatelemetry-v0.1.0.ipynb)","metadata":{}},{"id":"c1193fab-4757-4124-8e37-5681d8386efb","cell_type":"markdown","source":"Stops llama-server and verifies both GPUs have released all VRAM after large model deployment testing.","metadata":{}},{"id":"daa89110-21b2-447a-bc9f-ea149d25bbc3","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}