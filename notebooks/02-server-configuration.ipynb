{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Server Configuration\n",
    "\n",
    "**Duration:** ~15 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook explores the full server configuration surface — presets, custom\n",
    "parameters, health monitoring, tensor-split modes, and performance benchmarking.\n",
    "\n",
    "### What you'll learn\n",
    "1. Use built-in server presets for Kaggle\n",
    "2. Start servers with custom configuration\n",
    "3. Monitor server health\n",
    "4. Choose tensor-split modes for dual GPUs\n",
    "5. Benchmark inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "\n",
    "import llamatelemetry\n",
    "llamatelemetry.init(service_name=\"server-config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server Presets for Kaggle\n",
    "\n",
    "llamatelemetry ships with optimized presets for common GPU configurations.\n",
    "Each preset specifies context size, batch size, tensor split, and other parameters\n",
    "tuned for that hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.kaggle import ServerPreset, get_preset_config\n",
    "\n",
    "# List all available presets\n",
    "for preset in ServerPreset:\n",
    "    config = get_preset_config(preset)\n",
    "    print(f\"{preset.name:20s}  ctx={config.ctx_size:>5d}  batch={config.batch_size:>4d}  \"\n",
    "          f\"gpu_layers={config.gpu_layers:>3d}  split={config.tensor_split or 'none'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start with Presets\n",
    "\n",
    "`quick_start()` is a one-liner that selects the right preset, starts the server,\n",
    "and waits for readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llamatelemetry.llama import quick_start\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "mgr = quick_start(model_path, preset=\"kaggle_t4_dual\")\n",
    "print(\"Server started with Kaggle dual T4 preset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Configuration\n",
    "\n",
    "For fine-grained control, pass individual parameters to `start_server()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import ServerManager\n",
    "\n",
    "# Stop the preset server first\n",
    "mgr.stop_server()\n",
    "\n",
    "# Start with custom parameters\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(\n",
    "    model_path=model_path,\n",
    "    port=8090,\n",
    "    gpu_layers=99,\n",
    "    ctx_size=4096,         # larger context window\n",
    "    batch_size=1024,       # larger batch for throughput\n",
    "    ubatch_size=256,       # micro-batch for memory efficiency\n",
    "    n_parallel=2,          # concurrent request slots\n",
    "    tensor_split=\"0.5,0.5\",  # equal split across 2 GPUs\n",
    "    flash_attn=True,       # enable flash attention\n",
    ")\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "print(\"Custom server ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Monitoring\n",
    "\n",
    "Use the `LlamaCppClient` to query server health, slot status, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import LlamaCppClient\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Health status\n",
    "health = client.health()\n",
    "print(f\"Status: {health.status}\")\n",
    "print(f\"Idle slots: {health.slots_idle}, Processing: {health.slots_processing}\")\n",
    "\n",
    "# Server properties\n",
    "props = client.props()\n",
    "print(f\"\\nServer properties:\")\n",
    "for k, v in props.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Slot details\n",
    "slots = client.slots.list()\n",
    "for slot in slots:\n",
    "    print(f\"\\nSlot {slot.id}: processing={slot.is_processing}, ctx={slot.n_ctx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Split Modes\n",
    "\n",
    "On dual-GPU systems, tensor-split controls how model layers are distributed.\n",
    "llamatelemetry provides named modes for common configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.kaggle import TensorSplitMode\n",
    "\n",
    "print(\"Available tensor-split modes:\")\n",
    "for mode in TensorSplitMode:\n",
    "    split_str = mode.to_string() or \"N/A\"\n",
    "    print(f\"  {mode.name:15s}  → {split_str}\")\n",
    "\n",
    "# Recommended for dual T4:\n",
    "print(f\"\\nRecommended for dual T4: {TensorSplitMode.DUAL_50_50.value}\")\n",
    "print(f\"For LLM+RAPIDS split: {TensorSplitMode.NONE.value} (single GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Measure inference latency and throughput by running multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompts = [\n",
    "    \"Explain the difference between Q4_K_M and Q5_K_M quantization.\",\n",
    "    \"What are the benefits of tensor parallelism for LLM inference?\",\n",
    "    \"How does flash attention reduce memory usage?\",\n",
    "    \"Describe the GGUF file format in one paragraph.\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for prompt in prompts:\n",
    "    t0 = time.perf_counter()\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=128,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    elapsed_ms = (time.perf_counter() - t0) * 1000\n",
    "    tokens = resp.usage.completion_tokens\n",
    "    tps = tokens / (elapsed_ms / 1000) if elapsed_ms > 0 else 0\n",
    "    results.append((elapsed_ms, tokens, tps))\n",
    "    print(f\"  {elapsed_ms:7.0f} ms | {tokens:3d} tokens | {tps:5.1f} tok/s\")\n",
    "\n",
    "avg_ms = sum(r[0] for r in results) / len(results)\n",
    "avg_tps = sum(r[2] for r in results) / len(results)\n",
    "print(f\"\\nAverage: {avg_ms:.0f} ms, {avg_tps:.1f} tok/s\")\n",
    "\n",
    "# Cleanup\n",
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}