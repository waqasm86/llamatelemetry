{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embedding Visualization\n",
    "\n",
    "**Duration:** ~30 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook explores **text embeddings** — generating embeddings with the\n",
    "server's embedding endpoint, applying dimensionality reduction for visualization,\n",
    "clustering, and semantic similarity search.\n",
    "\n",
    "### What you'll learn\n",
    "1. Generate text embeddings\n",
    "2. Dimensionality reduction (PCA/t-SNE)\n",
    "3. Semantic clustering\n",
    "4. Similarity search\n",
    "5. Embedding quality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "!pip install -q matplotlib scikit-learn\n",
    "\n",
    "import llamatelemetry\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llamatelemetry.init(service_name=\"embedding-viz\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# Start server with embedding support\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "Use the server's embedding endpoint to create vector representations\n",
    "of diverse texts across multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Diverse texts across categories\n",
    "texts = {\n",
    "    \"ML\": [\n",
    "        \"Neural networks learn representations from data through backpropagation.\",\n",
    "        \"Gradient descent optimizes model parameters by minimizing the loss function.\",\n",
    "        \"Convolutional neural networks excel at image classification tasks.\",\n",
    "        \"Transformer models use self-attention for sequence processing.\",\n",
    "        \"Transfer learning adapts pre-trained models to new downstream tasks.\",\n",
    "    ],\n",
    "    \"GPU\": [\n",
    "        \"CUDA enables massively parallel computation on NVIDIA GPUs.\",\n",
    "        \"Tensor cores accelerate matrix multiplication operations.\",\n",
    "        \"GPU memory bandwidth is crucial for large model inference.\",\n",
    "        \"Multi-GPU training distributes model and data across devices.\",\n",
    "        \"Flash attention reduces memory usage for long sequences.\",\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        \"Photosynthesis converts sunlight into chemical energy in plants.\",\n",
    "        \"DNA replication ensures genetic information is copied accurately.\",\n",
    "        \"The periodic table organizes elements by atomic number.\",\n",
    "        \"Quantum mechanics describes behavior at subatomic scales.\",\n",
    "        \"Evolution through natural selection drives species adaptation.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Generate embeddings\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "all_embeddings = []\n",
    "\n",
    "for category, category_texts in texts.items():\n",
    "    for text in category_texts:\n",
    "        emb = client.embed(text)\n",
    "        all_texts.append(text)\n",
    "        all_labels.append(category)\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(all_embeddings)\n",
    "print(f\"Generated {len(embeddings)} embeddings, dimension={embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Reduce high-dimensional embeddings to 2D for visualization using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "@llamatelemetry.task(name=\"dimensionality-reduction\")\n",
    "def reduce_dimensions(embeddings, labels):\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(embeddings) - 1))\n",
    "    tsne_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    return pca_2d, tsne_2d, pca.explained_variance_ratio_\n",
    "\n",
    "pca_2d, tsne_2d, variance_ratio = reduce_dimensions(embeddings, all_labels)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "colors = {\"ML\": \"blue\", \"GPU\": \"green\", \"Science\": \"red\"}\n",
    "\n",
    "for label in set(all_labels):\n",
    "    mask = [l == label for l in all_labels]\n",
    "    axes[0].scatter(pca_2d[mask, 0], pca_2d[mask, 1], c=colors[label], label=label, s=60, alpha=0.8)\n",
    "    axes[1].scatter(tsne_2d[mask, 0], tsne_2d[mask, 1], c=colors[label], label=label, s=60, alpha=0.8)\n",
    "\n",
    "axes[0].set_title(f\"PCA (var explained: {sum(variance_ratio):.1%})\")\n",
    "axes[0].legend()\n",
    "axes[1].set_title(\"t-SNE\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Embedding Space Visualization\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Clustering\n",
    "\n",
    "Apply K-means clustering to discover natural groupings in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "@llamatelemetry.task(name=\"semantic-clustering\")\n",
    "def cluster_embeddings(embeddings, n_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    return cluster_labels, kmeans.inertia_\n",
    "\n",
    "cluster_labels, inertia = cluster_embeddings(embeddings, n_clusters=3)\n",
    "\n",
    "# Compare clusters with actual categories\n",
    "print(\"Clustering Results:\")\n",
    "print(f\"{'Text':<60} {'True':<8} {'Cluster'}\")\n",
    "print(\"-\" * 75)\n",
    "for text, true_label, cluster in zip(all_texts, all_labels, cluster_labels):\n",
    "    print(f\"{text[:57]+'...' if len(text) > 57 else text:<60} {true_label:<8} {cluster}\")\n",
    "\n",
    "# Cluster purity\n",
    "from collections import Counter\n",
    "for c in range(3):\n",
    "    cluster_labels_list = [all_labels[i] for i in range(len(all_labels)) if cluster_labels[i] == c]\n",
    "    most_common = Counter(cluster_labels_list).most_common(1)[0] if cluster_labels_list else (\"N/A\", 0)\n",
    "    purity = most_common[1] / len(cluster_labels_list) if cluster_labels_list else 0\n",
    "    print(f\"\\nCluster {c}: {len(cluster_labels_list)} items, majority={most_common[0]}, purity={purity:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity Search\n",
    "\n",
    "Find the most similar documents to a query using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.embeddings import cosine_similarity\n",
    "\n",
    "queries = [\n",
    "    \"How do GPUs accelerate machine learning?\",\n",
    "    \"What is the role of attention in transformers?\",\n",
    "    \"Tell me about biological processes.\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    query_emb = np.array(client.embed(query))\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        sim = cosine_similarity(query_emb, emb)\n",
    "        similarities.append((sim, all_texts[i], all_labels[i]))\n",
    "\n",
    "    # Top 3 results\n",
    "    similarities.sort(reverse=True)\n",
    "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "    for sim, text, label in similarities[:3]:\n",
    "        print(f\"  [{label}] {sim:.3f} — {text[:70]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Quality Analysis\n",
    "\n",
    "Analyze the embedding space properties: inter-cluster distance, intra-cluster\n",
    "cohesion, and isotropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter-category similarity (should be low)\n",
    "# Intra-category similarity (should be high)\n",
    "categories = list(texts.keys())\n",
    "\n",
    "print(f\"{'Cat1':<10} {'Cat2':<10} {'Avg Similarity'}\")\n",
    "print(\"-\" * 35)\n",
    "for i, cat1 in enumerate(categories):\n",
    "    for j, cat2 in enumerate(categories):\n",
    "        if j < i:\n",
    "            continue\n",
    "        mask1 = [l == cat1 for l in all_labels]\n",
    "        mask2 = [l == cat2 for l in all_labels]\n",
    "        emb1 = embeddings[mask1]\n",
    "        emb2 = embeddings[mask2]\n",
    "\n",
    "        sims = []\n",
    "        for e1 in emb1:\n",
    "            for e2 in emb2:\n",
    "                if not np.array_equal(e1, e2):\n",
    "                    sims.append(cosine_similarity(e1, e2))\n",
    "\n",
    "        avg_sim = np.mean(sims) if sims else 0\n",
    "        relation = \"intra\" if cat1 == cat2 else \"inter\"\n",
    "        print(f\"{cat1:<10} {cat2:<10} {avg_sim:.3f} ({relation})\")\n",
    "\n",
    "# Isotropy: how uniformly distributed are embeddings in space\n",
    "centroid = np.mean(embeddings, axis=0)\n",
    "dists = [np.linalg.norm(e - centroid) for e in embeddings]\n",
    "print(f\"\\nEmbedding space statistics:\")\n",
    "print(f\"  Mean distance from centroid: {np.mean(dists):.3f}\")\n",
    "print(f\"  Std distance: {np.std(dists):.3f}\")\n",
    "print(f\"  Dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- **Embedding generation** via the llama-server embedding endpoint\n",
    "- **Visualization** with PCA and t-SNE dimensionality reduction\n",
    "- **K-means clustering** for unsupervised topic discovery\n",
    "- **Similarity search** using cosine similarity\n",
    "- **Quality analysis** of the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}