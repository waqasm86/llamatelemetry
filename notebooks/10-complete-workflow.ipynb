{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Production Workflow\n",
    "\n",
    "**Duration:** ~45 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook demonstrates a **complete production workflow**: model selection,\n",
    "server deployment, inference pipelines, multi-turn chat, batch processing,\n",
    "GPU monitoring, and observability export.\n",
    "\n",
    "### What you'll learn\n",
    "1. Model selection and registry\n",
    "2. Optimal server deployment\n",
    "3. Traced inference pipelines\n",
    "4. Multi-turn chat with ChatEngine\n",
    "5. Batch processing with session tracking\n",
    "6. GPU monitoring and observability\n",
    "7. Graceful shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "\n",
    "import llamatelemetry\n",
    "\n",
    "# Initialize with OTLP endpoint (optional — set for Grafana Cloud export)\n",
    "llamatelemetry.init(\n",
    "    service_name=\"production-workflow\",\n",
    "    environment=\"kaggle\",\n",
    "    # otlp_endpoint=\"https://otlp-gateway-prod-us-central-0.grafana.net/otlp\",\n",
    "    # otlp_headers={\"Authorization\": \"Basic <token>\"},\n",
    ")\n",
    "print(f\"llamatelemetry {llamatelemetry.version()} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Model Selection\n",
    "\n",
    "Choose the best model for your task and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llamatelemetry.llama import parse_gguf_header, get_model_summary\n",
    "\n",
    "@llamatelemetry.task(name=\"select-model\")\n",
    "def select_model():\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "        filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "        cache_dir=\"/root/.cache/huggingface\",\n",
    "    )\n",
    "\n",
    "    info = parse_gguf_header(model_path)\n",
    "    print(f\"Selected model: {get_model_summary(model_path)}\")\n",
    "    print(f\"  Architecture: {info.metadata.architecture}\")\n",
    "    print(f\"  Size: {info.size_mb:.0f} MB\")\n",
    "    print(f\"  Context: {info.metadata.context_length}\")\n",
    "    return model_path\n",
    "\n",
    "model_path = select_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Server Deployment\n",
    "\n",
    "Deploy with optimal settings for dual T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "\n",
    "@llamatelemetry.task(name=\"deploy-server\")\n",
    "def deploy_server(model_path):\n",
    "    mgr = ServerManager()\n",
    "    mgr.start_server(\n",
    "        model_path=model_path,\n",
    "        gpu_layers=99,\n",
    "        tensor_split=\"0.5,0.5\",\n",
    "        ctx_size=2048,\n",
    "        batch_size=512,\n",
    "        n_parallel=2,\n",
    "    )\n",
    "    mgr.wait_until_ready(timeout=60)\n",
    "\n",
    "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "    health = client.health()\n",
    "    print(f\"Server deployed: {health.status} ({health.slots_idle} idle slots)\")\n",
    "    return mgr, client\n",
    "\n",
    "mgr, client = deploy_server(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Inference Pipeline\n",
    "\n",
    "Build a traced inference pipeline with prompt → completion → streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@llamatelemetry.workflow(name=\"inference-pipeline\")\n",
    "def run_inference(client, prompt, max_tokens=128):\n",
    "    \"\"\"Traced inference pipeline.\"\"\"\n",
    "    with llamatelemetry.span(\"prepare-request\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    with llamatelemetry.span(\"llm-completion\") as span:\n",
    "        t0 = time.perf_counter()\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=messages, max_tokens=max_tokens, temperature=0.7,\n",
    "        )\n",
    "        latency_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    with llamatelemetry.span(\"process-response\"):\n",
    "        result = {\n",
    "            \"text\": resp.choices[0].message.content,\n",
    "            \"tokens\": resp.usage.completion_tokens,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"tokens_per_sec\": resp.usage.completion_tokens / (latency_ms / 1000),\n",
    "        }\n",
    "\n",
    "    return result\n",
    "\n",
    "result = run_inference(client, \"What are the key components of a transformer architecture?\")\n",
    "print(f\"Response ({result['tokens']} tokens, {result['latency_ms']:.0f} ms, {result['tokens_per_sec']:.1f} tok/s):\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Multi-Turn Chat\n",
    "\n",
    "`ChatEngine` manages conversation history and context automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.chat import ChatEngine\n",
    "\n",
    "chat = ChatEngine(\n",
    "    engine=client,\n",
    "    system_prompt=\"You are a helpful AI assistant specializing in machine learning.\",\n",
    "    max_history=10,\n",
    "    max_tokens=128,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Multi-turn conversation\n",
    "turns = [\n",
    "    \"What is gradient descent?\",\n",
    "    \"How does the learning rate affect it?\",\n",
    "    \"What's a good default learning rate?\",\n",
    "]\n",
    "\n",
    "for turn in turns:\n",
    "    chat.add_user_message(turn)\n",
    "    response = chat.complete()\n",
    "    chat.add_assistant_message(response)\n",
    "    print(f\"User: {turn}\")\n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "print(f\"Chat history: {len(chat.get_history())} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Batch Processing\n",
    "\n",
    "Process multiple prompts with session-level tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.workflow(name=\"batch-inference\")\n",
    "def batch_process(client, prompts):\n",
    "    results = []\n",
    "    with llamatelemetry.session(\"batch-job-001\"):\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            with llamatelemetry.span(f\"request-{i}\", prompt_index=i):\n",
    "                resp = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=64, temperature=0.7,\n",
    "                )\n",
    "                results.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": resp.choices[0].message.content,\n",
    "                    \"tokens\": resp.usage.completion_tokens,\n",
    "                })\n",
    "    return results\n",
    "\n",
    "batch_prompts = [\n",
    "    \"Define overfitting in one sentence.\",\n",
    "    \"What is regularization?\",\n",
    "    \"Explain dropout in neural networks.\",\n",
    "    \"What is batch normalization?\",\n",
    "    \"Define the bias-variance tradeoff.\",\n",
    "]\n",
    "\n",
    "results = batch_process(client, batch_prompts)\n",
    "for r in results:\n",
    "    print(f\"Q: {r['prompt']}\")\n",
    "    print(f\"A: {r['response'][:100]}... ({r['tokens']} tokens)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — GPU Monitoring\n",
    "\n",
    "Continuous GPU monitoring with background sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.gpu import start_sampler, snapshot\n",
    "\n",
    "# Start background monitoring\n",
    "handle = start_sampler(interval_ms=500)\n",
    "\n",
    "# Run a workload\n",
    "for _ in range(5):\n",
    "    client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Write a detailed explanation of backpropagation.\"}],\n",
    "        max_tokens=128,\n",
    "    )\n",
    "\n",
    "handle.stop()\n",
    "samples = handle.get_snapshots()\n",
    "\n",
    "# Analyze\n",
    "if samples:\n",
    "    print(f\"Collected {len(samples)} GPU samples\")\n",
    "    for gpu_id in [0, 1]:\n",
    "        gpu_samples = [s for s in samples if s.gpu_id == gpu_id]\n",
    "        if gpu_samples:\n",
    "            mem_values = [s.mem_used_mb for s in gpu_samples]\n",
    "            util_values = [s.utilization_pct for s in gpu_samples]\n",
    "            print(f\"  GPU {gpu_id}: mem {min(mem_values)}-{max(mem_values)} MB, util {min(util_values)}-{max(util_values)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Observability Export\n",
    "\n",
    "Export traces and metrics to Grafana Cloud (or any OTLP-compatible backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.kaggle import auto_configure_grafana_cloud\n",
    "\n",
    "# Configure Grafana Cloud (uses Kaggle secrets: GRAFANA_CLOUD_ORG_ID, GRAFANA_CLOUD_API_TOKEN)\n",
    "try:\n",
    "    configured = auto_configure_grafana_cloud()\n",
    "    if configured:\n",
    "        print(\"Grafana Cloud configured — traces will be exported\")\n",
    "    else:\n",
    "        print(\"Grafana Cloud not configured — set Kaggle secrets to enable\")\n",
    "except Exception as e:\n",
    "    print(f\"Grafana Cloud setup: {e}\")\n",
    "\n",
    "# Flush pending telemetry\n",
    "llamatelemetry.flush(timeout_s=5.0)\n",
    "print(\"Telemetry flushed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 — Graceful Shutdown\n",
    "\n",
    "Always shut down cleanly to flush telemetry and release GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status\n",
    "health = client.health()\n",
    "print(f\"Server status: {health.status}\")\n",
    "\n",
    "mem = snapshot()\n",
    "for s in mem:\n",
    "    print(f\"GPU {s.gpu_id}: {s.mem_used_mb}/{s.mem_total_mb} MB\")\n",
    "\n",
    "# Shutdown sequence\n",
    "mgr.stop_server()          # Stop llama-server\n",
    "llamatelemetry.flush()     # Flush remaining telemetry\n",
    "llamatelemetry.shutdown()  # Release SDK resources\n",
    "print(\"\\nProduction workflow complete — all resources released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Checklist\n",
    "\n",
    "- [x] Model selected and validated with `parse_gguf_header()`\n",
    "- [x] Server deployed with optimal tensor-split\n",
    "- [x] Inference pipeline traced with `@workflow` / `@task`\n",
    "- [x] Multi-turn chat with `ChatEngine`\n",
    "- [x] Batch processing with `session()` tracking\n",
    "- [x] GPU monitoring with `start_sampler()`\n",
    "- [x] Observability export to Grafana Cloud\n",
    "- [x] Graceful shutdown with `flush()` + `shutdown()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Diagram\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────────┐     ┌─────────────────┐\n",
    "│  Notebook    │────▶│  llamatelemetry  │────▶│  Grafana Cloud  │\n",
    "│  (Client)    │     │  SDK             │     │  (Traces/Metrics)│\n",
    "└─────────────┘     └──────────────────┘     └─────────────────┘\n",
    "       │                     │\n",
    "       │              ┌──────┴──────┐\n",
    "       │              │             │\n",
    "       ▼              ▼             ▼\n",
    "┌─────────────┐ ┌──────────┐ ┌──────────┐\n",
    "│ llama-server│ │  GPU 0   │ │  GPU 1   │\n",
    "│ (port 8090) │ │ (T4 15GB)│ │ (T4 15GB)│\n",
    "└─────────────┘ └──────────┘ └──────────┘\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}