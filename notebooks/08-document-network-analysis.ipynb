{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Network Analysis\n",
    "\n",
    "**Duration:** ~30 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook builds a **document similarity network** using embeddings,\n",
    "performs network analysis (centrality, community detection), and uses\n",
    "the LLM to summarize discovered clusters.\n",
    "\n",
    "### What you'll learn\n",
    "1. Generate document embeddings\n",
    "2. Build a similarity network\n",
    "3. Run network analysis (centrality, Louvain communities)\n",
    "4. Visualize with Graphistry\n",
    "5. LLM-powered cluster summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "\n",
    "import llamatelemetry\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from llamatelemetry.kaggle import rapids_gpu\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llamatelemetry.init(service_name=\"doc-network\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"1.0,0.0\", ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Document Embeddings\n",
    "\n",
    "Use the server's embedding endpoint to create vector representations of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from llamatelemetry.embeddings import cosine_similarity\n",
    "\n",
    "documents = [\n",
    "    {\"id\": \"D1\", \"title\": \"Neural Networks\", \"text\": \"Neural networks are computing systems inspired by biological neural networks in the brain.\"},\n",
    "    {\"id\": \"D2\", \"title\": \"Deep Learning\", \"text\": \"Deep learning uses multiple layers of neural networks to learn representations of data.\"},\n",
    "    {\"id\": \"D3\", \"title\": \"Transformers\", \"text\": \"Transformer architecture uses self-attention mechanisms for sequence-to-sequence tasks.\"},\n",
    "    {\"id\": \"D4\", \"title\": \"CUDA Programming\", \"text\": \"CUDA enables parallel computing on NVIDIA GPUs for scientific and ML workloads.\"},\n",
    "    {\"id\": \"D5\", \"title\": \"GPU Memory\", \"text\": \"GPU memory management is critical for training large models that exceed VRAM capacity.\"},\n",
    "    {\"id\": \"D6\", \"title\": \"Quantization\", \"text\": \"Model quantization reduces precision of weights to fit larger models in limited memory.\"},\n",
    "    {\"id\": \"D7\", \"title\": \"Fine-tuning\", \"text\": \"Fine-tuning adapts pre-trained models to specific tasks using domain-specific data.\"},\n",
    "    {\"id\": \"D8\", \"title\": \"LoRA\", \"text\": \"Low-Rank Adaptation trains small adapter matrices instead of updating all model weights.\"},\n",
    "    {\"id\": \"D9\", \"title\": \"Inference\", \"text\": \"LLM inference optimizations include batching, KV caching, and speculative decoding.\"},\n",
    "    {\"id\": \"D10\", \"title\": \"Observability\", \"text\": \"OpenTelemetry provides distributed tracing and metrics for monitoring ML pipelines.\"},\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for doc in documents:\n",
    "    emb = client.embed(doc[\"text\"])\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Generated {len(embeddings)} embeddings, dimension={embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Document Network\n",
    "\n",
    "Create edges between documents whose cosine similarity exceeds a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Compute pairwise similarity matrix\n",
    "n = len(documents)\n",
    "sim_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        sim_matrix[i][j] = sim\n",
    "        sim_matrix[j][i] = sim\n",
    "\n",
    "# Build edges above threshold\n",
    "edges = []\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        if sim_matrix[i][j] >= SIMILARITY_THRESHOLD:\n",
    "            edges.append({\n",
    "                \"src\": documents[i][\"id\"],\n",
    "                \"dst\": documents[j][\"id\"],\n",
    "                \"similarity\": round(sim_matrix[i][j], 3),\n",
    "            })\n",
    "\n",
    "edge_df = pd.DataFrame(edges)\n",
    "node_df = pd.DataFrame([{\"id\": d[\"id\"], \"title\": d[\"title\"]} for d in documents])\n",
    "\n",
    "print(f\"Document network: {len(node_df)} nodes, {len(edge_df)} edges (threshold={SIMILARITY_THRESHOLD})\")\n",
    "if len(edge_df) > 0:\n",
    "    print(edge_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "Compute centrality metrics and detect communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.task(name=\"centrality-analysis\")\n",
    "def compute_centrality(node_df, edge_df):\n",
    "    \"\"\"Compute degree centrality for each document.\"\"\"\n",
    "    degree = {row[\"id\"]: 0 for _, row in node_df.iterrows()}\n",
    "    for _, row in edge_df.iterrows():\n",
    "        degree[row[\"src\"]] = degree.get(row[\"src\"], 0) + 1\n",
    "        degree[row[\"dst\"]] = degree.get(row[\"dst\"], 0) + 1\n",
    "\n",
    "    max_d = max(degree.values()) if degree and max(degree.values()) > 0 else 1\n",
    "    centrality = {k: v / max_d for k, v in degree.items()}\n",
    "    return centrality\n",
    "\n",
    "@llamatelemetry.task(name=\"community-detection\")\n",
    "def detect_communities(node_df, edge_df):\n",
    "    \"\"\"Simple community detection via connected components.\"\"\"\n",
    "    # Build adjacency\n",
    "    adj = {row[\"id\"]: set() for _, row in node_df.iterrows()}\n",
    "    for _, row in edge_df.iterrows():\n",
    "        adj[row[\"src\"]].add(row[\"dst\"])\n",
    "        adj[row[\"dst\"]].add(row[\"src\"])\n",
    "\n",
    "    # BFS to find connected components\n",
    "    visited = set()\n",
    "    communities = {}\n",
    "    community_id = 0\n",
    "    for node in adj:\n",
    "        if node not in visited:\n",
    "            queue = [node]\n",
    "            while queue:\n",
    "                current = queue.pop(0)\n",
    "                if current not in visited:\n",
    "                    visited.add(current)\n",
    "                    communities[current] = community_id\n",
    "                    queue.extend(adj[current] - visited)\n",
    "            community_id += 1\n",
    "    return communities\n",
    "\n",
    "centrality = compute_centrality(node_df, edge_df)\n",
    "communities = detect_communities(node_df, edge_df)\n",
    "\n",
    "print(f\"{'Document':<6} {'Title':<20} {'Centrality':<12} {'Community'}\")\n",
    "print(\"-\" * 50)\n",
    "for doc in documents:\n",
    "    did = doc[\"id\"]\n",
    "    print(f\"{did:<6} {doc['title']:<20} {centrality.get(did, 0):<12.2f} {communities.get(did, -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rapids_gpu(1):\n",
    "    try:\n",
    "        import graphistry\n",
    "\n",
    "        # Enrich node data\n",
    "        node_df[\"centrality\"] = node_df[\"id\"].map(centrality)\n",
    "        node_df[\"community\"] = node_df[\"id\"].map(communities)\n",
    "        node_df[\"size\"] = (node_df[\"centrality\"] * 25 + 5).astype(int)\n",
    "\n",
    "        g = (graphistry\n",
    "             .edges(edge_df, \"src\", \"dst\")\n",
    "             .nodes(node_df, \"id\")\n",
    "             .bind(point_title=\"title\", point_size=\"size\", edge_weight=\"similarity\")\n",
    "             .encode_point_color(\"community\", palette=[\"blue\", \"green\", \"red\", \"orange\", \"purple\"]))\n",
    "        g.plot()\n",
    "    except Exception as e:\n",
    "        print(f\"Graphistry: {e}\")\n",
    "        # Fallback: text summary\n",
    "        community_groups = {}\n",
    "        for doc in documents:\n",
    "            cid = communities.get(doc[\"id\"], -1)\n",
    "            community_groups.setdefault(cid, []).append(doc[\"title\"])\n",
    "        for cid, titles in sorted(community_groups.items()):\n",
    "            print(f\"  Community {cid}: {', '.join(titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-Powered Cluster Summarization\n",
    "\n",
    "Use the LLM to generate a summary for each discovered community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group documents by community\n",
    "community_docs = {}\n",
    "for doc in documents:\n",
    "    cid = communities.get(doc[\"id\"], -1)\n",
    "    community_docs.setdefault(cid, []).append(doc)\n",
    "\n",
    "for cid, docs in sorted(community_docs.items()):\n",
    "    doc_texts = \"\\n\".join(f\"- {d['title']}: {d['text']}\" for d in docs)\n",
    "\n",
    "    with llamatelemetry.span(\"summarize-cluster\", community_id=cid):\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize the common theme of these documents in one sentence:\\n{doc_texts}\"\n",
    "            }],\n",
    "            max_tokens=64, temperature=0.3,\n",
    "        )\n",
    "        summary = resp.choices[0].message.content\n",
    "\n",
    "    titles = [d[\"title\"] for d in docs]\n",
    "    print(f\"Community {cid} ({', '.join(titles)}):\")\n",
    "    print(f\"  {summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- **Embedding-based similarity** networks from document collections\n",
    "- **Network analysis**: degree centrality and community detection\n",
    "- **GPU-accelerated visualization** with Graphistry\n",
    "- **LLM summarization** of discovered clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}