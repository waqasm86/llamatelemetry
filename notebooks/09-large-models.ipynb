{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Large Models (13B-70B) on Kaggle\n",
    "\n",
    "**Duration:** ~35 min | **Platform:** Kaggle dual Tesla T4 (2 × 15 GB VRAM)\n",
    "\n",
    "This notebook covers deploying **large models** (13B to 70B parameters) on Kaggle's\n",
    "dual T4 GPUs using aggressive quantization and optimal tensor-split configuration.\n",
    "\n",
    "### What you'll learn\n",
    "1. VRAM planning for large models\n",
    "2. Deploy 13B models with Q4_K_M\n",
    "3. Deploy 70B models with IQ3_XS\n",
    "4. Memory optimization techniques\n",
    "5. Context window tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "\n",
    "import llamatelemetry\n",
    "from llamatelemetry.gpu import list_devices, snapshot, start_sampler\n",
    "\n",
    "llamatelemetry.init(service_name=\"large-models\")\n",
    "\n",
    "# GPU inventory\n",
    "devices = list_devices()\n",
    "total_vram = sum(d.memory_total_mb for d in devices)\n",
    "print(f\"GPUs: {len(devices)}\")\n",
    "for d in devices:\n",
    "    print(f\"  GPU {d.id}: {d.name} — {d.memory_total_mb} MB\")\n",
    "print(f\"Total VRAM: {total_vram} MB ({total_vram/1024:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VRAM Planning for Large Models\n",
    "\n",
    "Use the model size formula: `VRAM ≈ params × bits_per_weight / 8 + KV_cache_overhead`\n",
    "\n",
    "On dual T4 (30 GB total), you need to account for:\n",
    "- Model weights\n",
    "- KV cache (grows with context length)\n",
    "- Runtime overhead (~500 MB per GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_vram(params_b, bpw, ctx_size=2048, n_layers=None, embed_dim=None):\n",
    "    \"\"\"Estimate total VRAM needed for a model.\"\"\"\n",
    "    model_gb = params_b * bpw / 8\n",
    "    # KV cache estimate: ~0.5 MB per 1K context per billion params (rough)\n",
    "    kv_cache_gb = (ctx_size / 1024) * params_b * 0.0005\n",
    "    overhead_gb = 1.0  # runtime overhead\n",
    "    return model_gb + kv_cache_gb + overhead_gb\n",
    "\n",
    "DUAL_T4_GB = 30.0\n",
    "\n",
    "scenarios = [\n",
    "    (\"Gemma-3 4B\",    4,  \"Q4_K_M\", 4.5, 2048),\n",
    "    (\"Llama-3.2 7B\",  7,  \"Q4_K_M\", 4.5, 2048),\n",
    "    (\"Llama-3.2 7B\",  7,  \"Q5_K_M\", 5.5, 4096),\n",
    "    (\"Llama-3.1 13B\", 13, \"Q4_K_M\", 4.5, 2048),\n",
    "    (\"Llama-3.1 13B\", 13, \"Q4_K_M\", 4.5, 4096),\n",
    "    (\"CodeLlama 34B\",  34, \"Q4_K_M\", 4.5, 2048),\n",
    "    (\"Llama-3.1 70B\", 70, \"IQ3_XS\", 3.3, 512),\n",
    "    (\"Llama-3.1 70B\", 70, \"IQ3_XS\", 3.3, 1024),\n",
    "    (\"Llama-3.1 70B\", 70, \"Q4_K_M\", 4.5, 2048),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<20} {'Quant':<10} {'Ctx':<6} {'Est VRAM':<10} {'Fits?':<6} Notes\")\n",
    "print(\"-\" * 80)\n",
    "for name, params, quant, bpw, ctx in scenarios:\n",
    "    vram = estimate_vram(params, bpw, ctx)\n",
    "    fits = vram <= DUAL_T4_GB\n",
    "    margin = DUAL_T4_GB - vram\n",
    "    note = f\"{margin:+.1f} GB margin\" if fits else \"DOES NOT FIT\"\n",
    "    print(f\"{name:<20} {quant:<10} {ctx:<6} {vram:<10.1f} {'Yes' if fits else 'NO':<6} {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying 13B Models\n",
    "\n",
    "A 13B Q4_K_M model (~7.3 GB) fits comfortably on dual T4 with room for large contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "from llamatelemetry.kaggle import TensorSplitMode\n",
    "\n",
    "# Download a 13B model (using a smaller model as proxy for this demo)\n",
    "# In production, replace with actual 13B model:\n",
    "#   repo_id=\"TheBloke/CodeLlama-13B-Instruct-GGUF\"\n",
    "#   filename=\"codellama-13b-instruct.Q4_K_M.gguf\"\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# Dual GPU with 50/50 split\n",
    "split = TensorSplitMode.DUAL_50_50\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(\n",
    "    model_path=model_path,\n",
    "    gpu_layers=99,\n",
    "    tensor_split=split.to_string(),\n",
    "    ctx_size=4096,\n",
    "    batch_size=512,\n",
    ")\n",
    "mgr.wait_until_ready(timeout=120)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Benchmark\n",
    "t0 = time.perf_counter()\n",
    "resp = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a Python function to compute Fibonacci numbers using dynamic programming.\"}],\n",
    "    max_tokens=256, temperature=0.7,\n",
    ")\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Response ({resp.usage.completion_tokens} tokens in {elapsed:.1f}s, {resp.usage.completion_tokens/elapsed:.1f} tok/s):\")\n",
    "print(resp.choices[0].message.content)\n",
    "\n",
    "# Memory distribution\n",
    "mem = snapshot()\n",
    "for s in mem:\n",
    "    print(f\"\\nGPU {s.gpu_id}: {s.mem_used_mb}/{s.mem_total_mb} MB\")\n",
    "\n",
    "mgr.stop_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying 70B Models with IQ3_XS\n",
    "\n",
    "A 70B IQ3_XS model (~28.9 GB) barely fits on dual T4. Key settings:\n",
    "- **Tensor split**: 50/50 across both GPUs\n",
    "- **Context**: 512-1024 max (KV cache is expensive at 70B)\n",
    "- **Batch size**: Small to reduce memory spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70B deployment configuration (download separately if needed)\n",
    "# model_70b = hf_hub_download(\n",
    "#     repo_id=\"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\",\n",
    "#     filename=\"Meta-Llama-3.1-70B-Instruct-IQ3_XS.gguf\",\n",
    "#     cache_dir=\"/root/.cache/huggingface\",\n",
    "# )\n",
    "\n",
    "# Optimal 70B configuration for dual T4:\n",
    "config_70b = {\n",
    "    \"gpu_layers\": 99,\n",
    "    \"tensor_split\": TensorSplitMode.DUAL_50_50.to_string(),\n",
    "    \"ctx_size\": 512,       # minimal context to save VRAM\n",
    "    \"batch_size\": 256,     # small batch to reduce peaks\n",
    "    \"ubatch_size\": 64,     # small micro-batch\n",
    "    \"n_parallel\": 1,       # single slot only\n",
    "    \"flash_attn\": True,    # save memory with flash attention\n",
    "}\n",
    "\n",
    "print(\"70B IQ3_XS configuration for dual T4:\")\n",
    "for k, v in config_70b.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nEstimated VRAM: {estimate_vram(70, 3.3, 512):.1f} GB / {DUAL_T4_GB:.0f} GB available\")\n",
    "print(\"Note: Download the actual 70B model to run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization Techniques\n",
    "\n",
    "Monitor memory during inference to identify optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart with the demo model for monitoring\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"0.5,0.5\", ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Background GPU sampling during inference\n",
    "handle = start_sampler(interval_ms=200)\n",
    "\n",
    "# Generate varying-length responses\n",
    "for length in [32, 64, 128, 256]:\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Describe the history of computing in detail.\"}],\n",
    "        max_tokens=length, temperature=0.7,\n",
    "    )\n",
    "\n",
    "handle.stop()\n",
    "samples = handle.get_snapshots()\n",
    "\n",
    "# Analyze memory patterns\n",
    "if samples:\n",
    "    gpu0_mem = [s.mem_used_mb for s in samples if s.gpu_id == 0]\n",
    "    gpu1_mem = [s.mem_used_mb for s in samples if s.gpu_id == 1]\n",
    "\n",
    "    if gpu0_mem:\n",
    "        print(f\"GPU 0 memory: min={min(gpu0_mem)} MB, max={max(gpu0_mem)} MB, avg={sum(gpu0_mem)//len(gpu0_mem)} MB\")\n",
    "    if gpu1_mem:\n",
    "        print(f\"GPU 1 memory: min={min(gpu1_mem)} MB, max={max(gpu1_mem)} MB, avg={sum(gpu1_mem)//len(gpu1_mem)} MB\")\n",
    "    print(f\"Collected {len(samples)} samples over {samples[-1].timestamp - samples[0].timestamp:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Window Tuning\n",
    "\n",
    "Larger context windows consume more VRAM for the KV cache. Find the optimal\n",
    "context size for your model and available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sizes = [512, 1024, 2048, 4096]\n",
    "ctx_results = []\n",
    "\n",
    "for ctx in context_sizes:\n",
    "    mgr.stop_server()\n",
    "    time.sleep(2)\n",
    "\n",
    "    mgr = ServerManager()\n",
    "    mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"0.5,0.5\", ctx_size=ctx)\n",
    "    mgr.wait_until_ready(timeout=60)\n",
    "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "    mem = snapshot()\n",
    "    total_mem = sum(s.mem_used_mb for s in mem)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}],\n",
    "        max_tokens=64, temperature=0.7,\n",
    "    )\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    ctx_results.append((ctx, total_mem, elapsed * 1000))\n",
    "    print(f\"  ctx={ctx:5d}: {total_mem:5d} MB VRAM, {elapsed*1000:.0f} ms latency\")\n",
    "\n",
    "print(\"\\nRecommendation: Use the largest context that leaves ≥2 GB headroom.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "| Model | Quant | VRAM | Context | Speed | Use Case |\n",
    "|-------|-------|------|---------|-------|----------|\n",
    "| 7B | Q4_K_M | ~4 GB | 4096 | Fast | General tasks |\n",
    "| 13B | Q4_K_M | ~7 GB | 2048-4096 | Good | Better quality |\n",
    "| 34B | Q4_K_M | ~19 GB | 1024-2048 | Moderate | Specialized tasks |\n",
    "| 70B | IQ3_XS | ~29 GB | 512-1024 | Slow | Maximum capability |\n",
    "\n",
    "### Recommendations\n",
    "- **Start with 7B Q4_K_M** for development and prototyping\n",
    "- **Use 13B Q4_K_M** when quality matters and speed is acceptable\n",
    "- **Reserve 70B IQ3_XS** for tasks where model capability is critical\n",
    "- **Always use flash attention** (`flash_attn=True`) for memory savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}