{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Unsloth + GGUF Export\n",
    "\n",
    "**Duration:** ~30 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook shows how to fine-tune a model with **Unsloth**, export it to GGUF,\n",
    "and deploy it with llamatelemetry — all traced with OpenTelemetry.\n",
    "\n",
    "### What you'll learn\n",
    "1. Load a base model with Unsloth + LoRA adapters\n",
    "2. Prepare a dataset (Alpaca format)\n",
    "3. Train with SFTTrainer (traced with `@workflow`)\n",
    "4. Export to GGUF with Q4_K_M quantization\n",
    "5. Deploy and test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.2.0\n",
    "!pip install -q unsloth trl datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SDK with Tracing\n",
    "\n",
    "Enable tracing so every step of the fine-tuning pipeline is observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamatelemetry\n",
    "\n",
    "llamatelemetry.init(service_name=\"finetune-workflow\")\n",
    "print(f\"llamatelemetry {llamatelemetry.version()} — tracing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model\n",
    "\n",
    "Use Unsloth's `FastModel` for 2× faster loading and automatic LoRA injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.task(name=\"load-base-model\")\n",
    "def load_model():\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/gemma-3-1b-it\",\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,  # auto-detect\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    print(f\"Model loaded with LoRA (r=16, {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable params)\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Format the dataset in Alpaca style: instruction → input → output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.task(name=\"prepare-dataset\")\n",
    "def prepare_dataset(tokenizer):\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")\n",
    "\n",
    "    alpaca_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "    def format_example(example):\n",
    "        return {\"text\": alpaca_template.format(**example) + tokenizer.eos_token}\n",
    "\n",
    "    dataset = dataset.map(format_example)\n",
    "    print(f\"Dataset prepared: {len(dataset)} examples\")\n",
    "    return dataset\n",
    "\n",
    "dataset = prepare_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with SFT\n",
    "\n",
    "The `@workflow` decorator creates a parent span that groups all training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.workflow(name=\"sft-training\")\n",
    "def train(model, tokenizer, dataset):\n",
    "    from trl import SFTTrainer\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            max_steps=30,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=5,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\",\n",
    "        ),\n",
    "        max_seq_length=2048,\n",
    "    )\n",
    "\n",
    "    stats = trainer.train()\n",
    "    print(f\"Training complete: {stats.metrics['train_loss']:.4f} loss, {stats.metrics['train_steps_per_second']:.1f} steps/s\")\n",
    "    return model\n",
    "\n",
    "model = train(model, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to GGUF\n",
    "\n",
    "Merge LoRA weights back and export to GGUF with Q4_K_M quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llamatelemetry.task(name=\"gguf-export\")\n",
    "def export_gguf(model, tokenizer):\n",
    "    from unsloth import FastLanguageModel\n",
    "    import os\n",
    "\n",
    "    output_dir = \"outputs/gemma-3-1b-finetuned-gguf\"\n",
    "    model.save_pretrained_gguf(\n",
    "        output_dir,\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\",\n",
    "    )\n",
    "\n",
    "    # Find the exported file\n",
    "    gguf_files = [f for f in os.listdir(output_dir) if f.endswith(\".gguf\")]\n",
    "    gguf_path = os.path.join(output_dir, gguf_files[0])\n",
    "    size_mb = os.path.getsize(gguf_path) / (1024 * 1024)\n",
    "    print(f\"Exported: {gguf_path} ({size_mb:.0f} MB)\")\n",
    "    return gguf_path\n",
    "\n",
    "gguf_path = export_gguf(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Test\n",
    "\n",
    "Load the fine-tuned GGUF model into llama-server and test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=gguf_path, gpu_layers=99, ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Test the fine-tuned model\n",
    "test_prompts = [\n",
    "    \"Explain what machine learning is in one sentence.\",\n",
    "    \"Write a Python function that reverses a string.\",\n",
    "    \"What is the capital of France?\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    resp = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=128, temperature=0.7,\n",
    "    )\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {resp.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor GPU and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU status after fine-tuning + deployment\n",
    "for s in llamatelemetry.gpu.snapshot():\n",
    "    print(f\"GPU {s.gpu_id}: {s.mem_used_mb}/{s.mem_total_mb} MB ({s.utilization_pct}% util)\")\n",
    "\n",
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"\\nFine-tuning pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}