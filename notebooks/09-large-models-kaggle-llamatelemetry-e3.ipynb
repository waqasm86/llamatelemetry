{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7382eb7f-da4f-4f88-bab5-988d4bb96155","cell_type":"markdown","source":"# 09 - Large Models on Kaggle (llamatelemetry) - e3\n\nObjective: Download three GGUF models from Hugging Face and run them one-by-one on Kaggle T4 GPU 0, executing all steps after Step 4 for each model.\n","metadata":{}},{"id":"fbd2863b-89fe-41ad-850c-e0601b068832","cell_type":"markdown","source":"## Step 0.5: Verify Dual GPU Environment\n","metadata":{}},{"id":"66639405-0a14-4585-a10d-1a944d422924","cell_type":"code","source":"import subprocess\n\nprint('='*70)\nprint('?? GPU ENVIRONMENT CHECK')\nprint('='*70)\n\nresult = subprocess.run(\n    ['nvidia-smi', '--query-gpu=index,name,memory.total,memory.free,compute_cap', '--format=csv,noheader'],\n    capture_output=True, text=True\n)\nprint(result.stdout)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T07:54:46.377560Z","iopub.execute_input":"2026-02-05T07:54:46.377817Z","iopub.status.idle":"2026-02-05T07:54:46.422037Z","shell.execute_reply.started":"2026-02-05T07:54:46.377795Z","shell.execute_reply":"2026-02-05T07:54:46.421337Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? GPU ENVIRONMENT CHECK\n======================================================================\n0, Tesla T4, 15360 MiB, 14913 MiB, 7.5\n1, Tesla T4, 15360 MiB, 14913 MiB, 7.5\n\n","output_type":"stream"}],"execution_count":1},{"id":"083d6292-d8d5-4e16-bb42-216e9c81cfcf","cell_type":"markdown","source":"## Step 1: Install llamatelemetry and Dependencies\n","metadata":{}},{"id":"ee5dd3f2-f0f4-4410-8bc2-0bbb7071e76f","cell_type":"code","source":"%%time\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n!pip install -q huggingface_hub sseclient-py\n\nimport llamatelemetry\nprint('? llamatelemetry {} installed'.format(llamatelemetry.__version__))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T07:55:20.660536Z","iopub.execute_input":"2026-02-05T07:55:20.661119Z","iopub.status.idle":"2026-02-05T07:57:17.850164Z","shell.execute_reply.started":"2026-02-05T07:55:20.661090Z","shell.execute_reply":"2026-02-05T07:57:17.849294Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m193.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m307.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m298.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m365.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m331.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m332.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m312.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m342.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m362.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m208.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m224.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m260.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m237.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m219.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m319.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m289.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m181.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m282.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m279.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\ngoogle-adk 1.22.1 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nopentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\nWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\nWARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(‚Ä¶):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff646126a8c483185affbaaba7e7117"}},"metadata":{}},{"name":"stdout","text":"üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\n‚úÖ Binaries installed successfully!\n\n? llamatelemetry 0.1.0 installed\nCPU times: user 49 s, sys: 11.4 s, total: 1min\nWall time: 1min 57s\n","output_type":"stream"}],"execution_count":2},{"id":"c143ef4a-ee76-49be-b686-9c34549257ee","cell_type":"markdown","source":"## Step 2: Setup Secrets (Hugging Face + Graphistry Optional)\n","metadata":{}},{"id":"1c62d637-8991-452e-9352-a2a5560caa96","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\n\nhf_token = None\ntry:\n    hf_token = user_secrets.get_secret('HF_TOKEN_2')\nexcept Exception:\n    pass\n\nif hf_token:\n    login(token=hf_token)\n    print('? Hugging Face login OK')\nelse:\n    print('?? HF_TOKEN not found. Public models will still download.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T07:57:38.315428Z","iopub.execute_input":"2026-02-05T07:57:38.315734Z","iopub.status.idle":"2026-02-05T07:57:38.573713Z","shell.execute_reply.started":"2026-02-05T07:57:38.315710Z","shell.execute_reply":"2026-02-05T07:57:38.573127Z"}},"outputs":[{"name":"stdout","text":"? Hugging Face login OK\n","output_type":"stream"}],"execution_count":4},{"id":"ebb21585-1589-4840-9ce7-665141953ef4","cell_type":"markdown","source":"## Step 3: Download 3 GGUF Models (1?4 GB each)\n","metadata":{}},{"id":"0fe06500-ca06-40e3-a7df-70fe73885e72","cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport os\n\nprint('='*70)\nprint('?? DOWNLOADING 3 GGUF MODELS')\nprint('='*70)\n\nos.makedirs('/kaggle/working/models', exist_ok=True)\n\nmodel_configs = [\n    {\n        'name': 'Llama-3.2-3B-Instruct',\n        'repo': 'bartowski/Llama-3.2-3B-Instruct-GGUF',\n        'file': 'Llama-3.2-3B-Instruct-Q4_K_M.gguf',\n        'size_gb': 1.88\n    },\n    {\n        'name': 'Mistral-7B-Instruct',\n        'repo': 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF',\n        'file': 'mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n        'size_gb': 4.08\n    },\n    {\n        'name': 'Phi-3-mini-4k',\n        'repo': 'microsoft/Phi-3-mini-4k-instruct-gguf',\n        'file': 'Phi-3-mini-4k-instruct-q4.gguf',\n        'size_gb': 2.10\n    },\n]\n\nmodel_paths = {}\nfor model in model_configs:\n    print('\\n?? Downloading {}...'.format(model['name']))\n    print('   Estimated size: {:.2f} GB'.format(model['size_gb']))\n    try:\n        model_path = hf_hub_download(\n            repo_id=model['repo'],\n            filename=model['file'],\n            local_dir='/kaggle/working/models',\n            local_dir_use_symlinks=False\n        )\n        actual_size = os.path.getsize(model_path) / (1024**3)\n        model_paths[model['name']] = model_path\n        print('? Downloaded: {} ({:.2f} GB)'.format(os.path.basename(model_path), actual_size))\n    except Exception as e:\n        print('?? Failed to download {}: {}'.format(model['name'], e))\n\nprint('\\n?? Summary: {} models downloaded'.format(len(model_paths)))\nfor name, path in model_paths.items():\n    print('   ? {}: {}'.format(name, os.path.basename(path)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T07:57:43.020872Z","iopub.execute_input":"2026-02-05T07:57:43.021564Z","iopub.status.idle":"2026-02-05T07:58:08.788008Z","shell.execute_reply.started":"2026-02-05T07:57:43.021537Z","shell.execute_reply":"2026-02-05T07:58:08.787414Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? DOWNLOADING 3 GGUF MODELS\n======================================================================\n\n?? Downloading Llama-3.2-3B-Instruct...\n   Estimated size: 1.88 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44909c5cc712414697d82c596b91d239"}},"metadata":{}},{"name":"stdout","text":"? Downloaded: Llama-3.2-3B-Instruct-Q4_K_M.gguf (1.88 GB)\n\n?? Downloading Mistral-7B-Instruct...\n   Estimated size: 4.08 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"mistral-7b-instruct-v0.2.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c49165826e9436584aa7667d38d9057"}},"metadata":{}},{"name":"stdout","text":"? Downloaded: mistral-7b-instruct-v0.2.Q4_K_M.gguf (4.07 GB)\n\n?? Downloading Phi-3-mini-4k...\n   Estimated size: 2.10 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Phi-3-mini-4k-instruct-q4.gguf:   0%|          | 0.00/2.39G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff41f9d588141a9b73fe7ff046038f5"}},"metadata":{}},{"name":"stdout","text":"? Downloaded: Phi-3-mini-4k-instruct-q4.gguf (2.23 GB)\n\n?? Summary: 3 models downloaded\n   ? Llama-3.2-3B-Instruct: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   ? Mistral-7B-Instruct: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n   ? Phi-3-mini-4k: Phi-3-mini-4k-instruct-q4.gguf\n","output_type":"stream"}],"execution_count":5},{"id":"ee86e680-3490-49b9-8a5a-20a92fd33867","cell_type":"markdown","source":"## Step 4: Helper Functions\n","metadata":{}},{"id":"3c7ce3e1-f2f3-4979-bd89-69d602982bcb","cell_type":"code","source":"import time\nimport requests\n\ndef wait_until_ready(server_url, timeout=90):\n    start = time.time()\n    while time.time() - start < timeout:\n        try:\n            r = requests.get(server_url + '/health', timeout=2)\n            if r.status_code == 200:\n                return True\n        except Exception:\n            pass\n        time.sleep(1)\n    return False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T07:58:29.326192Z","iopub.execute_input":"2026-02-05T07:58:29.326895Z","iopub.status.idle":"2026-02-05T07:58:29.331193Z","shell.execute_reply.started":"2026-02-05T07:58:29.326860Z","shell.execute_reply":"2026-02-05T07:58:29.330660Z"}},"outputs":[],"execution_count":6},{"id":"b8c1a2c6-e1f7-4bd2-ae4c-5c3b65330b83","cell_type":"markdown","source":"## Steps 5?13: Run Each Model Sequentially (GPU 0)\n","metadata":{}},{"id":"15529ed8-77a0-4acf-96d6-66ba9f3680be","cell_type":"code","source":"from llamatelemetry.server import ServerManager\nfrom llamatelemetry.api.client import LlamaCppClient\nimport subprocess\nimport os\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nresults_summary = []\n\nfor model_name, model_path in model_paths.items():\n    print('\\n' + '='*80)\n    print('?? RUNNING MODEL: {}'.format(model_name))\n    print('='*80)\n\n    print('\\n?? Step 5: GPU Availability')\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=index,name,memory.total,memory.free', '--format=csv,noheader'],\n        capture_output=True, text=True\n    )\n    print(result.stdout)\n\n    print('?? Step 6: Start llama-server (GPU 0)')\n    server = ServerManager(server_url='http://127.0.0.1:8090')\n    try:\n        server.start_server(\n            model_path=model_path,\n            host='127.0.0.1',\n            port=8090,\n            gpu_layers=99,\n            tensor_split='1.0,0.0',\n            ctx_size=2048,\n            flash_attn='auto',\n            timeout=90,\n            verbose=True,\n        )\n        if not wait_until_ready('http://127.0.0.1:8090', timeout=90):\n            print('? Server did not become ready')\n            server.stop_server()\n            continue\n    except Exception as e:\n        print('? Failed to start server: {}'.format(e))\n        try:\n            server.stop_server()\n        except Exception:\n            pass\n        continue\n\n    print('\\n?? Step 7: GPU Memory Usage')\n    subprocess.run(['nvidia-smi'])\n\n    print('\\n?? Step 8: Test Inference')\n    client = LlamaCppClient(base_url='http://127.0.0.1:8090')\n    prompt = 'Explain quantum computing in simple terms.'\n    start = time.time()\n    response = client.chat.create(\n        messages=[{'role': 'user', 'content': prompt}],\n        max_tokens=200,\n        temperature=0.7,\n    )\n    elapsed = time.time() - start\n    tokens = response.usage.completion_tokens if response.usage else 0\n    print(response.choices[0].message.content)\n    print('\\n?? Inference: {:.2f}s | {} tokens | {:.1f} tok/s'.format(elapsed, tokens, (tokens/elapsed if elapsed>0 else 0)))\n\n    print('\\n?? Step 9: Performance Benchmarks')\n    prompts = [\n        'Hello',\n        'What is the capital of France?',\n        'Write a paragraph about machine learning.',\n        'Explain the differences between Python and JavaScript with examples.'\n    ]\n\n    bench_times = []\n    bench_speeds = []\n\n    for p in prompts:\n        t0 = time.time()\n        r = client.chat.create(\n            messages=[{'role': 'user', 'content': p}],\n            max_tokens=80,\n            temperature=0.7,\n        )\n        dt = time.time() - t0\n        tk = r.usage.completion_tokens if r.usage else 0\n        sp = tk/dt if dt>0 else 0\n        bench_times.append(dt)\n        bench_speeds.append(sp)\n        print('   Prompt: {}... | {:.2f}s | {:.1f} tok/s'.format(p[:40], dt, sp))\n\n    avg_time = sum(bench_times) / len(bench_times)\n    avg_speed = sum(bench_speeds) / len(bench_speeds)\n\n    print('\\n?? Step 10: Streaming Test')\n    stream_prompt = 'What are the benefits of streaming?'\n    try:\n        stream_resp = client.chat.create(\n            messages=[{'role': 'user', 'content': stream_prompt}],\n            max_tokens=80,\n            temperature=0.7,\n            stream=True\n        )\n        out = ''\n        for chunk in stream_resp:\n            if hasattr(chunk, 'choices') and chunk.choices:\n                choice = chunk.choices[0]\n                if hasattr(choice, 'delta') and choice.delta and getattr(choice.delta, 'content', None):\n                    out += choice.delta.content\n        print(out[:200])\n    except Exception as e:\n        print('?? Streaming error: {}'.format(e))\n\n    print('\\n?? Step 11: Stop server')\n    server.stop_server()\n    time.sleep(2)\n\n    results_summary.append({\n        'Model': model_name,\n        'Avg Time (s)': '{:.2f}'.format(avg_time),\n        'Avg Speed (tok/s)': '{:.1f}'.format(avg_speed),\n    })\n\nprint('\\n' + '='*80)\nprint('?? SUMMARY')\nprint('='*80)\nif results_summary:\n    import pandas as pd\n    df = pd.DataFrame(results_summary)\n    print(df.to_string(index=False))\nelse:\n    print('No benchmark results.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T07:58:35.711668Z","iopub.execute_input":"2026-02-05T07:58:35.712177Z","iopub.status.idle":"2026-02-05T07:59:21.512188Z","shell.execute_reply.started":"2026-02-05T07:58:35.712152Z","shell.execute_reply":"2026-02-05T07:59:21.511540Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\n?? RUNNING MODEL: Llama-3.2-3B-Instruct\n================================================================================\n\n?? Step 5: GPU Availability\n0, Tesla T4, 15360 MiB, 14913 MiB\n1, Tesla T4, 15360 MiB, 14913 MiB\n\n?? Step 6: Start llama-server (GPU 0)\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready....... ‚úì Ready in 4.1s\n\n?? Step 7: GPU Memory Usage\nThu Feb  5 07:58:39 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   43C    P0             25W /   70W |    2333MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             252      C   .../binaries/cuda12/llama-server       2330MiB |\n+-----------------------------------------------------------------------------------------+\n\n?? Step 8: Test Inference\nQuantum computing is a new way of processing information that's different from classical computers like the ones you use every day.\n\n**How classical computers work:**\n\nClassical computers use \"bits\" to store and process information. A bit can be either 0 or 1, and it's used like a light switch - on or off. The computer processes information by performing calculations with these bits, using algorithms that are based on simple rules.\n\n**How quantum computers work:**\n\nQuantum computers use \"qubits\" (quantum bits) to store and process information. Qubits can be both 0 AND 1 at the same time! This is because they exist in a special state called \"superposition\", where they're like a coin that's spinning in the air - it's both heads and tails at the same time.\n\nImagine you have a combination lock with 10 numbers (0-9). A classical computer would try each number one by one, using a bit to represent whether it\n\n?? Inference: 2.72s | 200 tokens | 73.5 tok/s\n\n?? Step 9: Performance Benchmarks\n   Prompt: Hello... | 0.14s | 56.0 tok/s\n   Prompt: What is the capital of France?... | 0.11s | 69.8 tok/s\n   Prompt: Write a paragraph about machine learning... | 1.08s | 74.0 tok/s\n   Prompt: Explain the differences between Python a... | 1.08s | 73.8 tok/s\n\n?? Step 10: Streaming Test\n\n\n?? Step 11: Stop server\n\n================================================================================\n?? RUNNING MODEL: Mistral-7B-Instruct\n================================================================================\n\n?? Step 5: GPU Availability\n0, Tesla T4, 15360 MiB, 14913 MiB\n1, Tesla T4, 15360 MiB, 14913 MiB\n\n?? Step 6: Start llama-server (GPU 0)\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... ‚úì Ready in 2.0s\n\n?? Step 7: GPU Memory Usage\nThu Feb  5 07:58:51 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   47C    P0             54W /   70W |    4499MiB /  15360MiB |     13%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             268      C   .../binaries/cuda12/llama-server       4496MiB |\n+-----------------------------------------------------------------------------------------+\n\n?? Step 8: Test Inference\n Quantum computing is a type of computing technology that uses the principles of quantum mechanics to perform complex calculations. In classical computers, information is processed using bits, which can only represent a value of 0 or 1. In contrast, a quantum bit, or qubit, can exist in multiple states at once - 0, 1, and both 0 and 1 at the same time, a property known as superposition. This allows quantum computers to process vast amounts of information simultaneously, making them potentially much faster than classical computers for certain tasks.\n\nAnother important feature of qubits is entanglement, which means that two or more qubits can be connected in such a way that the state of one instantly affects the state of the other, no matter how far apart they are. This property enables quantum computers to solve complex problems through parallel processing and efficient communication between qubits.\n\nQuantum computing is still an emerging technology and requires specialized hardware and software, as well\n\n?? Inference: 4.66s | 200 tokens | 43.0 tok/s\n\n?? Step 9: Performance Benchmarks\n   Prompt: Hello... | 1.44s | 41.8 tok/s\n   Prompt: What is the capital of France?... | 1.88s | 42.5 tok/s\n   Prompt: Write a paragraph about machine learning... | 1.88s | 42.4 tok/s\n   Prompt: Explain the differences between Python a... | 1.91s | 42.0 tok/s\n\n?? Step 10: Streaming Test\n\n\n?? Step 11: Stop server\n\n================================================================================\n?? RUNNING MODEL: Phi-3-mini-4k\n================================================================================\n\n?? Step 5: GPU Availability\n0, Tesla T4, 15360 MiB, 14913 MiB\n1, Tesla T4, 15360 MiB, 14913 MiB\n\n?? Step 6: Start llama-server (GPU 0)\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Phi-3-mini-4k-instruct-q4.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... ‚úì Ready in 2.0s\n\n?? Step 7: GPU Memory Usage\nThu Feb  5 07:59:09 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   54C    P0             26W /   70W |    3137MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             284      C   .../binaries/cuda12/llama-server       3134MiB |\n+-----------------------------------------------------------------------------------------+\n\n?? Step 8: Test Inference\n Quantum computing is a type of computing that uses the principles of quantum mechanics, which is a branch of physics that deals with the behavior of particles at extremely small scales. In classical computers, information is processed using bits, which can have two states: 0 or 1. However, in quantum computing, we use \"qubits\" (quantum bits) as the basic unit of information.\n\nQubits differ from regular bits in that they can exist in a superposition state, meaning they can be both 0 and 1 simultaneously until measured. This property allows quantum computers to process vast amounts of data more efficiently than classical computers. Additionally, qubits exhibit entanglement, another unique quantum phenomenon where the states of two or more qubits become connected, regardless of their distance apart.\n\nThe power of a quantum computer lies in its ability to perform multiple calculations simultaneously using superposition and exploiting entanglement for faster data processing, potentially solving complex\n\n?? Inference: 3.02s | 200 tokens | 66.3 tok/s\n\n?? Step 9: Performance Benchmarks\n   Prompt: Hello... | 0.52s | 57.4 tok/s\n   Prompt: What is the capital of France?... | 1.21s | 65.9 tok/s\n   Prompt: Write a paragraph about machine learning... | 1.24s | 64.7 tok/s\n   Prompt: Explain the differences between Python a... | 1.24s | 64.5 tok/s\n\n?? Step 10: Streaming Test\n\n\n?? Step 11: Stop server\n\n================================================================================\n?? SUMMARY\n================================================================================\n                Model Avg Time (s) Avg Speed (tok/s)\nLlama-3.2-3B-Instruct         0.61              68.4\n  Mistral-7B-Instruct         1.78              42.2\n        Phi-3-mini-4k         1.05              63.1\n","output_type":"stream"}],"execution_count":7},{"id":"02dc248b-e4a6-4687-a480-ab2857d2a33c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f5c3cde0-bf36-40ec-ae3f-f671899dcfd6","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"09b8fe37-540f-4482-b183-84f9e5903e50","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d9fd64e3-2d08-4d06-b8ec-cfb2f963fc92","cell_type":"code","source":"# Step 5: GPU Availability Check (run once before all models)\nfrom llamatelemetry.server import ServerManager\nfrom llamatelemetry.api.client import LlamaCppClient\nimport subprocess\nimport os\nimport time\nimport requests\n\n# Set GPU 0\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nprint('='*70)\nprint('?? STEP 5: GPU AVAILABILITY CHECK')\nprint('='*70)\n\nresult = subprocess.run(\n    ['nvidia-smi', '--query-gpu=index,name,memory.total,memory.free', '--format=csv,noheader'],\n    capture_output=True, text=True\n)\nprint(result.stdout)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:09:09.100147Z","iopub.execute_input":"2026-02-05T08:09:09.100704Z","iopub.status.idle":"2026-02-05T08:09:09.142807Z","shell.execute_reply.started":"2026-02-05T08:09:09.100666Z","shell.execute_reply":"2026-02-05T08:09:09.142224Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 5: GPU AVAILABILITY CHECK\n======================================================================\n0, Tesla T4, 15360 MiB, 14913 MiB\n1, Tesla T4, 15360 MiB, 14913 MiB\n\n","output_type":"stream"}],"execution_count":8},{"id":"1cdb4164-039c-4f3c-aa7c-f034a5d83be1","cell_type":"code","source":"# Step 6: Start llama-server for each model (will run sequentially)\nprint('='*70)\nprint('?? STEP 6: START LLAMA-SERVER FOR EACH MODEL')\nprint('='*70)\n\n# Helper function for waiting\ndef wait_until_ready(server_url, timeout=90):\n    start = time.time()\n    while time.time() - start < timeout:\n        try:\n            r = requests.get(server_url + '/health', timeout=2)\n            if r.status_code == 200:\n                return True\n        except Exception:\n            pass\n        time.sleep(1)\n    return False\n\n# Dictionary to store running servers\nservers = {}\n\nfor model_name, model_path in model_paths.items():\n    print(f'\\n?? Starting server for: {model_name}')\n    print(f'   Model path: {os.path.basename(model_path)}')\n    \n    server = ServerManager(server_url='http://127.0.0.1:8090')\n    try:\n        server.start_server(\n            model_path=model_path,\n            host='127.0.0.1',\n            port=8090,\n            gpu_layers=99,\n            tensor_split='1.0,0.0',\n            ctx_size=2048,\n            flash_attn='auto',\n            timeout=90,\n            verbose=True,\n        )\n        \n        if wait_until_ready('http://127.0.0.1:8090', timeout=90):\n            servers[model_name] = server\n            print(f'? Server for {model_name} is ready')\n        else:\n            print(f'? Server for {model_name} did not become ready')\n            server.stop_server()\n    except Exception as e:\n        print(f'? Failed to start server for {model_name}: {e}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:09:18.471179Z","iopub.execute_input":"2026-02-05T08:09:18.471937Z","iopub.status.idle":"2026-02-05T08:09:20.601853Z","shell.execute_reply.started":"2026-02-05T08:09:18.471906Z","shell.execute_reply":"2026-02-05T08:09:20.601149Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 6: START LLAMA-SERVER FOR EACH MODEL\n======================================================================\n\n?? Starting server for: Llama-3.2-3B-Instruct\n   Model path: Llama-3.2-3B-Instruct-Q4_K_M.gguf\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready..... ‚úì Ready in 2.0s\n? Server for Llama-3.2-3B-Instruct is ready\n\n?? Starting server for: Mistral-7B-Instruct\n   Model path: mistral-7b-instruct-v0.2.Q4_K_M.gguf\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\n‚úì llama-server already running at http://127.0.0.1:8090\n? Server for Mistral-7B-Instruct is ready\n\n?? Starting server for: Phi-3-mini-4k\n   Model path: Phi-3-mini-4k-instruct-q4.gguf\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\n‚úì llama-server already running at http://127.0.0.1:8090\n? Server for Phi-3-mini-4k is ready\n","output_type":"stream"}],"execution_count":9},{"id":"41bb04dd-ac33-412d-9a8d-6fc5b3ba18c6","cell_type":"code","source":"# Step 7: GPU Memory Usage after loading each model\nprint('='*70)\nprint('?? STEP 7: GPU MEMORY USAGE AFTER LOADING MODELS')\nprint('='*70)\n\nfor model_name in model_paths.keys():\n    if model_name in servers:\n        print(f'\\n?? GPU Memory Usage for {model_name}:')\n        subprocess.run(['nvidia-smi'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:09:36.454641Z","iopub.execute_input":"2026-02-05T08:09:36.455233Z","iopub.status.idle":"2026-02-05T08:09:37.322758Z","shell.execute_reply.started":"2026-02-05T08:09:36.455205Z","shell.execute_reply":"2026-02-05T08:09:37.322174Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 7: GPU MEMORY USAGE AFTER LOADING MODELS\n======================================================================\n\n?? GPU Memory Usage for Llama-3.2-3B-Instruct:\nThu Feb  5 08:09:36 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   44C    P0             25W /   70W |    2333MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   46C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             314      C   .../binaries/cuda12/llama-server       2330MiB |\n+-----------------------------------------------------------------------------------------+\n\n?? GPU Memory Usage for Mistral-7B-Instruct:\nThu Feb  5 08:09:36 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   44C    P0             25W /   70W |    2333MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   46C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             314      C   .../binaries/cuda12/llama-server       2330MiB |\n+-----------------------------------------------------------------------------------------+\n\n?? GPU Memory Usage for Phi-3-mini-4k:\nThu Feb  5 08:09:37 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   44C    P0             25W /   70W |    2333MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   46C    P0             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A             314      C   .../binaries/cuda12/llama-server       2330MiB |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":10},{"id":"2cb328cd-e207-4ae6-8999-70e91bb158c0","cell_type":"code","source":"# Step 8: Test Inference for each model\nprint('='*70)\nprint('?? STEP 8: TEST INFERENCE FOR EACH MODEL')\nprint('='*70)\n\ninference_results = {}\n\nfor model_name, model_path in model_paths.items():\n    if model_name not in servers:\n        print(f'\\n?? Skipping {model_name} - server not running')\n        continue\n    \n    print(f'\\n?? Testing inference for: {model_name}')\n    \n    client = LlamaCppClient(base_url='http://127.0.0.1:8090')\n    prompt = 'Explain quantum computing in simple terms.'\n    \n    start = time.time()\n    response = client.chat.create(\n        messages=[{'role': 'user', 'content': prompt}],\n        max_tokens=200,\n        temperature=0.7,\n    )\n    elapsed = time.time() - start\n    tokens = response.usage.completion_tokens if response.usage else 0\n    \n    print('Response preview:')\n    print(response.choices[0].message.content[:200] + '...')\n    print(f'\\n?? Inference: {elapsed:.2f}s | {tokens} tokens | {tokens/elapsed if elapsed>0 else 0:.1f} tok/s')\n    \n    inference_results[model_name] = {\n        'time': elapsed,\n        'tokens': tokens,\n        'speed': tokens/elapsed if elapsed>0 else 0\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:10:06.655842Z","iopub.execute_input":"2026-02-05T08:10:06.656486Z","iopub.status.idle":"2026-02-05T08:10:14.883468Z","shell.execute_reply.started":"2026-02-05T08:10:06.656454Z","shell.execute_reply":"2026-02-05T08:10:14.882733Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 8: TEST INFERENCE FOR EACH MODEL\n======================================================================\n\n?? Testing inference for: Llama-3.2-3B-Instruct\nResponse preview:\nQuantum computing is a new way of processing information that's different from traditional computers.\n\n**Traditional Computers**\n\nTraditional computers use \"bits\" to store and process information. Bit...\n\n?? Inference: 2.78s | 200 tokens | 71.9 tok/s\n\n?? Testing inference for: Mistral-7B-Instruct\nResponse preview:\nQuantum computing is a new way of processing information that's different from the computers we use today. Here are some simple explanations:\n\n**Classical Computing vs Quantum Computing**\n\nImagine you...\n\n?? Inference: 2.72s | 200 tokens | 73.5 tok/s\n\n?? Testing inference for: Phi-3-mini-4k\nResponse preview:\nQuantum computing is a way of processing information that's different from how regular computers work.\n\n**Classical Computers**\n\nImagine you have a giant library with millions of books on shelves. Eac...\n\n?? Inference: 2.72s | 200 tokens | 73.5 tok/s\n","output_type":"stream"}],"execution_count":11},{"id":"3074dfa7-d32c-4a31-95d7-c3742469b8a5","cell_type":"code","source":"# Step 9: Performance Benchmarks for each model\nprint('='*70)\nprint('?? STEP 9: PERFORMANCE BENCHMARKS')\nprint('='*70)\n\nbenchmark_results = {}\n\nprompts = [\n    'Hello',\n    'What is the capital of France?',\n    'Write a paragraph about machine learning.',\n    'Explain the differences between Python and JavaScript with examples.'\n]\n\nfor model_name, model_path in model_paths.items():\n    if model_name not in servers:\n        print(f'\\n?? Skipping {model_name} - server not running')\n        continue\n    \n    print(f'\\n?? Running benchmarks for: {model_name}')\n    \n    client = LlamaCppClient(base_url='http://127.0.0.1:8090')\n    bench_times = []\n    bench_speeds = []\n    \n    for i, p in enumerate(prompts, 1):\n        t0 = time.time()\n        r = client.chat.create(\n            messages=[{'role': 'user', 'content': p}],\n            max_tokens=80,\n            temperature=0.7,\n        )\n        dt = time.time() - t0\n        tk = r.usage.completion_tokens if r.usage else 0\n        sp = tk/dt if dt>0 else 0\n        bench_times.append(dt)\n        bench_speeds.append(sp)\n        print(f'   {i}. Prompt: {p[:40]}... | {dt:.2f}s | {sp:.1f} tok/s')\n    \n    avg_time = sum(bench_times) / len(bench_times)\n    avg_speed = sum(bench_speeds) / len(bench_speeds)\n    \n    benchmark_results[model_name] = {\n        'avg_time': avg_time,\n        'avg_speed': avg_speed,\n        'bench_times': bench_times,\n        'bench_speeds': bench_speeds\n    }\n    \n    print(f'   Average: {avg_time:.2f}s | {avg_speed:.1f} tok/s')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:10:25.391086Z","iopub.execute_input":"2026-02-05T08:10:25.391406Z","iopub.status.idle":"2026-02-05T08:10:32.795984Z","shell.execute_reply.started":"2026-02-05T08:10:25.391371Z","shell.execute_reply":"2026-02-05T08:10:32.795373Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 9: PERFORMANCE BENCHMARKS\n======================================================================\n\n?? Running benchmarks for: Llama-3.2-3B-Instruct\n   1. Prompt: Hello... | 0.22s | 37.0 tok/s\n   2. Prompt: What is the capital of France?... | 0.12s | 67.0 tok/s\n   3. Prompt: Write a paragraph about machine learning... | 1.09s | 73.7 tok/s\n   4. Prompt: Explain the differences between Python a... | 1.10s | 72.6 tok/s\n   Average: 0.63s | 62.5 tok/s\n\n?? Running benchmarks for: Mistral-7B-Instruct\n   1. Prompt: Hello... | 0.13s | 60.7 tok/s\n   2. Prompt: What is the capital of France?... | 0.12s | 68.2 tok/s\n   3. Prompt: Write a paragraph about machine learning... | 1.09s | 73.4 tok/s\n   4. Prompt: Explain the differences between Python a... | 1.09s | 73.2 tok/s\n   Average: 0.61s | 68.9 tok/s\n\n?? Running benchmarks for: Phi-3-mini-4k\n   1. Prompt: Hello... | 0.13s | 59.6 tok/s\n   2. Prompt: What is the capital of France?... | 0.12s | 67.8 tok/s\n   3. Prompt: Write a paragraph about machine learning... | 1.09s | 73.3 tok/s\n   4. Prompt: Explain the differences between Python a... | 1.10s | 72.9 tok/s\n   Average: 0.61s | 68.4 tok/s\n","output_type":"stream"}],"execution_count":12},{"id":"0689d4e6-473b-4e38-ae35-903b5ca76ec3","cell_type":"code","source":"# Step 10: Streaming Test with Enhanced Debugging\nprint('='*70)\nprint('?? STEP 10: STREAMING TEST WITH DEBUGGING')\nprint('='*70)\n\nfor model_name, model_path in model_paths.items():\n    if model_name not in servers:\n        print(f'\\n?? Skipping {model_name} - server not running')\n        continue\n    \n    print(f'\\n?? Testing streaming for: {model_name}')\n    \n    client = LlamaCppClient(base_url='http://127.0.0.1:8090')\n    stream_prompt = 'What are the benefits of streaming?'\n    \n    try:\n        print(f'Prompt: \"{stream_prompt}\"')\n        print('Streaming response with debugging:')\n        \n        stream_resp = client.chat.create(\n            messages=[{'role': 'user', 'content': stream_prompt}],\n            max_tokens=80,\n            temperature=0.7,\n            stream=True\n        )\n        \n        out = ''\n        chunk_count = 0\n        first_chunk_printed = False\n        \n        for chunk in stream_resp:\n            chunk_count += 1\n            \n            # Print first chunk structure for debugging\n            if not first_chunk_printed:\n                print(f\"\\n  First chunk structure:\")\n                print(f\"  Type: {type(chunk)}\")\n                if isinstance(chunk, dict):\n                    print(f\"  Keys: {list(chunk.keys())}\")\n                    # Print first level values\n                    for key, value in chunk.items():\n                        if key != 'choices':  # Don't print full choices if it's large\n                            print(f\"    {key}: {type(value)} - {str(value)[:100]}\")\n                        else:\n                            print(f\"    {key}: {type(value)} - length {len(value) if isinstance(value, list) else 'N/A'}\")\n                            if isinstance(value, list) and value:\n                                print(f\"      First choice type: {type(value[0])}\")\n                                if isinstance(value[0], dict):\n                                    print(f\"      First choice keys: {list(value[0].keys())}\")\n                first_chunk_printed = True\n            \n            # Extract content based on actual structure\n            if isinstance(chunk, dict):\n                # Check various possible structures\n                content = None\n                \n                # Structure 1: OpenAI compatible\n                if 'choices' in chunk and isinstance(chunk['choices'], list) and chunk['choices']:\n                    choice = chunk['choices'][0]\n                    if isinstance(choice, dict):\n                        if 'delta' in choice and isinstance(choice['delta'], dict) and 'content' in choice['delta']:\n                            content = choice['delta']['content']\n                        elif 'message' in choice and isinstance(choice['message'], dict) and 'content' in choice['message']:\n                            content = choice['message']['content']\n                        elif 'text' in choice:\n                            content = choice['text']\n                \n                # Structure 2: Direct content\n                elif 'content' in chunk:\n                    content = chunk['content']\n                \n                # Structure 3: Text field\n                elif 'text' in chunk:\n                    content = chunk['text']\n                \n                if content and content.strip():\n                    out += content\n                    print(content, end='', flush=True)\n        \n        print()  # New line after streaming\n        \n        if out:\n            print(f'\\n?? Streaming SUCCESS! Total chunks: {chunk_count}')\n            print(f'   Response length: {len(out)} characters')\n            print(f'   Full response: {out}')\n        else:\n            print(f'\\n?? No content extracted from {chunk_count} chunks')\n            \n    except Exception as e:\n        print(f'?? Streaming error for {model_name}: {e}')\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:14:42.304117Z","iopub.execute_input":"2026-02-05T08:14:42.304742Z","iopub.status.idle":"2026-02-05T08:14:45.770605Z","shell.execute_reply.started":"2026-02-05T08:14:42.304711Z","shell.execute_reply":"2026-02-05T08:14:45.770034Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 10: STREAMING TEST WITH DEBUGGING\n======================================================================\n\n?? Testing streaming for: Llama-3.2-3B-Instruct\nPrompt: \"What are the benefits of streaming?\"\nStreaming response with debugging:\n\n  First chunk structure:\n  Type: <class 'dict'>\n  Keys: ['choices', 'created', 'id', 'model', 'system_fingerprint', 'object']\n    choices: <class 'list'> - length 1\n      First choice type: <class 'dict'>\n      First choice keys: ['finish_reason', 'index', 'delta']\n    created: <class 'int'> - 1770279282\n    id: <class 'str'> - chatcmpl-OJNE9WPxmdFpQsDS6XjHbI02Tio3aoPF\n    model: <class 'str'> - Llama-3.2-3B-Instruct-Q4_K_M.gguf\n    system_fingerprint: <class 'str'> - b1-9f682fb\n    object: <class 'str'> - chat.completion.chunk\nStreaming has numerous benefits, including:\n\n1. **Convenience**: Streaming allows users to access content from anywhere, at any time, as long as they have an internet connection.\n2. **Cost-effective**: Streaming services often offer free or low-cost options for new users, and many subscription-based services provide affordable monthly fees.\n3. **Accessibility**: Streaming enables people with disabilities to access content that may be\n\n?? Streaming SUCCESS! Total chunks: 82\n   Response length: 437 characters\n   Full response: Streaming has numerous benefits, including:\n\n1. **Convenience**: Streaming allows users to access content from anywhere, at any time, as long as they have an internet connection.\n2. **Cost-effective**: Streaming services often offer free or low-cost options for new users, and many subscription-based services provide affordable monthly fees.\n3. **Accessibility**: Streaming enables people with disabilities to access content that may be\n\n?? Testing streaming for: Mistral-7B-Instruct\nPrompt: \"What are the benefits of streaming?\"\nStreaming response with debugging:\n\n  First chunk structure:\n  Type: <class 'dict'>\n  Keys: ['choices', 'created', 'id', 'model', 'system_fingerprint', 'object']\n    choices: <class 'list'> - length 1\n      First choice type: <class 'dict'>\n      First choice keys: ['finish_reason', 'index', 'delta']\n    created: <class 'int'> - 1770279283\n    id: <class 'str'> - chatcmpl-tgO31ySYSWh2gC79MCKHT21tvrmGkegh\n    model: <class 'str'> - Llama-3.2-3B-Instruct-Q4_K_M.gguf\n    system_fingerprint: <class 'str'> - b1-9f682fb\n    object: <class 'str'> - chat.completion.chunk\nThe benefits of streaming include:\n\n1. Convenience: Streaming allows users to access content from anywhere, at any time, as long as they have an internet connection.\n2. Cost-effective: Streaming services often offer a lower cost alternative to buying individual movies or TV shows, and many services also offer free options with ads.\n3. Wide selection: Streaming services have a vast library of content, including new releases\n\n?? Streaming SUCCESS! Total chunks: 82\n   Response length: 426 characters\n   Full response: The benefits of streaming include:\n\n1. Convenience: Streaming allows users to access content from anywhere, at any time, as long as they have an internet connection.\n2. Cost-effective: Streaming services often offer a lower cost alternative to buying individual movies or TV shows, and many services also offer free options with ads.\n3. Wide selection: Streaming services have a vast library of content, including new releases\n\n?? Testing streaming for: Phi-3-mini-4k\nPrompt: \"What are the benefits of streaming?\"\nStreaming response with debugging:\n\n  First chunk structure:\n  Type: <class 'dict'>\n  Keys: ['choices', 'created', 'id', 'model', 'system_fingerprint', 'object']\n    choices: <class 'list'> - length 1\n      First choice type: <class 'dict'>\n      First choice keys: ['finish_reason', 'index', 'delta']\n    created: <class 'int'> - 1770279284\n    id: <class 'str'> - chatcmpl-SkBBBJDb6gO6Iia47fav762E0uEsnb8x\n    model: <class 'str'> - Llama-3.2-3B-Instruct-Q4_K_M.gguf\n    system_fingerprint: <class 'str'> - b1-9f682fb\n    object: <class 'str'> - chat.completion.chunk\nThe benefits of streaming include:\n\n1. Convenience: Streaming allows users to access a vast library of content from anywhere, at any time, as long as they have an internet connection.\n\n2. Cost-effective: Streaming services often offer affordable subscription plans compared to traditional purchasing methods like buying DVDs or CDs.\n\n3. Access to new and emerging content: Streaming platforms provide users with early access to new releases, original\n\n?? Streaming SUCCESS! Total chunks: 82\n   Response length: 450 characters\n   Full response: The benefits of streaming include:\n\n1. Convenience: Streaming allows users to access a vast library of content from anywhere, at any time, as long as they have an internet connection.\n\n2. Cost-effective: Streaming services often offer affordable subscription plans compared to traditional purchasing methods like buying DVDs or CDs.\n\n3. Access to new and emerging content: Streaming platforms provide users with early access to new releases, original\n","output_type":"stream"}],"execution_count":15},{"id":"25842ae8-b016-452e-8af9-83c8c6d03f8e","cell_type":"code","source":"# Step 11: Stop servers for each model\nprint('='*70)\nprint('?? STEP 11: STOP SERVERS')\nprint('='*70)\n\nfor model_name, server in servers.items():\n    print(f'\\n?? Stopping server for: {model_name}')\n    try:\n        server.stop_server()\n        print(f'? Server stopped successfully')\n        time.sleep(2)  # Give time for cleanup\n    except Exception as e:\n        print(f'? Error stopping server: {e}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T08:16:30.974302Z","iopub.execute_input":"2026-02-05T08:16:30.974650Z","iopub.status.idle":"2026-02-05T08:16:47.143657Z","shell.execute_reply.started":"2026-02-05T08:16:30.974622Z","shell.execute_reply":"2026-02-05T08:16:47.142874Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n?? STEP 11: STOP SERVERS\n======================================================================\n\n?? Stopping server for: Llama-3.2-3B-Instruct\n? Server stopped successfully\n\n?? Stopping server for: Mistral-7B-Instruct\n? Server stopped successfully\n\n?? Stopping server for: Phi-3-mini-4k\n? Server stopped successfully\n","output_type":"stream"}],"execution_count":16},{"id":"c4a9e2ce-1f35-40e6-86ae-472e0fa8b7f0","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}