{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T18:39:00.280071Z","iopub.execute_input":"2026-02-07T18:39:00.280389Z","iopub.status.idle":"2026-02-07T18:39:01.303557Z","shell.execute_reply.started":"2026-02-07T18:39:00.280353Z","shell.execute_reply":"2026-02-07T18:39:01.302649Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T06:58:21.227524Z","iopub.execute_input":"2026-02-08T06:58:21.228106Z","iopub.status.idle":"2026-02-08T06:58:21.230019Z","shell.execute_reply.started":"2026-02-08T06:58:21.228077Z","shell.execute_reply":"2026-02-08T06:58:21.229820Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ==============================================================================\n# Step 1: Verify Dual GPU Environment\n# ==============================================================================\n\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"üîç SPLIT-GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nüìä Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\n‚úÖ Dual T4 ready for split-GPU operation!\")\n    print(\"   GPU 0 ‚Üí llama-server (GGUF model inference)\")\n    print(\"   GPU 1 ‚Üí RAPIDS/Graphistry (architecture visualization)\")\nelse:\n    print(\"\\n‚ö†Ô∏è Need 2 GPUs for split operation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:49:30.303093Z","iopub.execute_input":"2026-02-08T08:49:30.303347Z","iopub.status.idle":"2026-02-08T08:49:30.354415Z","shell.execute_reply.started":"2026-02-08T08:49:30.303317Z","shell.execute_reply":"2026-02-08T08:49:30.353708Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîç SPLIT-GPU ENVIRONMENT CHECK\n======================================================================\n\nüìä Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 14913 MiB\n   1, Tesla T4, 15360 MiB, 14913 MiB\n\n‚úÖ Dual T4 ready for split-GPU operation!\n   GPU 0 ‚Üí llama-server (GGUF model inference)\n   GPU 1 ‚Üí RAPIDS/Graphistry (architecture visualization)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 2: Install llamatelemetry v0.1.0\n# ==============================================================================\nprint(\"üì¶ Installing dependencies...\")\n\n# Install llamatelemetry v0.1.0\n!pip install -q https://github.com/llamatelemetry/llamatelemetry/releases/download/v0.1.0/llamatelemetry-v0.1.0-source.tar.gz\n#!pip install -q --no-cache-dir git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\" \"cudf-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy huggingface_hub\n\n# Verify installations\nimport llamatelemetry\nprint(f\"\\n‚úÖ llamatelemetry {llamatelemetry.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"‚úÖ cuDF {cudf.__version__}\")\n    print(f\"‚úÖ cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"‚úÖ Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Graphistry: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:49:43.808150Z","iopub.execute_input":"2026-02-08T08:49:43.808956Z","iopub.status.idle":"2026-02-08T08:51:20.729428Z","shell.execute_reply.started":"2026-02-08T08:49:43.808923Z","shell.execute_reply":"2026-02-08T08:51:20.728741Z"}},"outputs":[{"name":"stdout","text":"üì¶ Installing dependencies...\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m763.5/763.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llamatelemetry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llamatelemetry: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ llamatelemetry v0.1.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llamatelemetry v0.1.0!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v0.1.0 from HuggingFace Hub...\n   Repo: waqasm86/llamatelemetry-binaries\n   File: v0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v0.1.0/llamatelemetry-v0.1.0-cuda12-kagg(‚Ä¶):   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a62c9efc114d4145a8089fcac481cfef"}},"metadata":{}},{"name":"stdout","text":"üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llamatelemetry-v0.1.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llamatelemetry/extract_0.1.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llamatelemetry/extract_0.1.0/llamatelemetry-v0.1.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12\n  Copied 2 libraries to /usr/local/lib/python3.12/dist-packages/llamatelemetry/lib\n‚úÖ Binaries installed successfully!\n\n\n‚úÖ llamatelemetry 0.1.0 installed\n‚úÖ cuDF 25.06.00\n‚úÖ cuGraph 25.06.00\n‚úÖ Graphistry 0.50.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import matplotlib, numpy, pandas, requests, pyarrow\n\nprint(\"requests:\", requests.__version__)\nprint(\"numpy:\", numpy.__version__)\nprint(\"pandas:\", pandas.__version__)\nprint(\"matplotplib:\", matplotlib.__version__)\nprint(\"pyarrow:\", pyarrow.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:51:25.651349Z","iopub.execute_input":"2026-02-08T08:51:25.652586Z","iopub.status.idle":"2026-02-08T08:51:25.657877Z","shell.execute_reply.started":"2026-02-08T08:51:25.652538Z","shell.execute_reply":"2026-02-08T08:51:25.657291Z"}},"outputs":[{"name":"stdout","text":"requests: 2.32.5\nnumpy: 2.0.2\npandas: 2.2.2\nmatplotplib: 3.10.0\npyarrow: 19.0.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:51:27.069901Z","iopub.execute_input":"2026-02-08T08:51:27.070218Z","iopub.status.idle":"2026-02-08T08:51:30.644356Z","shell.execute_reply.started":"2026-02-08T08:51:27.070196Z","shell.execute_reply":"2026-02-08T08:51:30.643577Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m299.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# First, downgrade to compatible versions\n!pip install -q \\\n  opentelemetry-api==1.37.0 \\\n  opentelemetry-sdk==1.37.0 \\\n  opentelemetry-proto==1.37.0 \\\n  opentelemetry-exporter-otlp-proto-common==1.37.0 \\\n  opentelemetry-exporter-otlp-proto-grpc==1.37.0 \\\n  rich==13.9.4 \\\n  --upgrade-strategy=only-if-needed\n\n# Also install the missing bigquery storage package\n!pip install -q google-cloud-bigquery-storage==2.31.0 --upgrade-strategy=only-if-needed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:51:33.375201Z","iopub.execute_input":"2026-02-08T08:51:33.375511Z","iopub.status.idle":"2026-02-08T08:51:40.910457Z","shell.execute_reply.started":"2026-02-08T08:51:33.375481Z","shell.execute_reply":"2026-02-08T08:51:40.909726Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.5/256.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# Step 3: Setup Secrets (Kaggle Secrets)\n# ==============================================================================\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\n\n# Grafana OTLP\nGRAFANA_OTLP_ENDPOINT = secrets.get_secret(\"GRAFANA_OTLP_ENDPOINT\").rstrip(\"/\")\nGRAFANA_OTLP_BASIC_B64 = secrets.get_secret(\"GRAFANA_OTLP_BASIC_B64\")\nGRAFANA_OTLP_INSTANCE_ID = secrets.get_secret(\"GRAFANA_OTLP_INSTANCE_ID\")\nGRAFANA_OTLP_TOKEN = secrets.get_secret(\"GRAFANA_OTLP_TOKEN\")\n\n# HuggingFace\nHF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n\n# Graphistry\n#GRAPHISTRY_USERNAME = secrets.get_secret(\"Graphistry_Username\")\nGRAPHISTRY_PERSONAL_KEY_ID = secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nGRAPHISTRY_PERSONAL_KEY_SECRET = secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n\n# Export OTel env vars for SDK auto-config (explicit v1 paths)\nos.environ[\"OTEL_EXPORTER_OTLP_PROTOCOL\"] = \"http/protobuf\"\nos.environ[\"OTEL_EXPORTER_OTLP_LOGS_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/logs\"\nos.environ[\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/traces\"\nos.environ[\"OTEL_EXPORTER_OTLP_METRICS_ENDPOINT\"] = f\"{GRAFANA_OTLP_ENDPOINT}/v1/metrics\"\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic%20{GRAFANA_OTLP_BASIC_B64}\"\nos.environ[\"OTEL_TRACES_EXPORTER\"] = \"otlp\"\nos.environ[\"OTEL_METRICS_EXPORTER\"] = \"otlp\"\n\n# Login/register\nfrom huggingface_hub import login\nimport graphistry\n\nlogin(HF_TOKEN)\n\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=GRAPHISTRY_PERSONAL_KEY_ID,\n    personal_key_secret=GRAPHISTRY_PERSONAL_KEY_SECRET,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:51:59.552525Z","iopub.execute_input":"2026-02-08T08:51:59.553152Z","iopub.status.idle":"2026-02-08T08:52:01.059198Z","shell.execute_reply.started":"2026-02-08T08:51:59.553122Z","shell.execute_reply":"2026-02-08T08:52:01.058596Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<graphistry.pygraphistry.GraphistryClient at 0x7d2390ec22a0>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"### **Step 4: OpenTelemetry Setup (Grafana OTLP, Silent)**\n\nimport logging\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\n# Hard-silence OTel logs\nlogging.getLogger().setLevel(logging.CRITICAL)\nlogging.getLogger(\"opentelemetry\").setLevel(logging.CRITICAL)\nlogging.getLogger(\"opentelemetry\").propagate = False\n\n# Shut down any previous providers to stop old exporters\ntry:\n    trace.get_tracer_provider().shutdown()\nexcept Exception:\n    pass\ntry:\n    metrics.get_meter_provider().shutdown()\nexcept Exception:\n    pass\n\n# Normalize endpoint\nGRAFANA_OTLP_ENDPOINT = GRAFANA_OTLP_ENDPOINT.rstrip(\"/\")\n\n# Define service resource with GPU context\nresource = Resource.create({\n    \"service.name\": \"llamatelemetry-inference\",\n    \"service.version\": \"0.1.0\",\n    \"service.instance.id\": \"kaggle-t4-worker-1\",\n    \"deployment.environment\": \"kaggle-notebook\",\n    \"host.name\": \"kaggle-gpu-0\",\n    \"gpu.model\": \"Tesla T4\",\n    \"gpu.memory.total\": 15360,  # MB\n    \"gpu.compute_capability\": \"7.5\",\n})\n\n# Create tracer provider with resource (NO console exporter)\ntracer_provider = TracerProvider(resource=resource)\n\n# OTLP exporter to Grafana (explicit traces endpoint)\nspan_exporter = OTLPSpanExporter(\n    endpoint=f\"{GRAFANA_OTLP_ENDPOINT}/v1/traces\",\n    headers={\n        \"Authorization\": f\"Basic {GRAFANA_OTLP_BASIC_B64}\",\n        \"Content-Type\": \"application/x-protobuf\",\n    },\n)\ntracer_provider.add_span_processor(BatchSpanProcessor(span_exporter))\n\ntrace.set_tracer_provider(tracer_provider)\ntracer = trace.get_tracer(__name__)\n\n# Grafana sanity check (silent)\nwith tracer.start_as_current_span(\"grafana.sanity\") as span:\n    span.set_attribute(\"check\", \"ok\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:52:02.186262Z","iopub.execute_input":"2026-02-08T08:52:02.186556Z","iopub.status.idle":"2026-02-08T08:52:02.472041Z","shell.execute_reply.started":"2026-02-08T08:52:02.186530Z","shell.execute_reply":"2026-02-08T08:52:02.471430Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# **Setup MeterProvider (Grafana OTLP, Silent)**\n\nfrom opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n\n# OTLP metric exporter to Grafana (silent)\nmetric_exporter = OTLPMetricExporter(\n    endpoint=f\"{GRAFANA_OTLP_ENDPOINT}/v1/metrics\",\n    headers={\"Authorization\": f\"Basic {GRAFANA_OTLP_BASIC_B64}\"},\n)\nmetric_reader = PeriodicExportingMetricReader(\n    metric_exporter,\n    export_interval_millis=5000,\n)\n\nmeter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])\nmetrics.set_meter_provider(meter_provider)\nmeter = metrics.get_meter(__name__)\n\n# Create custom instruments\nrequest_counter = meter.create_counter(\n    name=\"llm.requests.total\",\n    description=\"Total number of LLM requests\",\n    unit=\"1\",\n)\n\nlatency_histogram = meter.create_histogram(\n    name=\"llm.request.duration\",\n    description=\"LLM request latency\",\n    unit=\"ms\",\n)\n\ntoken_histogram = meter.create_histogram(\n    name=\"llm.tokens.total\",\n    description=\"Token usage per request\",\n    unit=\"{token}\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:52:18.119897Z","iopub.execute_input":"2026-02-08T08:52:18.120775Z","iopub.status.idle":"2026-02-08T08:52:18.129276Z","shell.execute_reply.started":"2026-02-08T08:52:18.120733Z","shell.execute_reply":"2026-02-08T08:52:18.128189Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"### **Step 5: Simplified Model Loading**\n\nfrom huggingface_hub import hf_hub_download\nfrom llamatelemetry.server import ServerManager\nimport os\n\n# Create models directory\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\n\n# Download from a confirmed working repository\nprint(\"Downloading model...\")\nmodel_path = hf_hub_download(\n    repo_id=\"bartowski/Qwen2.5-3B-Instruct-GGUF\",\n    filename=\"Qwen2.5-3B-Instruct-Q4_K_M.gguf\",\n    local_dir=\"/kaggle/working/models\",\n)\nprint(f\"‚úì Model downloaded: {model_path}\")\n\n# Check GPUs\nimport torch\nprint(f\"\\nFound {torch.cuda.device_count()} GPUs:\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n# Start server\nserver = ServerManager(server_url=\"http://127.0.0.1:8090\")\n\n# Minimal working configuration\nserver.start_server(\n    model_path=model_path,\n    gpu_layers=99,\n    tensor_split=\"1.0\",  # Use only first GPU\n    port=8090,\n    host=\"127.0.0.1\",\n    ctx_size=4096,\n    batch_size=512,\n)\n\nprint(\"\\n‚úì Server running on http://127.0.0.1:8090\")\nprint(\"‚úì GPU 0: Used for LLM\")\nprint(\"‚úì GPU 1: Free for Graphistry\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:52:22.836214Z","iopub.execute_input":"2026-02-08T08:52:22.837082Z","iopub.status.idle":"2026-02-08T08:52:33.371730Z","shell.execute_reply.started":"2026-02-08T08:52:22.837051Z","shell.execute_reply":"2026-02-08T08:52:33.370888Z"}},"outputs":[{"name":"stdout","text":"Downloading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Qwen2.5-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443c29fafb624c5599ff50ccdcf5a30d"}},"metadata":{}},{"name":"stdout","text":"‚úì Model downloaded: /kaggle/working/models/Qwen2.5-3B-Instruct-Q4_K_M.gguf\n\nFound 2 GPUs:\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llamatelemetry/binaries/cuda12/llama-server\n  Model: Qwen2.5-3B-Instruct-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n\n‚úì Server running on http://127.0.0.1:8090\n‚úì GPU 0: Used for LLM\n‚úì GPU 1: Free for Graphistry\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"### **Step 6: Instrumented Inference (Silent)**\n\nfrom llamatelemetry.api import LlamaCppClient\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\nimport time\n\nclass InstrumentedLLMClient:\n    \"\"\"LLM client with OpenTelemetry instrumentation\"\"\"\n\n    def __init__(self, base_url: str, tracer, meter):\n        self.client = LlamaCppClient(base_url)\n        self.tracer = tracer\n        self.request_counter = request_counter\n        self.latency_histogram = latency_histogram\n        self.token_histogram = token_histogram\n\n    def chat_completion(self, messages: list, **kwargs):\n        model = kwargs.get(\"model\", \"unknown\")\n        max_tokens = kwargs.get(\"max_tokens\", 100)\n        temperature = kwargs.get(\"temperature\", 0.7)\n\n        with self.tracer.start_as_current_span(\n            name=f\"llm.chat.{model}\",\n            kind=trace.SpanKind.CLIENT,\n        ) as span:\n            try:\n                span.set_attribute(\"llm.system\", \"llama.cpp\")\n                span.set_attribute(\"llm.model\", model)\n                span.set_attribute(\"llm.request.max_tokens\", max_tokens)\n                span.set_attribute(\"llm.request.temperature\", temperature)\n                span.set_attribute(\"llm.request.messages\", len(messages))\n\n                start_time = time.time()\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    **kwargs\n                )\n                latency_ms = (time.time() - start_time) * 1000\n\n                finish_reason = response.choices[0].finish_reason\n                content = response.choices[0].message.content\n\n                span.set_attribute(\"llm.response.finish_reason\", finish_reason)\n                span.set_attribute(\"llm.response.length\", len(content))\n\n                self.request_counter.add(\n                    1,\n                    attributes={\n                        \"model\": model,\n                        \"finish_reason\": finish_reason,\n                        \"status\": \"success\",\n                    }\n                )\n                self.latency_histogram.record(\n                    latency_ms,\n                    attributes={\"model\": model, \"status\": \"success\"}\n                )\n\n                if hasattr(response, 'usage'):\n                    input_tokens = getattr(response.usage, 'prompt_tokens', 0)\n                    output_tokens = getattr(response.usage, 'completion_tokens', 0)\n\n                    span.set_attribute(\"llm.usage.input_tokens\", input_tokens)\n                    span.set_attribute(\"llm.usage.output_tokens\", output_tokens)\n\n                    self.token_histogram.record(\n                        input_tokens,\n                        attributes={\"model\": model, \"token_type\": \"input\"}\n                    )\n                    self.token_histogram.record(\n                        output_tokens,\n                        attributes={\"model\": model, \"token_type\": \"output\"}\n                    )\n\n                span.set_status(Status(StatusCode.OK))\n                return response\n\n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                span.record_exception(e)\n                self.request_counter.add(\n                    1,\n                    attributes={\n                        \"model\": model,\n                        \"status\": \"error\",\n                        \"error_type\": type(e).__name__,\n                    }\n                )\n                raise\n\n# Initialize instrumented client\nllm = InstrumentedLLMClient(\"http://127.0.0.1:8090\", tracer, meter)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:52:35.692132Z","iopub.execute_input":"2026-02-08T08:52:35.693745Z","iopub.status.idle":"2026-02-08T08:52:35.723899Z","shell.execute_reply.started":"2026-02-08T08:52:35.693661Z","shell.execute_reply":"2026-02-08T08:52:35.723244Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#** Step 7: Generate Sample Requests**\n\n# Collect telemetry data for visualization\nfrom opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry import trace\n\nmemory_exporter = InMemorySpanExporter()\ntracer_provider.add_span_processor(SimpleSpanProcessor(memory_exporter))\n\nimport random\nimport time\n\n# Wrap batches in a parent span so child spans have parents\nwith tracer.start_as_current_span(\"llm.batch.requests\"):\n    response = llm.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"What is CUDA?\"}],\n        max_tokens=100,\n        temperature=0.7,\n    )\n    print(f\"Response: {response.choices[0].message.content}\")\n\n    prompts = [\n        \"Explain transformer architecture\",\n        \"What is quantization in LLMs?\",\n        \"How does FlashAttention work?\",\n        \"Describe the attention mechanism\",\n        \"What is GGUF format?\",\n    ]\n\n    responses = []\n    for i, prompt in enumerate(prompts):\n        print(f\"Request {i+1}/{len(prompts)}: {prompt[:50]}...\")\n        resp = llm.chat_completion(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=random.randint(50, 150),\n            temperature=random.uniform(0.5, 0.9),\n        )\n        responses.append(resp)\n        time.sleep(0.5)\n\n    print(f\"Completed {len(responses)} requests\")\n\nwith tracer.start_as_current_span(\"llm.batch.test\"):\n    for i in range(10):\n        llm.chat_completion(\n            messages=[{\"role\": \"user\", \"content\": f\"Test request {i}\"}],\n            max_tokens=50,\n        )\n\ntry:\n    tracer_provider.force_flush()\nexcept Exception:\n    pass\n\nfinished_spans = memory_exporter.get_finished_spans()\nprint(f\"Captured {len(finished_spans)} spans\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T09:02:14.684288Z","iopub.execute_input":"2026-02-08T09:02:14.684583Z","iopub.status.idle":"2026-02-08T09:02:33.710501Z","shell.execute_reply.started":"2026-02-08T09:02:14.684558Z","shell.execute_reply":"2026-02-08T09:02:33.709875Z"}},"outputs":[{"name":"stdout","text":"Response: CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use a GPU for general-purpose processing in addition to its traditional role of accelerating graphics processing.\n\nKey points about CUDA:\n\n1. Purpose: CUDA enables the execution of thousands or millions of threads in parallel, which can be significantly faster than CPU-based approaches when handling tasks that are well-suited to such an architecture.\n\n2. Target Hardware: The main target hardware for\nRequest 1/5: Explain transformer architecture...\nRequest 2/5: What is quantization in LLMs?...\nRequest 3/5: How does FlashAttention work?...\nRequest 4/5: Describe the attention mechanism...\nRequest 5/5: What is GGUF format?...\nCompleted 5 requests\nCaptured 18 spans\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# (intentionally left empty)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T08:22:29.228527Z","iopub.execute_input":"2026-02-08T08:22:29.229017Z","iopub.status.idle":"2026-02-08T08:22:29.232055Z","shell.execute_reply.started":"2026-02-08T08:22:29.228989Z","shell.execute_reply":"2026-02-08T08:22:29.231551Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"### Step 8: Transform Spans to Graph Data**\n\n# Prefer RAPIDS device selection over CuPy\nimport os\nimport rmm\nrmm.reinitialize(devices=[1])\n\nimport cudf\n\nspan_data = []\nfor span in finished_spans:\n    span_data.append({\n        \"span_id\": format(span.context.span_id, \"016x\"),\n        \"parent_span_id\": format(span.parent.span_id, \"016x\") if span.parent else None,\n        \"trace_id\": format(span.context.trace_id, \"032x\"),\n        \"name\": span.name,\n        \"start_time\": span.start_time,\n        \"end_time\": span.end_time,\n        \"duration_ms\": (span.end_time - span.start_time) / 1_000_000,  # nanoseconds to ms\n        \"status\": span.status.status_code.name,\n        \"attributes\": dict(span.attributes) if span.attributes else {},\n    })\n\ndf_spans = cudf.DataFrame(span_data)\nprint(f\"Span DataFrame shape: {df_spans.shape}\")\nprint(df_spans.head())\n\n# Create edges (parent-child relationships). If no parents, fallback to ordering per trace.\nif len(df_spans) > 0:\n    df_edges = df_spans[df_spans[\"parent_span_id\"].notnull()][\n        [\"parent_span_id\", \"span_id\", \"trace_id\"]\n    ].rename(columns={\n        \"parent_span_id\": \"source\",\n        \"span_id\": \"destination\",\n    })\n\n    if len(df_edges) == 0:\n        # Fallback: connect spans by start_time across all spans\n        df_sorted = df_spans.sort_values([\"start_time\"])\n        df_sorted[\"next_span_id\"] = df_sorted[\"span_id\"].shift(-1)\n        df_edges = df_sorted[[\"span_id\", \"next_span_id\", \"trace_id\"]].rename(columns={\n            \"span_id\": \"source\",\n            \"next_span_id\": \"destination\",\n        })\n        df_edges = df_edges[df_edges[\"destination\"].notnull()]\nelse:\n    df_edges = cudf.DataFrame(columns=[\"source\", \"destination\", \"trace_id\"])\n\nprint(f\"Edges DataFrame shape: {df_edges.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T09:02:37.543406Z","iopub.execute_input":"2026-02-08T09:02:37.543698Z","iopub.status.idle":"2026-02-08T09:02:37.578823Z","shell.execute_reply.started":"2026-02-08T09:02:37.543647Z","shell.execute_reply":"2026-02-08T09:02:37.578073Z"}},"outputs":[{"name":"stdout","text":"Span DataFrame shape: (18, 9)\n            span_id    parent_span_id                          trace_id  \\\n0  42fa297f74cdc67f  b87f1a3c74e81ffd  045221a1646f2503df62cc6aa606b8d3   \n1  d513e6752768e944  b87f1a3c74e81ffd  045221a1646f2503df62cc6aa606b8d3   \n2  5d73c7963936917f  b87f1a3c74e81ffd  045221a1646f2503df62cc6aa606b8d3   \n3  d4cbe780a9801fd6  b87f1a3c74e81ffd  045221a1646f2503df62cc6aa606b8d3   \n4  aae43e0a41962af8  b87f1a3c74e81ffd  045221a1646f2503df62cc6aa606b8d3   \n\n               name           start_time             end_time  duration_ms  \\\n0  llm.chat.unknown  1770541334689255810  1770541336263532014  1574.276204   \n1  llm.chat.unknown  1770541336263772564  1770541337629603191  1365.830627   \n2  llm.chat.unknown  1770541338130001282  1770541339643591060  1513.589778   \n3  llm.chat.unknown  1770541340144027093  1770541341231449896  1087.422803   \n4  llm.chat.unknown  1770541341731882169  1770541343596870025  1864.987856   \n\n  status                                         attributes  \n0     OK  {'llm.model': 'unknown', 'llm.request.max_toke...  \n1     OK  {'llm.model': 'unknown', 'llm.request.max_toke...  \n2     OK  {'llm.model': 'unknown', 'llm.request.max_toke...  \n3     OK  {'llm.model': 'unknown', 'llm.request.max_toke...  \n4     OK  {'llm.model': 'unknown', 'llm.request.max_toke...  \nEdges DataFrame shape: (16, 3)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Step 9 First ensure you have actual edges\nimport os, json\nimport graphistry\n\nif len(df_edges) > 0:\n    g = graphistry.edges(df_edges, \"source\", \"destination\")\n    g = g.nodes(df_spans, \"span_id\")\n    g = g.bind(point_title=\"name\", edge_title=\"trace_id\")\n    g = g.layout_settings(play=0)\n    url = g.plot(render=False)\n    print(f\"Graph URL: {url}\")\n\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    with open(\"/kaggle/working/graphistry_trace_url.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"trace_graph_url\": url}, f, indent=2)\nelse:\n    print(\"No edges in df_edges! (No spans captured)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T09:04:41.473811Z","iopub.execute_input":"2026-02-08T09:04:41.474386Z","iopub.status.idle":"2026-02-08T09:04:42.984944Z","shell.execute_reply.started":"2026-02-08T09:04:41.474355Z","shell.execute_reply":"2026-02-08T09:04:42.984233Z"}},"outputs":[{"name":"stdout","text":"Graph URL: https://hub.graphistry.com/graph/graph.html?dataset=ca8434889f9f4d1396df59d4fe798aca&type=arrow&viztoken=e85c7e34-1fee-40a6-8f24-7cea7873096d&usertag=28c8414b-pygraphistry-0.50.6&splashAfter=1770541497&info=true&play=0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Step 10 Create Trace Graph Visualization**\nimport os, json\nimport graphistry\n\nif len(df_edges) > 0:\n    g = graphistry.edges(df_edges, \"source\", \"destination\")\n    g = g.nodes(df_spans, \"span_id\")\n    g = g.bind(\n        point_title=\"name\",\n        point_size=\"duration_ms\",\n        point_color=\"status\",\n        edge_title=\"trace_id\",\n    )\n\n    g = g.layout_settings(play=0)\n\n    # Only encode if status exists\n    if \"status\" in df_spans.columns:\n        g = g.encode_point_color(\"status\", categorical_mapping={\n            \"OK\": \"#4CAF50\",\n            \"ERROR\": \"#F44336\",\n            \"UNSET\": \"#9E9E9E\",\n        }, as_categorical=True)\n\n    url = g.plot(render=False)\n    print(f\"üîó Trace Graph Dashboard: {url}\")\n\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    with open(\"/kaggle/working/graphistry_trace_graph_url.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"trace_graph_dashboard_url\": url}, f, indent=2)\nelse:\n    print(\"Skipping Graphistry plot: no edges available\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T09:06:04.603863Z","iopub.execute_input":"2026-02-08T09:06:04.604479Z","iopub.status.idle":"2026-02-08T09:06:06.051131Z","shell.execute_reply.started":"2026-02-08T09:06:04.604450Z","shell.execute_reply":"2026-02-08T09:06:06.050354Z"}},"outputs":[{"name":"stdout","text":"üîó Trace Graph Dashboard: https://hub.graphistry.com/graph/graph.html?dataset=7700674cf576440e854913a06997f59b&type=arrow&viztoken=5385d4a7-1d29-4e0e-a9af-0b12c0218c26&usertag=28c8414b-pygraphistry-0.50.6&splashAfter=1770541581&info=true&play=0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### **Step 11: Metrics Dashboards with Plotly (GPU 1) (5 min)**\n\nimport cudf\nimport rmm\nimport pandas as pd\n\n# Ensure GPU 1 for RAPIDS operations\nrmm.reinitialize(devices=[1])\n\nmetrics_data = []\nfor span in finished_spans:\n    attrs = span.attributes or {}\n    metrics_data.append({\n        \"timestamp\": span.start_time,  # keep as ns int\n        \"duration_ms\": (span.end_time - span.start_time) / 1_000_000,\n        \"model\": attrs.get(\"llm.model\", \"unknown\"),\n        \"input_tokens\": attrs.get(\"llm.usage.input_tokens\", 0),\n        \"output_tokens\": attrs.get(\"llm.usage.output_tokens\", 0),\n        \"total_tokens\": attrs.get(\"llm.usage.input_tokens\", 0) + attrs.get(\"llm.usage.output_tokens\", 0),\n        \"status\": span.status.status_code.name,\n    })\n\ndf_metrics = cudf.DataFrame(metrics_data)\nif len(df_metrics) == 0:\n    print(\"No spans available for Plotly metrics\")\nelse:\n    df_metrics = df_metrics.sort_values(\"timestamp\")\n    print(f\"Metrics DataFrame shape: {df_metrics.shape}\")\n\n    # Convert to pandas for Plotly rendering\n    dfp = df_metrics.to_pandas()\n    dfp[\"timestamp\"] = pd.to_datetime(dfp[\"timestamp\"], unit=\"ns\")\n\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=(\n            \"Request Latency Distribution\",\n            \"Token Usage Over Time\",\n            \"Tokens per Request (Input vs Output)\",\n            \"Request Rate Over Time\"\n        ),\n        specs=[\n            [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n        ]\n    )\n\n    fig.add_trace(\n        go.Histogram(x=dfp[\"duration_ms\"], nbinsx=20, name=\"Latency (ms)\"),\n        row=1, col=1\n    )\n\n    fig.add_trace(\n        go.Scatter(x=dfp[\"timestamp\"], y=dfp[\"total_tokens\"], mode=\"lines+markers\", name=\"Total Tokens\"),\n        row=1, col=2\n    )\n\n    fig.add_trace(\n        go.Scatter(x=dfp[\"timestamp\"], y=dfp[\"input_tokens\"], mode=\"lines\", name=\"Input Tokens\"),\n        row=2, col=1\n    )\n    fig.add_trace(\n        go.Scatter(x=dfp[\"timestamp\"], y=dfp[\"output_tokens\"], mode=\"lines\", name=\"Output Tokens\"),\n        row=2, col=1\n    )\n\n    fig.add_trace(\n        go.Scatter(x=dfp[\"timestamp\"], y=[1]*len(dfp), mode=\"lines+markers\", name=\"Requests\"),\n        row=2, col=2\n    )\n\n    fig.update_layout(height=700, showlegend=True, title_text=\"LLM Metrics Dashboard\")\n    fig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T09:13:43.563333Z","iopub.execute_input":"2026-02-08T09:13:43.563892Z","iopub.status.idle":"2026-02-08T09:13:45.710519Z","shell.execute_reply.started":"2026-02-08T09:13:43.563861Z","shell.execute_reply":"2026-02-08T09:13:45.709934Z"}},"outputs":[{"name":"stdout","text":"Metrics DataFrame shape: (18, 7)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"82858bc8-2ae5-4af7-a6b1-a5c3eb06c6cb\" class=\"plotly-graph-div\" style=\"height:700px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"82858bc8-2ae5-4af7-a6b1-a5c3eb06c6cb\")) {                    Plotly.newPlot(                        \"82858bc8-2ae5-4af7-a6b1-a5c3eb06c6cb\",                        [{\"name\":\"Latency (ms)\",\"nbinsx\":20,\"x\":[11064.540011,1574.276204,1365.830627,1513.589778,1087.422803,1864.987856,1156.112644,7446.559314,750.302012,757.874323,754.102932,620.681877,759.146977,757.928991,758.58339,759.837477,763.633107,763.467882],\"type\":\"histogram\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines+markers\",\"name\":\"Total Tokens\",\"x\":[\"2026-02-08T09:02:14.689196\",\"2026-02-08T09:02:14.689255\",\"2026-02-08T09:02:16.263772\",\"2026-02-08T09:02:18.130001\",\"2026-02-08T09:02:20.144027\",\"2026-02-08T09:02:21.731882\",\"2026-02-08T09:02:24.097290\",\"2026-02-08T09:02:25.754040\",\"2026-02-08T09:02:25.754094\",\"2026-02-08T09:02:26.504493\",\"2026-02-08T09:02:27.262461\",\"2026-02-08T09:02:28.016654\",\"2026-02-08T09:02:28.637431\",\"2026-02-08T09:02:29.396694\",\"2026-02-08T09:02:30.154744\",\"2026-02-08T09:02:30.913416\",\"2026-02-08T09:02:31.673345\",\"2026-02-08T09:02:32.437090\"],\"y\":[0,133,126,140,108,158,112,0,83,83,83,74,83,83,83,83,83,83],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"Input Tokens\",\"x\":[\"2026-02-08T09:02:14.689196\",\"2026-02-08T09:02:14.689255\",\"2026-02-08T09:02:16.263772\",\"2026-02-08T09:02:18.130001\",\"2026-02-08T09:02:20.144027\",\"2026-02-08T09:02:21.731882\",\"2026-02-08T09:02:24.097290\",\"2026-02-08T09:02:25.754040\",\"2026-02-08T09:02:25.754094\",\"2026-02-08T09:02:26.504493\",\"2026-02-08T09:02:27.262461\",\"2026-02-08T09:02:28.016654\",\"2026-02-08T09:02:28.637431\",\"2026-02-08T09:02:29.396694\",\"2026-02-08T09:02:30.154744\",\"2026-02-08T09:02:30.913416\",\"2026-02-08T09:02:31.673345\",\"2026-02-08T09:02:32.437090\"],\"y\":[0,33,33,38,35,33,35,0,33,33,33,33,33,33,33,33,33,33],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines\",\"name\":\"Output Tokens\",\"x\":[\"2026-02-08T09:02:14.689196\",\"2026-02-08T09:02:14.689255\",\"2026-02-08T09:02:16.263772\",\"2026-02-08T09:02:18.130001\",\"2026-02-08T09:02:20.144027\",\"2026-02-08T09:02:21.731882\",\"2026-02-08T09:02:24.097290\",\"2026-02-08T09:02:25.754040\",\"2026-02-08T09:02:25.754094\",\"2026-02-08T09:02:26.504493\",\"2026-02-08T09:02:27.262461\",\"2026-02-08T09:02:28.016654\",\"2026-02-08T09:02:28.637431\",\"2026-02-08T09:02:29.396694\",\"2026-02-08T09:02:30.154744\",\"2026-02-08T09:02:30.913416\",\"2026-02-08T09:02:31.673345\",\"2026-02-08T09:02:32.437090\"],\"y\":[0,100,93,102,73,125,77,0,50,50,50,41,50,50,50,50,50,50],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines+markers\",\"name\":\"Requests\",\"x\":[\"2026-02-08T09:02:14.689196\",\"2026-02-08T09:02:14.689255\",\"2026-02-08T09:02:16.263772\",\"2026-02-08T09:02:18.130001\",\"2026-02-08T09:02:20.144027\",\"2026-02-08T09:02:21.731882\",\"2026-02-08T09:02:24.097290\",\"2026-02-08T09:02:25.754040\",\"2026-02-08T09:02:25.754094\",\"2026-02-08T09:02:26.504493\",\"2026-02-08T09:02:27.262461\",\"2026-02-08T09:02:28.016654\",\"2026-02-08T09:02:28.637431\",\"2026-02-08T09:02:29.396694\",\"2026-02-08T09:02:30.154744\",\"2026-02-08T09:02:30.913416\",\"2026-02-08T09:02:31.673345\",\"2026-02-08T09:02:32.437090\"],\"y\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Request Latency Distribution\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Token Usage Over Time\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Tokens per Request (Input vs Output)\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Request Rate Over Time\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"LLM Metrics Dashboard\"},\"height\":700,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('82858bc8-2ae5-4af7-a6b1-a5c3eb06c6cb');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nKey Learnings\n\n### **1. OpenTelemetry Integration**\n#- ‚úÖ Full instrumentation with traces, metrics, and logs\n#- ‚úÖ Semantic conventions for GenAI workloads\n#- ‚úÖ Custom resource attributes for GPU context\n#- ‚úÖ Flexible export to multiple backends\n\n### **2. Trace Visualization**\n#- ‚úÖ Parent-child span relationships as interactive graphs\n#- ‚úÖ Request flow waterfall diagrams\n#- ‚úÖ Error propagation visualization\n#- ‚úÖ GPU-accelerated graph analytics with Graphistry\n\n### **3. Metrics Monitoring**\n#- ‚úÖ Request latency tracking\n#- ‚úÖ Token usage analysis\n#- ‚úÖ Throughput monitoring\n#- ‚úÖ Real-time dashboards with Plotly\n\n### **4. Production Patterns**\n#- ‚úÖ Context propagation for distributed tracing\n#- ‚úÖ Batch export for performance\n#- ‚úÖ Error handling and exception recording\n#- ‚úÖ Resource attribution for multi-GPU environments\n\n---\n\n## Next Steps\n\n#- **Notebook 15:** Real-time performance monitoring with live metrics\n#- **Notebook 16:** End-to-end production observability stack\n#- Integrate with external collectors (Jaeger, Tempo, DataDog)\n#- Add custom span processors for filtering/enrichment\n#- Implement sampling strategies for high-volume workloads\n\n\n#**üéØ Objectives Achieved:**\n#‚úÖ CUDA Inference (GPU 0)\n#‚úÖ LLM Observability (GPU 0)\n#‚úÖ Graphistry + Plotly Visualizations (GPU 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T07:16:31.471293Z","iopub.execute_input":"2026-02-08T07:16:31.471976Z","iopub.status.idle":"2026-02-08T07:16:31.477402Z","shell.execute_reply.started":"2026-02-08T07:16:31.471946Z","shell.execute_reply":"2026-02-08T07:16:31.476519Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_55/3664757259.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Key Learnings\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (3664757259.py, line 1)","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}