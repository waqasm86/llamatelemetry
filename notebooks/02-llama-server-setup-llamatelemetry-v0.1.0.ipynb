{"cells": [{"cell_type": "markdown", "id": "32eed8ef", "metadata": {}, "source": ["## Step 1: Environment Verification"]}, {"cell_type": "code", "execution_count": null, "id": "cbf1e6ab", "metadata": {}, "outputs": [], "source": ["import subprocess\n", "import os\n", "\n", "print(\"=\"*70)\n", "print(\"ðŸ” ENVIRONMENT CHECK\")\n", "print(\"=\"*70)\n", "\n", "# GPU check\n", "result = subprocess.run([\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \n", "                         \"--format=csv,noheader\"], capture_output=True, text=True)\n", "print(\"\\nðŸ“Š GPUs Available:\")\n", "for line in result.stdout.strip().split('\\n'):\n", "    print(f\"   {line}\")\n", "\n", "# CUDA version\n", "print(\"\\nðŸ“Š CUDA Version:\")\n", "!nvcc --version | grep release\n", "\n", "print(\"\\nâœ… Environment ready for llama-server configuration\")"]}, {"cell_type": "markdown", "id": "8b2c6025", "metadata": {}, "source": ["## Step 2: Install llamatelemetry and Dependencies"]}, {"cell_type": "code", "execution_count": null, "id": "5fb85191", "metadata": {}, "outputs": [], "source": ["%%time\n", "# Install llamatelemetry v0.1.0 (force fresh install to ensure correct binaries)\n", "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llamatelemetry/llamatelemetry.git@v0.1.0\n", "!pip install -q huggingface_hub sseclient-py\n", "\n", "import llamatelemetry\n", "print(f\"âœ… llamatelemetry {llamatelemetry.__version__} installed\")"]}, {"cell_type": "markdown", "id": "bff35470", "metadata": {}, "source": ["## Step 3: Understanding Server Configuration Options\n", "\n", "llama-server has many configuration options. Here's a comprehensive overview:"]}, {"cell_type": "code", "execution_count": null, "id": "17b81ee0", "metadata": {}, "outputs": [], "source": ["from llamatelemetry.server import ServerManager\n", "from llamatelemetry.api.multigpu import MultiGPUConfig, SplitMode\n", "\n", "# Display all configuration options\n", "print(\"=\"*70)\n", "print(\"ðŸ“‹ LLAMA-SERVER CONFIGURATION OPTIONS\")\n", "print(\"=\"*70)\n", "\n", "config_options = {\n", "    \"Model Settings\": {\n", "        \"--model, -m\": \"Path to GGUF model file\",\n", "        \"--alias, -a\": \"Model alias for API responses\",\n", "        \"--ctx-size, -c\": \"Context size (default: 4096)\",\n", "        \"--batch-size, -b\": \"Batch size for prompt processing\",\n", "        \"--ubatch-size\": \"Physical batch size (default: 512)\",\n", "    },\n", "    \"GPU Settings\": {\n", "        \"--n-gpu-layers, -ngl\": \"Layers to offload to GPU (99 = all)\",\n", "        \"--main-gpu, -mg\": \"Main GPU for computations\",\n", "        \"--tensor-split, -ts\": \"VRAM distribution across GPUs\",\n", "        \"--split-mode, -sm\": \"Split mode: layer, row, none\",\n", "    },\n", "    \"Performance\": {\n", "        \"--flash-attn, -fa\": \"Enable FlashAttention (faster)\",\n", "        \"--threads, -t\": \"CPU threads for generation\",\n", "        \"--threads-batch, -tb\": \"CPU threads for batch processing\",\n", "        \"--cont-batching\": \"Enable continuous batching\",\n", "        \"--parallel, -np\": \"Number of parallel sequences\",\n", "    },\n", "    \"Server Settings\": {\n", "        \"--host\": \"Host address (default: 127.0.0.1)\",\n", "        \"--port\": \"Port number (default: 8080)\",\n", "        \"--timeout\": \"Server timeout in seconds\",\n", "        \"--embeddings\": \"Enable embeddings endpoint\",\n", "    },\n", "}\n", "\n", "for category, options in config_options.items():\n", "    print(f\"\\nðŸ“Œ {category}:\")\n", "    for flag, desc in options.items():\n", "        print(f\"   {flag:25} {desc}\")"]}, {"cell_type": "markdown", "id": "2b95c505", "metadata": {}, "source": ["## Step 4: Configuration Presets for Kaggle T4"]}, {"cell_type": "code", "execution_count": null, "id": "12d6d414", "metadata": {}, "outputs": [], "source": ["from llamatelemetry.api.multigpu import kaggle_t4_dual_config, kaggle_t4_single_config\n", "\n", "print(\"=\"*70)\n", "print(\"ðŸ“‹ KAGGLE T4 CONFIGURATION PRESETS\")\n", "print(\"=\"*70)\n", "\n", "# Single GPU configuration (use GPU 0 only)\n", "# Note: kaggle_t4_single_config works for Kaggle single T4 as well (same GPU)\n", "print(\"\\nðŸ”¹ Single T4 Configuration (15GB VRAM):\")\n", "single_config = kaggle_t4_single_config()\n", "print(f\"   GPU Layers: {single_config.n_gpu_layers}\")\n", "print(f\"   Context Size: {single_config.ctx_size}\")\n", "print(f\"   Batch Size: {single_config.batch_size}\")\n", "print(f\"   Flash Attention: {single_config.flash_attention}\")\n", "print(f\"   Best for: Models up to ~7B Q4_K_M\")\n", "\n", "# Dual GPU configuration (split across both)\n", "print(\"\\nðŸ”¹ Dual T4 Configuration (30GB VRAM total):\")\n", "dual_config = kaggle_t4_dual_config()\n", "print(f\"   GPU Layers: {dual_config.n_gpu_layers}\")\n", "print(f\"   Context Size: {dual_config.ctx_size}\")\n", "print(f\"   Tensor Split: {dual_config.tensor_split}\")\n", "print(f\"   Split Mode: {dual_config.split_mode}\")\n", "print(f\"   Flash Attention: {dual_config.flash_attention}\")\n", "print(f\"   Best for: Models up to ~13B Q4_K_M\")\n", "\n", "# Split GPU configuration (LLM on GPU 0, RAPIDS on GPU 1)\n", "print(\"\\nðŸ”¹ Split-GPU Configuration (Recommended):\")\n", "print(\"   GPU 0: llama-server (LLM inference)\")\n", "print(\"   GPU 1: RAPIDS/Graphistry (graph processing)\")\n", "print(\"   Best for: Combined LLM + visualization workflows\")"]}, {"cell_type": "markdown", "id": "da13fa6e", "metadata": {}, "source": ["## Step 5: Download Test Model"]}, {"cell_type": "code", "execution_count": null, "id": "83a39360", "metadata": {}, "outputs": [], "source": ["%%time\n", "from huggingface_hub import hf_hub_download\n", "import os\n", "\n", "# Download a small model for testing configurations\n", "MODEL_REPO = \"unsloth/gemma-3-1b-it-GGUF\"\n", "MODEL_FILE = \"gemma-3-1b-it-Q4_K_M.gguf\"\n", "\n", "print(f\"ðŸ“¥ Downloading {MODEL_FILE} for configuration testing...\")\n", "\n", "model_path = hf_hub_download(\n", "    repo_id=MODEL_REPO,\n", "    filename=MODEL_FILE,\n", "    local_dir=\"/kaggle/working/models\"\n", ")\n", "\n", "size_gb = os.path.getsize(model_path) / (1024**3)\n", "print(f\"\\nâœ… Model downloaded: {model_path}\")\n", "print(f\"   Size: {size_gb:.2f} GB\")"]}, {"cell_type": "markdown", "id": "4c09c3a3", "metadata": {}, "source": ["## Step 6: Basic Server Configuration"]}, {"cell_type": "code", "execution_count": null, "id": "d518b25b", "metadata": {}, "outputs": [], "source": ["from llamatelemetry.server import ServerManager\n", "\n", "# Create basic configuration settings (used as parameters to start_server)\n", "print(\"=\"*70)\n", "print(\"ðŸ”§ BASIC SERVER CONFIGURATION\")\n", "print(\"=\"*70)\n", "\n", "# Configuration parameters\n", "config = {\n", "    \"model_path\": model_path,\n", "    \"host\": \"127.0.0.1\",\n", "    \"port\": 8080,\n", "    \"gpu_layers\": 99,       # Offload all layers to GPU\n", "    \"ctx_size\": 4096,       # 4K context\n", "    \"batch_size\": 512,      # Batch size for prompt processing\n", "}\n", "\n", "print(f\"\\nðŸ“‹ Configuration:\")\n", "print(f\"   Model: {config['model_path']}\")\n", "print(f\"   Host: {config['host']}:{config['port']}\")\n", "print(f\"   GPU Layers: {config['gpu_layers']}\")\n", "print(f\"   Context: {config['ctx_size']}\")\n", "\n", "# Start server using ServerManager.start_server() API\n", "server = ServerManager(server_url=f\"http://{config['host']}:{config['port']}\")\n", "print(\"\\nðŸš€ Starting server with basic configuration...\")\n", "\n", "try:\n", "    server.start_server(\n", "        model_path=config['model_path'],\n", "        host=config['host'],\n", "        port=config['port'],\n", "        gpu_layers=config['gpu_layers'],\n", "        ctx_size=config['ctx_size'],\n", "        timeout=60,\n", "        verbose=True\n", "    )\n", "    print(\"\\nâœ… Server started successfully!\")\n", "except Exception as e:\n", "    print(f\"\\nâŒ Server failed to start: {e}\")"]}, {"cell_type": "markdown", "id": "68f127a0", "metadata": {}, "source": ["## Step 7: Server Health Monitoring"]}, {"cell_type": "code", "execution_count": null, "id": "3037817f", "metadata": {}, "outputs": [], "source": ["import requests\n", "import json\n", "\n", "print(\"=\"*70)\n", "print(\"ðŸ¥ SERVER HEALTH MONITORING\")\n", "print(\"=\"*70)\n", "\n", "# Health check\n", "try:\n", "    health = requests.get(\"http://127.0.0.1:8080/health\", timeout=5)\n", "    print(f\"\\nðŸ“Š Health Status: {health.json()}\")\n", "except Exception as e:\n", "    print(f\"âŒ Health check failed: {e}\")\n", "\n", "# Get server slots info\n", "try:\n", "    slots = requests.get(\"http://127.0.0.1:8080/slots\", timeout=5)\n", "    print(f\"\\nðŸ“Š Server Slots:\")\n", "    for slot in slots.json():\n", "        print(f\"   Slot {slot.get('id', 'N/A')}: {slot.get('state', 'unknown')}\")\n", "except Exception as e:\n", "    print(f\"   Slots endpoint not available: {e}\")\n", "\n", "# Get model info\n", "try:\n", "    props = requests.get(\"http://127.0.0.1:8080/props\", timeout=5)\n", "    print(f\"\\nðŸ“Š Model Properties:\")\n", "    data = props.json()\n", "    print(f\"   Model: {data.get('default_generation_settings', {}).get('model', 'N/A')}\")\n", "    print(f\"   Context: {data.get('default_generation_settings', {}).get('n_ctx', 'N/A')}\")\n", "except Exception as e:\n", "    print(f\"   Props endpoint not available: {e}\")"]}, {"cell_type": "markdown", "id": "8d0968c3", "metadata": {}, "source": ["## Step 8: Stop Server and Test Advanced Configuration"]}, {"cell_type": "code", "execution_count": null, "id": "1b1a5cd1", "metadata": {}, "outputs": [], "source": ["# Stop current server\n", "print(\"ðŸ›‘ Stopping current server...\")\n", "server.stop_server()\n", "\n", "import time\n", "time.sleep(2)  # Wait for port to be released\n", "\n", "print(\"\\nâœ… Server stopped\")"]}, {"cell_type": "markdown", "id": "54b06ed1", "metadata": {}, "source": ["## Step 9: High-Performance Configuration\n", "\n", "Optimized for maximum throughput on Kaggle T4."]}, {"cell_type": "code", "execution_count": null, "id": "d8b3ca62", "metadata": {}, "outputs": [], "source": ["print(\"=\"*70)\n", "print(\"âš¡ HIGH-PERFORMANCE CONFIGURATION\")\n", "print(\"=\"*70)\n", "\n", "# High-performance configuration parameters\n", "hp_config = {\n", "    \"model_path\": model_path,\n", "    \"host\": \"127.0.0.1\",\n", "    \"port\": 8080,\n", "    \n", "    # GPU settings - maximize GPU utilization\n", "    \"gpu_layers\": 99,\n", "    \n", "    # Context and batching\n", "    \"ctx_size\": 8192,      # Larger context\n", "    \"batch_size\": 1024,    # Larger batch for prompt processing\n", "    \"ubatch_size\": 512,    # Physical batch size\n", "    \n", "    # Parallelism\n", "    \"n_parallel\": 4,       # 4 parallel request slots\n", "}\n", "\n", "print(f\"\\nðŸ“‹ High-Performance Settings:\")\n", "print(f\"   Context Size: {hp_config['ctx_size']} tokens\")\n", "print(f\"   Batch Size: {hp_config['batch_size']}\")\n", "print(f\"   Parallel Slots: {hp_config['n_parallel']}\")\n", "\n", "# Create new server manager\n", "server = ServerManager(server_url=f\"http://{hp_config['host']}:{hp_config['port']}\")\n", "\n", "# Start with high-performance config\n", "print(\"\\nðŸš€ Starting server with high-performance configuration...\")\n", "try:\n", "    server.start_server(\n", "        model_path=hp_config['model_path'],\n", "        host=hp_config['host'],\n", "        port=hp_config['port'],\n", "        gpu_layers=hp_config['gpu_layers'],\n", "        ctx_size=hp_config['ctx_size'],\n", "        batch_size=hp_config['batch_size'],\n", "        ubatch_size=hp_config['ubatch_size'],\n", "        n_parallel=hp_config['n_parallel'],\n", "        timeout=60,\n", "        verbose=True\n", "    )\n", "    print(\"\\nâœ… High-performance server started!\")\n", "except Exception as e:\n", "    print(f\"\\nâŒ Server failed to start: {e}\")"]}, {"cell_type": "markdown", "id": "12a2acb0", "metadata": {}, "source": ["## Step 10: Benchmark Inference Performance"]}, {"cell_type": "code", "execution_count": null, "id": "645fd1ec", "metadata": {}, "outputs": [], "source": ["import time\n", "from llamatelemetry.api.client import LlamaCppClient\n", "\n", "print(\"=\"*70)\n", "print(\"ðŸ“Š INFERENCE PERFORMANCE BENCHMARK\")\n", "print(\"=\"*70)\n", "\n", "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n", "\n", "# Benchmark parameters\n", "prompts = [\n", "    \"Explain quantum computing in simple terms.\",\n", "    \"Write a haiku about machine learning.\",\n", "    \"What are the benefits of GPU acceleration?\",\n", "]\n", "\n", "print(\"\\nðŸƒ Running benchmark with 3 prompts...\\n\")\n", "\n", "total_tokens = 0\n", "total_time = 0\n", "\n", "for i, prompt in enumerate(prompts, 1):\n", "    start = time.time()\n", "    \n", "    response = client.chat.create(\n", "        messages=[{\"role\": \"user\", \"content\": prompt}],\n", "        max_tokens=100,\n", "        temperature=0.7\n", "    )\n", "    \n", "    elapsed = time.time() - start\n", "    tokens = response.usage.completion_tokens\n", "    \n", "    total_tokens += tokens\n", "    total_time += elapsed\n", "    \n", "    print(f\"   Prompt {i}: {tokens} tokens in {elapsed:.2f}s ({tokens/elapsed:.1f} tok/s)\")\n", "\n", "print(f\"\\nðŸ“Š Benchmark Results:\")\n", "print(f\"   Total Tokens: {total_tokens}\")\n", "print(f\"   Total Time: {total_time:.2f}s\")\n", "print(f\"   Average Speed: {total_tokens/total_time:.1f} tokens/second\")"]}, {"cell_type": "markdown", "id": "6538c39c", "metadata": {}, "source": ["## Step 11: GPU Memory Monitoring"]}, {"cell_type": "code", "execution_count": null, "id": "c72be8f0", "metadata": {}, "outputs": [], "source": ["print(\"=\"*70)\n", "print(\"ðŸ“Š GPU MEMORY MONITORING\")\n", "print(\"=\"*70)\n", "\n", "# Current memory usage\n", "print(\"\\nðŸ”¹ Current GPU Memory Usage:\")\n", "!nvidia-smi --query-gpu=index,name,memory.used,memory.total,memory.free --format=csv\n", "\n", "# Memory usage over time (single snapshot)\n", "import subprocess\n", "result = subprocess.run(\n", "    [\"nvidia-smi\", \"--query-gpu=index,memory.used,utilization.gpu\", \"--format=csv,noheader\"],\n", "    capture_output=True, text=True\n", ")\n", "\n", "print(\"\\nðŸ”¹ GPU Utilization:\")\n", "for line in result.stdout.strip().split('\\n'):\n", "    parts = line.split(', ')\n", "    if len(parts) >= 3:\n", "        print(f\"   GPU {parts[0]}: {parts[1]} used, {parts[2]} utilization\")"]}, {"cell_type": "markdown", "id": "f54554a7", "metadata": {}, "source": ["## Step 12: Command-Line Reference\n", "\n", "For running llama-server directly from command line."]}, {"cell_type": "code", "execution_count": null, "id": "0de3bfdb", "metadata": {}, "outputs": [], "source": ["print(\"=\"*70)\n", "print(\"ðŸ“‹ COMMAND-LINE REFERENCE\")\n", "print(\"=\"*70)\n", "\n", "cli_examples = f\"\"\"\n", "ðŸ”¹ Basic Start:\n", "   llama-server -m {model_path} --host 0.0.0.0 --port 8080\n", "\n", "ðŸ”¹ Single GPU (GPU 0, 15GB):\n", "   llama-server -m {model_path} \\\\\n", "       --host 0.0.0.0 --port 8080 \\\\\n", "       --n-gpu-layers 99 --main-gpu 0 \\\\\n", "       --ctx-size 4096 --flash-attn\n", "\n", "ðŸ”¹ Dual GPU (30GB total):\n", "   llama-server -m {model_path} \\\\\n", "       --host 0.0.0.0 --port 8080 \\\\\n", "       --n-gpu-layers 99 \\\\\n", "       --tensor-split 0.5,0.5 \\\\\n", "       --split-mode layer \\\\\n", "       --ctx-size 8192 --flash-attn\n", "\n", "ðŸ”¹ High-Performance:\n", "   llama-server -m {model_path} \\\\\n", "       --host 0.0.0.0 --port 8080 \\\\\n", "       --n-gpu-layers 99 --flash-attn \\\\\n", "       --ctx-size 8192 --batch-size 1024 \\\\\n", "       --parallel 4 --cont-batching \\\\\n", "       --threads 4 --threads-batch 4\n", "\n", "ðŸ”¹ With Embeddings:\n", "   llama-server -m {model_path} \\\\\n", "       --host 0.0.0.0 --port 8080 \\\\\n", "       --n-gpu-layers 99 --flash-attn \\\\\n", "       --embeddings\n", "\"\"\"\n", "\n", "print(cli_examples)"]}, {"cell_type": "markdown", "id": "4b8bc155", "metadata": {}, "source": ["## Step 13: Cleanup"]}, {"cell_type": "code", "execution_count": null, "id": "58236462", "metadata": {}, "outputs": [], "source": ["# Stop server\n", "print(\"ðŸ›‘ Stopping server...\")\n", "server.stop_server()\n", "\n", "print(\"\\nâœ… Server stopped. Resources freed.\")\n", "\n", "# Final GPU status\n", "print(\"\\nðŸ“Š Final GPU Memory Status:\")\n", "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"]}, {"cell_type": "markdown", "id": "9ac5776e", "metadata": {}, "source": ["## ðŸ“š Summary\n", "\n", "You've learned:\n", "1. âœ… Server configuration options\n", "2. âœ… Kaggle T4 presets (single/dual GPU)\n", "3. âœ… High-performance tuning\n", "4. âœ… Health monitoring\n", "5. âœ… Command-line reference\n", "\n", "## Configuration Tips for Kaggle T4\n", "\n", "| Model Size | Quantization | VRAM | Context | Config |\n", "|------------|--------------|------|---------|--------|\n", "| 1-3B | Q4_K_M | ~2GB | 8192 | Single T4 |\n", "| 4-7B | Q4_K_M | ~5GB | 4096 | Single T4 |\n", "| 8-13B | Q4_K_M | ~8GB | 4096 | Dual T4 |\n", "| 13-30B | IQ3_XS | ~12GB | 2048 | Dual T4 |\n", "\n", "---\n", "\n", "**Next:** [03-multi-gpu-inference](03-multi-gpu-inference-llamatelemetry-v0.1.0.ipynb)"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}