{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Inference with Tensor Split\n",
    "\n",
    "**Duration:** ~20 min | **Platform:** Kaggle dual Tesla T4 (2 × 15 GB VRAM)\n",
    "\n",
    "This notebook demonstrates how to leverage both T4 GPUs for LLM inference,\n",
    "compare different tensor-split strategies, and use the split-GPU architecture\n",
    "for simultaneous LLM + RAPIDS workloads.\n",
    "\n",
    "### What you'll learn\n",
    "1. Verify and configure dual GPUs\n",
    "2. Tensor-split strategies and benchmarking\n",
    "3. Split-GPU mode (LLM on GPU 0, RAPIDS on GPU 1)\n",
    "4. GPU context managers\n",
    "5. Background GPU monitoring during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "\n",
    "import llamatelemetry\n",
    "from llamatelemetry.gpu import list_devices, snapshot, start_sampler\n",
    "\n",
    "llamatelemetry.init(service_name=\"multi-gpu\")\n",
    "\n",
    "# Verify dual GPUs\n",
    "devices = list_devices()\n",
    "print(f\"Found {len(devices)} GPU(s):\")\n",
    "for d in devices:\n",
    "    print(f\"  GPU {d.id}: {d.name} — {d.memory_total_mb} MB (SM {d.compute_capability})\")\n",
    "assert len(devices) >= 2, \"This notebook requires 2 GPUs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor-Split Strategies\n",
    "\n",
    "Tensor-split controls how model weights are distributed across GPUs.\n",
    "A 50/50 split gives each GPU half the model; a 70/30 split favors GPU 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llamatelemetry.llama import ServerManager, LlamaCppClient\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "# Start with 50/50 split\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"0.5,0.5\", ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "\n",
    "# Check memory distribution\n",
    "snaps = snapshot()\n",
    "print(\"Memory after 50/50 split:\")\n",
    "for s in snaps:\n",
    "    print(f\"  GPU {s.gpu_id}: {s.mem_used_mb}/{s.mem_total_mb} MB\")\n",
    "\n",
    "mgr.stop_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Benchmark inference across different tensor-split configurations to find the\n",
    "optimal setting for your model and workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_to_test = [\n",
    "    (\"50/50\", \"0.5,0.5\"),\n",
    "    (\"70/30\", \"0.7,0.3\"),\n",
    "    (\"100/0\", \"1.0,0.0\"),\n",
    "]\n",
    "\n",
    "benchmark_prompt = \"Explain how tensor parallelism works for large language models in detail.\"\n",
    "benchmark_results = []\n",
    "\n",
    "for name, split in splits_to_test:\n",
    "    mgr = ServerManager()\n",
    "    mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=split, ctx_size=2048)\n",
    "    mgr.wait_until_ready(timeout=60)\n",
    "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "    # Warm-up\n",
    "    client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}], max_tokens=16\n",
    "    )\n",
    "\n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        t0 = time.perf_counter()\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": benchmark_prompt}],\n",
    "            max_tokens=128, temperature=0.7,\n",
    "        )\n",
    "        times.append(time.perf_counter() - t0)\n",
    "\n",
    "    avg_s = sum(times) / len(times)\n",
    "    tokens = resp.usage.completion_tokens\n",
    "    tps = tokens / avg_s if avg_s > 0 else 0\n",
    "    mem = snapshot()\n",
    "    benchmark_results.append((name, avg_s * 1000, tps, mem))\n",
    "    print(f\"  {name}: {avg_s*1000:.0f} ms avg, {tps:.1f} tok/s\")\n",
    "\n",
    "    mgr.stop_server()\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\nBenchmark complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-GPU Mode (LLM + RAPIDS)\n",
    "\n",
    "Run the LLM server on GPU 0 while reserving GPU 1 for RAPIDS/cuDF analytics.\n",
    "Use `tensor_split=\"1.0,0.0\"` to confine the model to GPU 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.kaggle import rapids_gpu\n",
    "\n",
    "# LLM on GPU 0 only\n",
    "mgr = ServerManager()\n",
    "mgr.start_server(model_path=model_path, gpu_layers=99, tensor_split=\"1.0,0.0\", ctx_size=2048)\n",
    "mgr.wait_until_ready(timeout=60)\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# RAPIDS work on GPU 1\n",
    "with rapids_gpu(1):\n",
    "    try:\n",
    "        import cudf\n",
    "        df = cudf.DataFrame({\"text\": [\"hello\", \"world\"], \"score\": [0.9, 0.8]})\n",
    "        print(f\"cuDF DataFrame on GPU 1: {len(df)} rows\")\n",
    "    except ImportError:\n",
    "        print(\"cuDF not available — RAPIDS context still set CUDA_VISIBLE_DEVICES=1\")\n",
    "\n",
    "# LLM inference on GPU 0 (concurrent)\n",
    "resp = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize split-GPU architecture.\"}],\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(f\"\\nLLM response: {resp.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Context Managers\n",
    "\n",
    "llamatelemetry provides context managers to temporarily pin operations to specific GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamatelemetry.kaggle import GPUContext, llm_gpu, single_gpu\n",
    "import os\n",
    "\n",
    "# Explicit GPU context\n",
    "with GPUContext(gpu_ids=[0, 1]):\n",
    "    print(f\"Both GPUs visible: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES', 'all')}\")\n",
    "\n",
    "with single_gpu(0):\n",
    "    print(f\"GPU 0 only: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES', 'all')}\")\n",
    "\n",
    "with llm_gpu([0]):\n",
    "    print(f\"LLM GPU context: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES', 'all')}\")\n",
    "\n",
    "print(f\"Restored: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES', 'all')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Monitoring During Inference\n",
    "\n",
    "`start_sampler()` launches a background thread that periodically captures GPU\n",
    "metrics, allowing you to profile memory and utilization over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start background GPU sampler\n",
    "handle = start_sampler(interval_ms=500)\n",
    "\n",
    "# Run inference workload\n",
    "for i in range(5):\n",
    "    client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Tell me fact #{i+1} about neural networks.\"}],\n",
    "        max_tokens=64,\n",
    "    )\n",
    "\n",
    "# Retrieve collected snapshots\n",
    "handle.stop()\n",
    "samples = handle.get_snapshots()\n",
    "print(f\"Collected {len(samples)} GPU snapshots during inference:\")\n",
    "for s in samples[:6]:  # show first 6\n",
    "    print(f\"  t={s.timestamp:.1f}  GPU {s.gpu_id}: {s.mem_used_mb} MB, {s.utilization_pct}% util\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Strategy | Use Case | Tensor Split |\n",
    "|----------|----------|-------------|\n",
    "| **50/50** | Maximum model size | `0.5,0.5` |\n",
    "| **70/30** | Faster GPU gets more layers | `0.7,0.3` |\n",
    "| **100/0** | LLM + RAPIDS split-GPU | `1.0,0.0` |\n",
    "| **Single GPU** | Small models, other GPU free | `None` + `main_gpu=0` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.stop_server()\n",
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
