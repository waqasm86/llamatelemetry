{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture Visualization\n",
    "\n",
    "**Duration:** ~40 min | **Platform:** Kaggle dual Tesla T4\n",
    "\n",
    "This notebook explores **GGUF model internals** — parsing architecture metadata,\n",
    "analyzing layer structures, visualizing weight distributions, and comparing\n",
    "model architectures using Graphistry.\n",
    "\n",
    "### What you'll learn\n",
    "1. Parse GGUF metadata and architecture details\n",
    "2. Analyze layer types and parameter counts\n",
    "3. Visualize weight distributions\n",
    "4. Build architecture graphs with Graphistry\n",
    "5. Compare different model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/llamatelemetry/llamatelemetry.git@v1.0.0\n",
    "!pip install -q matplotlib\n",
    "\n",
    "import llamatelemetry\n",
    "llamatelemetry.init(service_name=\"arch-explorer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing GGUF Metadata\n",
    "\n",
    "Read architecture details directly from the GGUF file header without loading\n",
    "the full model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llamatelemetry.llama import parse_gguf_header\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "info = parse_gguf_header(model_path, read_tensors=True)\n",
    "\n",
    "print(f\"=== {info.file} ===\")\n",
    "print(f\"Size: {info.size_mb:.1f} MB\")\n",
    "print(f\"Architecture: {info.metadata.architecture}\")\n",
    "print(f\"Layers (blocks): {info.metadata.block_count}\")\n",
    "print(f\"Embedding dim: {info.metadata.embedding_length}\")\n",
    "print(f\"Context length: {info.metadata.context_length}\")\n",
    "print(f\"Total tensors: {len(info.tensors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Analysis\n",
    "\n",
    "Categorize tensors by type and analyze parameter counts per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "@llamatelemetry.task(name=\"analyze-layers\")\n",
    "def analyze_layers(tensors):\n",
    "    \"\"\"Categorize tensors by type and compute statistics.\"\"\"\n",
    "    categories = defaultdict(list)\n",
    "\n",
    "    for t in tensors:\n",
    "        name = t.name\n",
    "        params = 1\n",
    "        for dim in t.shape:\n",
    "            params *= dim\n",
    "\n",
    "        # Categorize by tensor name pattern\n",
    "        if \"attn\" in name or \"q_proj\" in name or \"k_proj\" in name or \"v_proj\" in name or \"o_proj\" in name:\n",
    "            categories[\"attention\"].append((name, params, t.dtype))\n",
    "        elif \"ffn\" in name or \"gate_proj\" in name or \"up_proj\" in name or \"down_proj\" in name:\n",
    "            categories[\"feed_forward\"].append((name, params, t.dtype))\n",
    "        elif \"norm\" in name:\n",
    "            categories[\"normalization\"].append((name, params, t.dtype))\n",
    "        elif \"embed\" in name or \"token\" in name:\n",
    "            categories[\"embedding\"].append((name, params, t.dtype))\n",
    "        elif \"output\" in name:\n",
    "            categories[\"output\"].append((name, params, t.dtype))\n",
    "        else:\n",
    "            categories[\"other\"].append((name, params, t.dtype))\n",
    "\n",
    "    # Print summary\n",
    "    total_params = sum(p for cat in categories.values() for _, p, _ in cat)\n",
    "    print(f\"\\n{'Category':<18} {'Tensors':<10} {'Params':<15} {'% of Total'}\")\n",
    "    print(\"-\" * 55)\n",
    "    for cat, items in sorted(categories.items(), key=lambda x: -sum(p for _, p, _ in x[1])):\n",
    "        cat_params = sum(p for _, p, _ in items)\n",
    "        pct = cat_params / total_params * 100 if total_params > 0 else 0\n",
    "        print(f\"{cat:<18} {len(items):<10} {cat_params:<15,} {pct:.1f}%\")\n",
    "\n",
    "    return categories\n",
    "\n",
    "categories = analyze_layers(info.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Distribution\n",
    "\n",
    "Visualize the distribution of tensor sizes across different layer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameter counts per category\n",
    "cat_names = []\n",
    "cat_params = []\n",
    "for cat, items in sorted(categories.items(), key=lambda x: -sum(p for _, p, _ in x[1])):\n",
    "    cat_names.append(cat)\n",
    "    cat_params.append(sum(p for _, p, _ in items))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart of parameter distribution\n",
    "axes[0].pie(cat_params, labels=cat_names, autopct=\"%1.1f%%\", startangle=90)\n",
    "axes[0].set_title(\"Parameter Distribution by Layer Type\")\n",
    "\n",
    "# Bar chart of tensor counts\n",
    "cat_counts = [len(categories[c]) for c in cat_names]\n",
    "axes[1].barh(cat_names, cat_counts, color=\"steelblue\")\n",
    "axes[1].set_xlabel(\"Number of Tensors\")\n",
    "axes[1].set_title(\"Tensor Count by Layer Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture as Graph\n",
    "\n",
    "Build a graph representing the model architecture, where nodes are layers\n",
    "and edges represent data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llamatelemetry.kaggle import rapids_gpu\n",
    "\n",
    "with rapids_gpu(1):\n",
    "    # Build architecture graph\n",
    "    n_blocks = info.metadata.block_count or 1\n",
    "\n",
    "    nodes = [{\"id\": \"input\", \"type\": \"embedding\", \"layer\": -1}]\n",
    "    edges = []\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "        block_nodes = [\n",
    "            {\"id\": f\"attn_{i}\", \"type\": \"attention\", \"layer\": i},\n",
    "            {\"id\": f\"norm_attn_{i}\", \"type\": \"normalization\", \"layer\": i},\n",
    "            {\"id\": f\"ffn_{i}\", \"type\": \"feed_forward\", \"layer\": i},\n",
    "            {\"id\": f\"norm_ffn_{i}\", \"type\": \"normalization\", \"layer\": i},\n",
    "        ]\n",
    "        nodes.extend(block_nodes)\n",
    "\n",
    "        prev = \"input\" if i == 0 else f\"norm_ffn_{i-1}\"\n",
    "        edges.extend([\n",
    "            {\"src\": prev, \"dst\": f\"norm_attn_{i}\", \"type\": \"residual\"},\n",
    "            {\"src\": f\"norm_attn_{i}\", \"dst\": f\"attn_{i}\", \"type\": \"forward\"},\n",
    "            {\"src\": f\"attn_{i}\", \"dst\": f\"norm_ffn_{i}\", \"type\": \"residual\"},\n",
    "            {\"src\": f\"norm_ffn_{i}\", \"dst\": f\"ffn_{i}\", \"type\": \"forward\"},\n",
    "        ])\n",
    "\n",
    "    nodes.append({\"id\": \"output\", \"type\": \"output\", \"layer\": n_blocks})\n",
    "    edges.append({\"src\": f\"norm_ffn_{n_blocks-1}\" if n_blocks > 0 else \"input\", \"dst\": \"output\", \"type\": \"forward\"})\n",
    "\n",
    "    node_df = pd.DataFrame(nodes)\n",
    "    edge_df = pd.DataFrame(edges)\n",
    "\n",
    "    print(f\"Architecture graph: {len(node_df)} nodes, {len(edge_df)} edges\")\n",
    "    print(f\"Blocks: {n_blocks}\")\n",
    "\n",
    "    try:\n",
    "        import graphistry\n",
    "        g = (graphistry\n",
    "             .edges(edge_df, \"src\", \"dst\")\n",
    "             .nodes(node_df, \"id\")\n",
    "             .bind(point_title=\"id\", edge_title=\"type\")\n",
    "             .encode_point_color(\"type\", categorical_mapping={\n",
    "                 \"embedding\": \"blue\", \"attention\": \"red\",\n",
    "                 \"feed_forward\": \"green\", \"normalization\": \"gray\",\n",
    "                 \"output\": \"orange\",\n",
    "             }))\n",
    "        g.plot()\n",
    "    except Exception as e:\n",
    "        print(f\"Graphistry: {e}\")\n",
    "        print(\"\\nArchitecture (first 3 blocks):\")\n",
    "        for _, row in edge_df.head(12).iterrows():\n",
    "            print(f\"  {row['src']} → {row['dst']} ({row['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism Structure\n",
    "\n",
    "Analyze the attention head configuration from GGUF metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = info.metadata\n",
    "\n",
    "# Attention configuration\n",
    "attn_tensors = [t for t in info.tensors if any(k in t.name for k in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"])]\n",
    "\n",
    "print(\"Attention Configuration:\")\n",
    "print(f\"  Embedding dim: {meta.embedding_length}\")\n",
    "print(f\"  Layers: {meta.block_count}\")\n",
    "print(f\"  Attention tensors: {len(attn_tensors)}\")\n",
    "\n",
    "# Analyze Q/K/V shapes\n",
    "print(f\"\\nAttention tensor shapes (first block):\")\n",
    "for t in attn_tensors[:4]:\n",
    "    print(f\"  {t.name}: {t.shape} ({t.dtype})\")\n",
    "\n",
    "# Estimate head count from Q projection shape\n",
    "q_tensors = [t for t in attn_tensors if \"q_proj\" in t.name]\n",
    "if q_tensors:\n",
    "    q_shape = q_tensors[0].shape\n",
    "    embed_dim = meta.embedding_length or q_shape[-1]\n",
    "    # Head dim is typically 64 or 128\n",
    "    for head_dim in [64, 128]:\n",
    "        n_heads = embed_dim // head_dim\n",
    "        if n_heads > 0:\n",
    "            print(f\"\\n  Estimated heads (dim={head_dim}): {n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Architectures\n",
    "\n",
    "Download and compare two different model architectures side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two models (using same model with different quants as demo)\n",
    "model_q5_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/google_gemma-3-1b-it-GGUF\",\n",
    "    filename=\"google_gemma-3-1b-it-Q5_K_M.gguf\",\n",
    "    cache_dir=\"/root/.cache/huggingface\",\n",
    ")\n",
    "\n",
    "info_q4 = parse_gguf_header(model_path, read_tensors=True)\n",
    "info_q5 = parse_gguf_header(model_q5_path, read_tensors=True)\n",
    "\n",
    "print(f\"{'Property':<25} {'Q4_K_M':<20} {'Q5_K_M'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'File size (MB)':<25} {info_q4.size_mb:<20.1f} {info_q5.size_mb:.1f}\")\n",
    "print(f\"{'Architecture':<25} {str(info_q4.metadata.architecture):<20} {info_q5.metadata.architecture}\")\n",
    "print(f\"{'Layers':<25} {str(info_q4.metadata.block_count):<20} {info_q5.metadata.block_count}\")\n",
    "print(f\"{'Embedding dim':<25} {str(info_q4.metadata.embedding_length):<20} {info_q5.metadata.embedding_length}\")\n",
    "print(f\"{'Context length':<25} {str(info_q4.metadata.context_length):<20} {info_q5.metadata.context_length}\")\n",
    "print(f\"{'Tensor count':<25} {len(info_q4.tensors):<20} {len(info_q5.tensors)}\")\n",
    "print(f\"{'Size ratio':<25} {'1.00x':<20} {info_q5.size_mb/info_q4.size_mb:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- **GGUF parsing** without loading model weights into GPU memory\n",
    "- **Layer analysis** with automatic categorization\n",
    "- **Architecture graphs** rendered with Graphistry\n",
    "- **Model comparison** across quantization levels\n",
    "\n",
    "These tools help you understand model internals before deploying on constrained hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamatelemetry.shutdown()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
